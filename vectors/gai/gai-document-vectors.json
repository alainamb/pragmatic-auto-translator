{
  "metadata": {
    "model": "distiluse-base-multilingual-cased-v2",
    "dimension": 512
  },
  "vectors": [
    {
      "id": "gai-eng_item001",
      "title": "Attention is All You Need",
      "count": 1,
      "created": "2025-05-30T02:47:52.455419",
      "text": "Attention is All You Need The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Introduction Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. ht ht−1 t Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9]. Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1,.,xn) to a sequence of continuous representations z = (z1,.,zn). Given z, the decoder then generates an output sequence (y1,.,ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. (x1,.,xn) z = (z1,.,zn) (y1,.,ym) The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. N = 6 LayerNorm(x + Sublayer(x)) Sublayer(x) dmodel = 512 Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. N = 6 position i Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values. dk dv √dk In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as: Q K V Attention(Q,K,V) = softmax(QKT/√dk)V (1) Attention(Q,K,V) = softmax(QKT/√dk)V Equation (1) The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1/√dk. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. 1/√dk While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1/√dk. dk 1/√dk Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. dmodel h dk dv Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHead(Q,K,V) = Concat(head1,.,headh)WO where headi = Attention(QWQi,KWKi,VWVi) MultiHead(Q,K,V) Concat(head1,.,headh) WO headi Attention(QWQi,KWKi,VWVi) WQi WKi WVi Where the projections are parameter matrices WQi ∈Rdmodel×dk, WKi ∈Rdmodel×dk, WVi ∈Rdmodel×dv and WO ∈Rhdv×dmodel. WQi ∈Rdmodel×dk WKi ∈Rdmodel×dk WVi ∈Rdmodel×dv WO ∈Rhdv×dmodel In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality h = 8 dk = dv = dmodel/h = 64 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2. −∞ Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(0, xW1 + b1)W2 + b2 (2) FFN(x) = max(0, xW1 + b1)W2 + b2 Equation (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048. dmodel = 512 dff = 2048 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel. dmodel √dmodel Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9]. dmodel In this work, we use sine and cosine functions of different frequencies: PE(pos,2i) = sin(pos/100002i/dmodel) PE(pos,2i) = sin(pos/100002i/dmodel) PE(pos,2i+1) = cos(pos/100002i/dmodel) PE(pos,2i+1) = cos(pos/100002i/dmodel) where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos. pos i 2π 10000 · 2π k PEpos+k PEpos We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1,.,xn) to another sequence of equal length (z1,.,zn), with xi,zi ∈ Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. (x1,.,xn) (z1,.,zn) xi,zi ∈ Rd One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. O(n) n d r O(n/r) A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k ·n ·d + n ·d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. k < n O(n/k) O(logk(n)) k O(k ·n ·d + n ·d2) k = n As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. Training This section describes the training regime for our models. Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, (described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). Optimizer We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ε = 10−9. We varied the learning rate over the course of training, according to the formula: β1 = 0.9 β2 = 0.98 ε = 10−9 lrate = d−0.5model · min(step_num−0.5, step_num · warmup_steps−1.5) (3) lrate = d−0.5model · min(step_num−0.5, step_num · warmup_steps−1.5) Equation (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000. warmup_steps warmup_steps = 4000 Regularization We employ three types of regularization during training: Residual Dropout: We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1. Pdrop = 0.1 Label Smoothing: During training, we employed label smoothing of value εls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. εls = 0.1 Results Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3. Pdrop = 0.1 For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. α = 0.6 Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5. Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. dk English Constituency Parsing To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting. dmodel = 1024 We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3 for both WSJ only and the semi-supervised setting. α = 0.3 Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/tensorflow/tensor2tensor. Acknowledgements: We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.",
      "vector": [
        -0.01936890371143818,
        -0.023367807269096375,
        -0.03598420321941376,
        -0.030867017805576324,
        0.07637834548950195,
        -0.014022710733115673,
        -0.0051175812259316444,
        0.007455617655068636,
        0.053152985870838165,
        0.025570616126060486,
        -0.023776313289999962,
        -0.010299667716026306,
        0.0010159133234992623,
        -0.046273209154605865,
        -0.03696122393012047,
        0.03868010267615318,
        -0.01789514161646366,
        -0.006067298352718353,
        0.014763827435672283,
        -0.0008551206556148827,
        0.009190571494400501,
        -0.006006842013448477,
        0.032072920352220535,
        0.05313652381300926,
        0.010043083690106869,
        0.03791732341051102,
        0.05655413120985031,
        0.03471721336245537,
        -0.0013633593916893005,
        -0.04921219125390053,
        0.011555985547602177,
        -0.025471681728959084,
        0.06202303618192673,
        0.008677558042109013,
        -0.02121696062386036,
        -0.01456397958099842,
        -0.009682492353022099,
        -0.010033381171524525,
        -0.012145627290010452,
        0.006561896298080683,
        -0.03771049156785011,
        0.01672782562673092,
        -0.04863521456718445,
        0.04124777764081955,
        -0.026199031621217728,
        -0.0003015175461769104,
        -0.0034147545229643583,
        0.08434826880693436,
        -0.004204452969133854,
        -0.029243813827633858,
        0.0420411080121994,
        0.012248210608959198,
        0.028693176805973053,
        0.11809033155441284,
        0.0702444314956665,
        -0.03799467533826828,
        -0.0713338851928711,
        0.02041422203183174,
        -0.03451412916183472,
        0.05969725176692009,
        -0.079066202044487,
        0.03155912086367607,
        0.0052988785319030285,
        0.023740367963910103,
        -0.011360379867255688,
        0.03195120394229889,
        0.0030236158054322004,
        0.03753103315830231,
        0.013507476076483727,
        -0.021603899076581,
        -0.0006269714212976396,
        0.08386419713497162,
        0.014961629174649715,
        0.03617878258228302,
        0.007712605409324169,
        -0.09847187250852585,
        -0.01835484616458416,
        0.047333527356386185,
        0.00811069831252098,
        0.0073940334841609,
        -0.06806614995002747,
        -0.012454572133719921,
        0.07828692346811295,
        -0.056692998856306076,
        0.01276103500276804,
        0.07248520851135254,
        0.022805478423833847,
        -0.04076525568962097,
        0.0028473332058638334,
        -0.012127837166190147,
        0.01606878452003002,
        -0.006829564925283194,
        0.11316949129104614,
        0.038770802319049835,
        0.04584662616252899,
        0.0355626605451107,
        -0.05232960358262062,
        -0.026106232777237892,
        0.0546887032687664,
        -0.043485935777425766,
        -0.026209065690636635,
        0.04524767026305199,
        -0.03598220273852348,
        -0.033235445618629456,
        0.038113318383693695,
        -0.017077365890145302,
        0.05912170559167862,
        0.027476029470562935,
        0.026649871841073036,
        0.00224318727850914,
        0.003927948884665966,
        -0.06422332674264908,
        -0.05222122743725777,
        -0.018278837203979492,
        -0.029111234471201897,
        0.03312259167432785,
        0.0002637124271132052,
        -0.03634655848145485,
        0.010638226754963398,
        0.030948743224143982,
        -0.04752668738365173,
        0.09149666130542755,
        0.013515335507690907,
        0.03724287822842598,
        -0.02260695956647396,
        0.011386465281248093,
        0.005519763100892305,
        0.03787335753440857,
        -0.019851403310894966,
        0.017344916239380836,
        0.02550974115729332,
        0.09476214647293091,
        0.04152384400367737,
        -0.024011097848415375,
        -0.008222677744925022,
        0.045703914016485214,
        -0.09170184284448624,
        0.06431017071008682,
        0.042579129338264465,
        0.07189704477787018,
        -0.08884388953447342,
        -0.02241550013422966,
        -0.01014226209372282,
        -0.035464197397232056,
        0.011120973154902458,
        -0.042394258081912994,
        -0.0005283583304844797,
        -0.06309454888105392,
        0.033391073346138,
        0.053559720516204834,
        -0.04616289958357811,
        0.09580816328525543,
        0.023475510999560356,
        -0.06357366591691971,
        0.04434656351804733,
        -0.022256003692746162,
        0.03908248618245125,
        0.008536367677152157,
        -0.011279215104877949,
        -0.06880704313516617,
        0.019287940114736557,
        -0.009153209626674652,
        -0.021806485950946808,
        0.05945628136396408,
        -0.03626348823308945,
        -0.04672340303659439,
        0.07279884815216064,
        0.0005203699111007154,
        0.021146127954125404,
        0.031133439391851425,
        0.008681860752403736,
        0.014398900792002678,
        0.0520826056599617,
        0.03007020428776741,
        0.0013368105283007026,
        0.04382709413766861,
        -0.015703050419688225,
        -0.01745774783194065,
        0.05075183883309364,
        0.07174672931432724,
        -0.031088145449757576,
        -0.028757244348526,
        0.08377229422330856,
        0.04447344318032265,
        0.027924316003918648,
        -0.014943872578442097,
        -0.032398123294115067,
        0.04379455745220184,
        -0.03395964577794075,
        0.03867015242576599,
        -0.013850612565875053,
        -0.04976893588900566,
        0.04597201198339462,
        0.044116243720054626,
        0.013607035391032696,
        -0.017050202935934067,
        0.05051996186375618,
        0.09186446666717529,
        0.034993816167116165,
        0.03708590567111969,
        -0.0029870171565562487,
        -0.005574313923716545,
        0.04734876751899719,
        -0.030105456709861755,
        0.012641924433410168,
        -0.051459312438964844,
        -0.03822288289666176,
        0.03821928799152374,
        -0.03225253149867058,
        -0.0010564954718574882,
        0.033792514353990555,
        -0.030866479501128197,
        -0.06632886081933975,
        -0.01849508099257946,
        -0.024586744606494904,
        -0.010793093591928482,
        -0.04303785786032677,
        -0.026616442948579788,
        -0.011869148351252079,
        -0.03809897601604462,
        -0.011035311035811901,
        0.02609143778681755,
        0.08040961623191833,
        0.09402117878198624,
        0.045322395861148834,
        0.004196517635136843,
        0.04232481122016907,
        -0.058388616889715195,
        -0.009653883054852486,
        -0.016152242198586464,
        0.02276078425347805,
        -0.03213227912783623,
        -0.055881571024656296,
        -0.006935272831469774,
        0.05092323198914528,
        -0.014015826396644115,
        0.056799303740262985,
        -0.00431882543489337,
        -0.02780880406498909,
        0.014106822200119495,
        -0.03047102317214012,
        0.001493301009759307,
        -0.018088744953274727,
        -0.007907313294708729,
        0.027096332982182503,
        -6.708118598908186e-05,
        -0.04363032430410385,
        0.018294725567102432,
        -0.023253977298736572,
        -0.013778211548924446,
        -0.031470466405153275,
        0.029339784756302834,
        0.015195917338132858,
        -0.03680680692195892,
        0.09242260456085205,
        0.03665359690785408,
        0.019660741090774536,
        0.0180558729916811,
        0.054658934473991394,
        -0.008345009759068489,
        0.03114105574786663,
        -0.006914472207427025,
        0.03133969008922577,
        -0.018254848197102547,
        0.0326102077960968,
        -0.08504148572683334,
        -0.016423972323536873,
        0.07875610142946243,
        -0.013999241404235363,
        -0.052775051444768906,
        0.001161656342446804,
        0.10043489187955856,
        0.007196509744971991,
        -0.005314277019351721,
        -0.05461348593235016,
        -0.020883336663246155,
        -0.008354926481842995,
        -0.03031814843416214,
        0.011703365482389927,
        -0.013968669809401035,
        -0.05511019751429558,
        0.0416966937482357,
        0.05585337430238724,
        -0.05772768706083298,
        0.009618029929697514,
        -0.04380738362669945,
        -0.05717701092362404,
        -0.07591533660888672,
        -0.005407523829489946,
        -0.026888150721788406,
        0.025167115032672882,
        -0.05556068196892738,
        0.08121901750564575,
        -0.05237803980708122,
        -0.04401807859539986,
        0.049290478229522705,
        0.023188825696706772,
        -0.025663767009973526,
        -0.023510098457336426,
        -0.05110582336783409,
        0.05641467496752739,
        -0.04953614994883537,
        -0.0013231417397037148,
        0.082754947245121,
        0.029747774824500084,
        -0.05038878321647644,
        0.06616850942373276,
        -0.003337433096021414,
        0.0878259465098381,
        0.03691507503390312,
        0.06529112160205841,
        -0.054410431534051895,
        0.025311868637800217,
        0.07217948138713837,
        -0.01600324548780918,
        0.06516963988542557,
        -0.00933114904910326,
        -0.0017063920386135578,
        0.07952164113521576,
        0.01916147954761982,
        0.041295893490314484,
        0.02188687212765217,
        0.03475922346115112,
        0.030872881412506104,
        0.02886219322681427,
        -0.0054658143781125546,
        0.017345668748021126,
        0.004693371243774891,
        -0.03135702759027481,
        0.031794093549251556,
        0.01050231046974659,
        0.04346005246043205,
        -0.016245165839791298,
        -0.008140930905938148,
        -0.031115099787712097,
        0.0058570969849824905,
        0.039594486355781555,
        0.03379730135202408,
        0.011612941510975361,
        -0.06093424931168556,
        0.007031989749521017,
        0.008846147917211056,
        0.06562310457229614,
        0.012349012307822704,
        0.053355880081653595,
        0.028065871447324753,
        -0.0874011367559433,
        -0.010657419450581074,
        0.011989143677055836,
        -0.0207349993288517,
        0.007212183438241482,
        0.05155828222632408,
        -0.04518379271030426,
        0.03749673068523407,
        -0.05325685441493988,
        0.007210648152977228,
        -0.006880762986838818,
        0.026033516973257065,
        -0.00564207648858428,
        0.02436063624918461,
        0.04172736778855324,
        0.019941801205277443,
        0.001778950565494597,
        0.03639042377471924,
        -0.0389002189040184,
        -0.029381316155195236,
        -0.02534930780529976,
        -0.03539717197418213,
        0.04472111165523529,
        0.04066002741456032,
        0.03247704729437828,
        -0.028425248339772224,
        0.05425334349274635,
        0.054070569574832916,
        0.0234834011644125,
        0.04061673581600189,
        -0.05074058473110199,
        0.008608171716332436,
        0.05145636945962906,
        -0.03997056558728218,
        -0.027982188388705254,
        0.0022478573955595493,
        0.02174876630306244,
        0.03664940968155861,
        0.04404453933238983,
        -0.044024672359228134,
        -0.009636182337999344,
        0.0831756591796875,
        -0.005213956814259291,
        0.0929798036813736,
        -0.071010060608387,
        -0.030123913660645485,
        0.04646178334951401,
        0.05706321820616722,
        0.03447182476520538,
        -0.026314018294215202,
        -0.03904649615287781,
        -0.008406956680119038,
        0.041865527629852295,
        -0.05638629198074341,
        -0.03572887182235718,
        -0.06777601689100266,
        0.003679888555780053,
        0.006936992052942514,
        -0.013375605456531048,
        0.08747602999210358,
        -0.03247283399105072,
        0.03315785154700279,
        -0.042039286345243454,
        0.0274090226739645,
        0.04320843517780304,
        -0.010946481488645077,
        0.0352335125207901,
        0.01953180506825447,
        -0.047382522374391556,
        0.04660625383257866,
        -0.005558861885219812,
        -0.01762223243713379,
        0.022130640223622322,
        -0.027839817106723785,
        0.0027240333147346973,
        0.002186724217608571,
        -0.09517919272184372,
        -0.01666097156703472,
        -0.04770338162779808,
        -0.05492386966943741,
        -0.0013865823857486248,
        0.01590798981487751,
        0.048001885414123535,
        -0.007667258847504854,
        0.10374843329191208,
        0.016479630023241043,
        -0.027008501812815666,
        -0.01430038083344698,
        -0.031308408826589584,
        0.04917319491505623,
        0.01838277094066143,
        0.05603213235735893,
        -0.052336256951093674,
        0.0007281935540959239,
        0.05840625241398811,
        -0.04679691046476364,
        -0.015526451170444489,
        0.013392392545938492,
        0.06193897873163223,
        -0.030279384925961494,
        0.0189047921448946,
        0.020748626440763474,
        -0.004315996076911688,
        -0.02359309419989586,
        -0.008781787008047104,
        0.03207849711179733,
        0.05605975165963173,
        -0.019480818882584572,
        -0.08354872465133667,
        0.0536482147872448,
        -0.002968142507597804,
        0.007318086456507444,
        -0.030247313901782036,
        0.02040279470384121,
        0.017550475895404816,
        0.026606054976582527,
        0.003098134184256196,
        -0.0035903414245694876,
        -0.0492633692920208,
        -0.020702730864286423,
        -0.00018382968846708536,
        0.030080562457442284,
        -0.02547801099717617,
        0.0025452706031501293,
        0.03236747160553932,
        -0.05062658712267876,
        -0.043863292783498764,
        0.01525037456303835,
        0.0018336568027734756,
        0.034420184791088104,
        -0.031790591776371,
        -0.023717010393738747,
        0.0391235277056694,
        0.008696665056049824,
        -0.0016654891660436988,
        0.05841269716620445,
        0.03385893628001213,
        0.04983639717102051,
        0.05357637628912926,
        -0.05879580229520798,
        -0.0023908917792141438,
        -0.03541160747408867,
        0.04121286794543266,
        -0.04112982004880905,
        -0.09006138145923615,
        0.026975607499480247,
        -0.008004396222531796,
        0.011756736785173416,
        -0.05951059237122536,
        0.024336401373147964,
        0.03270873427391052,
        0.005512189585715532,
        -0.024241145700216293,
        -0.06540749222040176,
        0.033510394394397736,
        -0.022309869527816772,
        -0.04993383213877678,
        0.01965450495481491,
        -0.0282229445874691,
        0.042075660079717636,
        -0.024002183228731155,
        -0.012778770178556442,
        -0.058875780552625656,
        0.05799216777086258,
        0.014230024069547653,
        -0.030952222645282745
      ]
    },
    {
      "id": "gai-eng_item002",
      "count": 2,
      "created": "2025-05-30T03:27:25.837402",
      "text": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models. CCS CONCEPTS • Computing methodologies → Natural language processing. ACM Reference Format: Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Conference on Fairness, Accountability, and Transparency (FAccT ’21), March 3–10, 2021, Virtual Event, Canada. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3442188.3445922. INTRODUCTION One of the biggest trends in natural language processing (NLP) has been the increasing size of language models (LMs) as measured by the number of parameters and size of training data. Since 2018 alone, we have seen the emergence of BERT and its variants [39, 70, 74, 113, 146], GPT-2 [106], T-NLG [112], GPT-3 [25], and most recently Switch-C [43], with institutions seemingly competing to produce ever larger LMs. While investigating properties of LMs and how they change with size holds scientific interest, and large LMs have shown improvements on various tasks (§2), we ask whether enough thought has been put into the potential risks associated with developing them and strategies to mitigate these risks. Citations: [[39, 70, 74, 113, 146]: ref_39, ref_70, ref_74, ref_113, ref_146] [[106]: ref_106] [[112]: ref_112] [[25]: ref_25] [[43]: ref_43] [Ref: §2 -> section_2] We first consider environmental risks. Echoing a line of recent work outlining the environmental and financial costs of deep learning systems [129], we encourage the research community to prioritize these impacts. One way this can be done is by reporting costs and evaluating works based on the amount of resources they consume [57]. As we outline in §3, increasing the environmental and financial costs of these models doubly punishes marginalized communities that are least likely to benefit from the progress achieved by large LMs and most likely to be harmed by negative environmental consequences of its resource consumption. At the scale we are discussing (outlined in §2), the first consideration should be the environmental cost. Citations: [[129]: ref_129] [[57]: ref_57] [Ref: §3 -> section_3] [Ref: §2 -> section_2] Just as environmental impact scales with model size, so does the difficulty of understanding what is in the training data. In §4, we discuss how large datasets based on texts from the Internet overrepresent hegemonic viewpoints and encode biases potentially damaging to marginalized populations. In collecting ever larger datasets we risk incurring documentation debt. We recommend mitigating these risks by budgeting for curation and documentation at the start of a project and only creating datasets as large as can be sufficiently documented. [Ref: §4 -> section_4] As argued by Bender and Koller [14], it is important to understand the limitations of LMs and put their success in context. This not only helps reduce hype which can mislead the public and researchers themselves regarding the capabilities of these LMs, but might encourage new research directions that do not necessarily depend on having larger LMs. As we discuss in §5, LMs are not performing natural language understanding (NLU), and only have success in tasks that can be approached by manipulating linguistic form [14]. Focusing on state-of-the-art results on leaderboards without encouraging deeper understanding of the mechanism by which they are achieved can cause misleading results as shown in [21, 93] and direct resources away from efforts that would facilitate long-term progress towards natural language understanding, without using unfathomable training data. Citations: [[14]: ref_14] [[21, 93]: ref_21, ref_93] [Ref: §5 -> section_5] Furthermore, the tendency of human interlocutors to impute meaning where there is none can mislead both NLP researchers and the general public into taking synthetic text as meaningful. Combined with the ability of LMs to pick up on both subtle biases and overtly abusive language patterns in training data, this leads to risks of harms, including encountering derogatory language and experiencing discrimination at the hands of others who reproduce racist, sexist, ableist, extremist or other harmful ideologies reinforced through interactions with synthetic language. We explore these potential harms in §6 and potential paths forward in §7. [Ref: §6 -> section_6] [Ref: §7 -> section_7] We hope that a critical overview of the risks of relying on ever-increasing size of LMs as the primary driver of increased performance of language technology can facilitate a reallocation of efforts towards approaches that avoid some of these risks while still reaping the benefits of improvements to language technology. BACKGROUND Similar to [14], we understand the term language model (LM) to refer to systems which are trained on string prediction tasks: that is, predicting the likelihood of a token (character, word or string) given either its preceding context or (in bidirectional and masked LMs) its surrounding context. Such systems are unsupervised and when deployed, take a text as input, commonly outputting scores or string predictions. Initially proposed by Shannon in 1949 [117], some of the earliest implemented LMs date to the early 1980s and were used as components in systems for automatic speech recognition (ASR), machine translation (MT), document classification, and more [111]. In this section, we provide a brief overview of the general trend of language modeling in recent years. For a more in-depth survey of pretrained LMs, see [105]. Citations: [[14]: ref_14] [[117]: ref_117] [[111]: ref_111] [[105]: ref_105] Before neural models, n-gram models also used large amounts of data [20, 87]. In addition to ASR, these large n-gram models of English were developed in the context of machine translation from another source language with far fewer direct translation examples. For example, [20] developed an n-gram model for English with a total of 1.8T n-grams and noted steady improvements in BLEU score on the test set of 1797 Arabic translations as the training data was increased from 13M tokens. Citations: [[20, 87]: ref_20, ref_87] [[20]: ref_20] The next big step was the move towards using pretrained representations of the distribution of words (called word embeddings) in other (supervised) NLP tasks. These word vectors came from systems such as word2vec [85] and GloVe [98] and later LSTM models such as context2vec [82] and ELMo [99] and supported state of the art performance on question answering, textual entailment, semantic role labeling (SRL), coreference resolution, named entity recognition (NER), and sentiment analysis, at first in English and later for other languages as well. While training the word embeddings required a (relatively) large amount of data, it reduced the amount of labeled data necessary for training on the various supervised tasks. For example, [99] showed that a model trained with ELMo reduced the necessary amount of training data needed to achieve similar results on SRL compared to models without, as shown in one instance where a model trained with ELMo reached the maximum development F1 score in 10 epochs as opposed to 486 without ELMo. This model furthermore achieved the same F1 score with 1% of the data as the baseline model achieved with 10% of the training data. Increasing the number of model parameters, however, did not yield noticeable increases for LSTMs [e.g. 82]. Citations: [[85]: ref_85] [[98]: ref_98] [[82]: ref_82] [[99]: ref_99] [[99]: ref_99] [[e.g. 82]: ref_82] Transformer models, on the other hand, have been able to continuously benefit from larger architectures and larger quantities of data. Devlin et al. [39] in particular noted that training on a large dataset and fine-tuning for specific tasks leads to strictly increasing results on the GLUE tasks [138] for English as the hyperparameters of the model were increased. Initially developed as Chinese LMs, the ERNIE family [130, 131, 145] produced ERNIE-Gen, which was also trained on the original (English) BERT dataset, joining the ranks of very large LMs. NVIDIA released the MegatronLM which has 8.3B parameters and was trained on 174GB of text from the English Wikipedia, OpenWebText, RealNews and CC-Stories datasets [122]. Trained on the same dataset, Microsoft released T-NLG 1, an LM with 17B parameters. OpenAI's GPT-3 [25] and Google's GShard [73] and Switch-C [43] have increased the definition of large LM by orders of magnitude in terms of parameters at 175B, 600B, and 1.6T parameters, respectively. Table 1 summarizes a selection of these LMs in terms of training data size and parameters. As increasingly large amounts of text are collected from the web in datasets such as the Colossal Clean Crawled Corpus [107] and the Pile [51], this trend of increasingly large LMs can be expected to continue as long as they correlate with an increase in performance. [Footnote 1]: https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameterlanguage-model-by-microsoft/ Citations: [[39]: ref_39] [[138]: ref_138] [[130, 131, 145]: ref_130, ref_131, ref_145] [[122]: ref_122] [[25]: ref_25] [[73]: ref_73] [[43]: ref_43] [[107]: ref_107] [[51]: ref_51] [Ref: Table 1 -> table_1] A number of these models also have multilingual variants such as mBERT [39] and mT5 [148] or are trained with some amount of multilingual data such as GPT-3 where 7% of the training data was not in English [25]. The performance of these multilingual models across languages is an active area of research. Wu and Drezde [144] found that while mBERT does not perform equally well across all 104 languages in its training data, it performed better at NER, POS tagging, and dependency parsing than monolingual models trained with comparable amounts of data for four low-resource languages. Conversely, [95] surveyed monolingual BERT models developed with more specific architecture considerations or additional monolingual data and found that they generally outperform mBERT across 29 tasks. Either way, these models do not address the inclusion problems raised by [65], who note that over 90% of the world's languages used by more than a billion people currently have little to no support in terms of language technology. Citations: [[39]: ref_39] [[148]: ref_148] [[25]: ref_25] [[144]: ref_144] [[95]: ref_95] [[65]: ref_65] Alongside work investigating what information the models retain from the data, we see a trend in reducing the size of these models using various techniques such as knowledge distillation [26, 58], quantization [118, 153], factorized embedding parameterization and cross-layer parameter sharing [70], and progressive module replacing [146]. Rogers et al. [110] provide a comprehensive comparison of models derived from BERT using these techniques, such as DistilBERT [113] and ALBERT [70]. While these models maintain and sometimes exceed the performance of the original BERT model, despite their much smaller size, they ultimately still rely on large quantities of data and significant processing and storage capabilities to both hold and reduce the model. Citations: [[26, 58]: ref_26, ref_58] [[118, 153]: ref_118, ref_153] [[70]: ref_70] [[146]: ref_146] [[110]: ref_110] [[113]: ref_113] [[70]: ref_70] We note that the change from n-gram LMs to word vectors distilled from neural LMs to pretrained Transformer LMs is paralleled by an expansion and change in the types of tasks they are useful for: n-gram LMs were initially typically deployed in selecting among the outputs of e.g. acoustical or translation models; the LSTM-derived word vectors were quickly picked up as more effective representations of words (in place of bag of words features) in a variety of NLP tasks involving labeling and classification; and the pretrained Transformer models can be retrained on very small datasets (few-shot, one-shot or even zero-shot learning) to perform apparently meaning-manipulating tasks such as summarization, question answering and the like. Nonetheless, all of these systems share the property of being LMs in the sense we give above, that is, systems trained to predict sequences of words (or characters or sentences). Where they differ is in the size of the training datasets they leverage and the spheres of influence they can possibly affect. By scaling up in these two ways, modern very large LMs incur new kinds of risk, which we turn to in the following sections. ENVIRONMENTAL AND FINANCIAL COST Strubell et al. recently benchmarked model training and development costs in terms of dollars and estimated CO₂ emissions [129]. While the average human is responsible for an estimated 5t CO₂e per year,2 the authors trained a Transformer (big) model [136] with neural architecture search and estimated that the training procedure emitted 284t of CO₂. Training a single BERT base model (without hyperparameter tuning) on GPUs was estimated to require as much energy as a trans-American flight. [Footnote 2]: Data for 2017, from https://ourworldindata.org/co2-emissions, accessed Jan 21, 2021 Citations: [[129]: ref_129] [[136]: ref_136] While some of this energy comes from renewable sources, or cloud compute companies' use of carbon credit-offset sources, the authors note that the majority of cloud compute providers' energy is not sourced from renewable sources and many energy sources in the world are not carbon neutral. In addition, renewable energy sources are still costly to the environment,3 and data centers with increasing computation requirements take away from other potential uses of green energy,4 underscoring the need for energy efficient model architectures and training paradigms. [Footnote 3]: https://www.heraldscotland.com/news/18270734.14m-trees-cut-scotland-make-waywind-farms/ [Footnote 4]: https://news.microsoft.com/2017/11/02/microsoft-announces-one-of-the-largestwind-deals-in-the-netherlands-with-vattenfall/ Strubell et al. also examine the cost of these models vs. their accuracy gains. For the task of machine translation where large LMs have resulted in performance gains, they estimate that an increase in 0.1 BLEU score using neural architecture search for English to German translation results in an increase of $150,000 compute cost in addition to the carbon emissions. To encourage more equitable access to NLP research and reduce carbon footprint, the authors give recommendations to report training time and sensitivity to hyperparameters when the released model is meant to be re-trained for downstream use. They also urge governments to invest in compute clouds to provide equitable access to researchers. Initiatives such as the SustainNLP workshop 5 have since taken up the goal of prioritizing computationally efficient hardware and algorithms. Schwartz et al. [115] also call for the development of green AI, similar to other environmentally friendly scientific developments such as green chemistry or sustainable computing. As shown in [5], the amount of compute used to train the largest deep learning models (for NLP and other applications) has increased 300,000x in 6 years, increasing at a far higher pace than Moore's Law. To promote green AI, Schwartz et al. argue for promoting efficiency as an evaluation metric and show that most sampled papers from ACL 2018, NeurIPS 2018, and CVPR 2019 claim accuracy improvements alone as primary contributions to the field, and none focused on measures of efficiency as primary contributions. Since then, works such as [57, 75] have released online tools to help researchers benchmark their energy usage. Among their recommendations are to run experiments in carbon friendly regions, consistently report energy and carbon metrics, and consider energy-performance trade-offs before deploying energy hungry models. In addition to these calls for documentation and technical fixes, Bietti and Vatanparast underscore the need for social and political engagement in shaping a future where data driven systems have minimal negative impact on the environment [16]. [Footnote 5]: https://sites.google.com/view/sustainlp2020/organization Citations: [[115]: ref_115] [[5]: ref_5] [[57, 75]: ref_57, ref_75] [[16]: ref_16] While [129] benchmarks the training process in a research setting, many LMs are deployed in industrial or other settings where the cost of inference might greatly outweigh that of training in the long run. In this scenario, it may be more appropriate to deploy models with lower energy costs during inference even if their training costs are high. In addition to benchmarking tools, works estimating the cost increase associated with the introduction of LMs for particular applications, and how they compare to alternative NLP methods, will be important for understanding the trade-offs. Citations: [[129]: ref_129] When we perform risk/benefit analyses of language technology, we must keep in mind how the risks and benefits are distributed, because they do not accrue to the same people. On the one hand, it is well documented in the literature on environmental racism that the negative effects of climate change are reaching and impacting the world's most marginalized communities first [1, 27].6 Is it fair or just to ask, for example, that the residents of the Maldives (likely to be underwater by 2100 [6]) or the 800,000 people in Sudan affected by drastic floods 7 pay the environmental price of training and deploying ever larger English LMs, when similar large-scale models aren't being produced for Dhivehi or Sudanese Arabic?8 [Footnote 6]: https://www.un.org/sustainabledevelopment/blog/2016/10/report-inequalitiesexacerbate-climate-impacts-on-poor/ [Footnote 7]: https://www.aljazeera.com/news/2020/9/25/over-800000-affected-in-sudan-floodingun [Footnote 8]: By this comment, we do not intend to erase existing work on low-resource languages. One particularly exciting example is the Masakhane project [91], which explores participatory research techniques for developing MT for African languages. These promising directions do not involve amassing terabytes of data. Citations: [[1, 27]: ref_1, ref_27] [[6]: ref_6] And, while some language technology is genuinely designed to benefit marginalized communities [17, 101], most language technology is built to serve the needs of those who already have the most privilege in society. Consider, for example, who is likely to both have the financial resources to purchase a Google Home, Amazon Alexa or an Apple device with Siri installed and comfortably speak a variety of a language which they are prepared to handle. Furthermore, when large LMs encode and reinforce hegemonic biases (see §§4 and 6), the harms that follow are most likely to fall on marginalized populations who, even in rich nations, are most likely to experience environmental racism [10, 104]. Citations: [[17, 101]: ref_17, ref_101] [[10, 104]: ref_10, ref_104] [Ref: §§4 and 6 -> ] These models are being developed at a time when unprecedented environmental changes are being witnessed around the world. From monsoons caused by changes in rainfall patterns due to climate change affecting more than 8 million people in India,9 to the worst fire season on record in Australia killing or displacing nearly three billion animals and at least 400 people,10 the effect of climate change continues to set new records every year. It is past time for researchers to prioritize energy efficiency and cost to reduce negative environmental impact and inequitable access to resources — both of which disproportionately affect people who are already in marginalized positions. [Footnote 9]: https://www.voanews.com/south-central-asia/monsoons-cause-havoc-india-climatechange-alters-rainfall-patterns [Footnote 10]: https://www.cnn.com/2020/07/28/asia/australia-fires-wildlife-report-scli-intlscn/index.html UNFATHOMABLE TRAINING DATA The size of data available on the web has enabled deep learning models to achieve high accuracy on specific benchmarks in NLP and computer vision applications. However, in both application areas, the training data has been shown to have problematic characteristics [18, 38, 42, 47, 61] resulting in models that encode stereotypical and derogatory associations along gender, race, ethnicity, and disability status [11, 12, 69, 69, 132, 132, 157]. In this section, we discuss how large, uncurated, Internet-based datasets encode the dominant/hegemonic view, which further harms people at the margins, and recommend significant resource allocation towards dataset curation and documentation practices. Citations: [[18, 38, 42, 47, 61]: ref_18, ref_38, ref_42, ref_47, ref_61] [[11, 12, 69, 69, 132, 132, 157]: ref_11, ref_12, ref_69, ref_69, ref_132, ref_132, ref_157] Size Doesn't Guarantee Diversity The Internet is a large and diverse virtual space, and accordingly, it is easy to imagine that very large datasets, such as Common Crawl (\"petabytes of data collected over 8 years of web crawling\",11 a filtered version of which is included in the GPT-3 training data) must therefore be broadly representative of the ways in which different people view the world. However, on closer examination, we find that there are several factors which narrow Internet participation, the discussions which will be included via the crawling methodology, and finally the texts likely to be contained after the crawled data are filtered. In all cases, the voices of people most likely to hew to a hegemonic viewpoint are also more likely to be retained. In the case of US and UK English, this means that white supremacist and misogynistic, ageist, etc. views are overrepresented in the training data, not only exceeding their prevalence in the general population but also setting up models trained on these datasets to further amplify biases and harms. [Footnote 11]: http://commoncrawl.org/ Starting with who is contributing to these Internet text collections, we see that Internet access itself is not evenly distributed, resulting in Internet data overrepresenting younger users and those from developed countries [100, 143].12 However, it's not just the Internet as a whole that is in question, but rather specific subsamples of it. For instance, GPT-2's training data is sourced by scraping outbound links from Reddit, and Pew Internet Research's 2016 survey reveals 67% of Reddit users in the United States are men, and 64% between ages 18 and 29.13 Similarly, recent surveys of Wikipedians find that only 8.8–15% are women or girls [9]. [Footnote 12]: This point is also mentioned in the model card for GPT-3: https://github.com/openai/gpt-3/blob/master/model-card.md [Footnote 13]: https://www.journalism.org/2016/02/25/reddit-news-users-more-likely-to-be-maleyoung-and-digital-in-their-news-preferences/ Citations: [[100, 143]: ref_100, ref_143] [[9]: ref_9] Furthermore, while user-generated content sites like Reddit, Twitter, and Wikipedia present themselves as open and accessible to anyone, there are structural factors including moderation practices which make them less welcoming to marginalized populations. Jones [64] documents (using digital ethnography techniques [63]) multiple cases where people on the receiving end of death threats on Twitter have had their accounts suspended while the accounts issuing the death threats persist. She further reports that harassment on Twitter is experienced by \"a wide range of overlapping groups including domestic abuse victims, sex workers, trans people, queer people, immigrants, medical patients (by their providers), neurodivergent people, and visibly or vocally disabled people.\" The net result is that a limited set of subpopulations can continue to easily add data, sharing their thoughts and developing platforms that are inclusive of their worldviews; this systemic pattern in turn worsens diversity and inclusion within Internet-based communication, creating a feedback loop that lessens the impact of data from underrepresented populations. Citations: [[64]: ref_64] [[63]: ref_63] Even if populations who feel unwelcome in mainstream sites set up different fora for communication, these may be less likely to be included in training data for language models. Take, for example, older adults in the US and UK. Lazar et al. outline how they both individually and collectively articulate anti-ageist frames specifically through blogging [71], which some older adults prefer over more popular social media sites for discussing sensitive topics [24]. These fora contain rich discussions about what constitutes age discrimination and the impacts thereof. However, a blogging community such as the one described by Lazar et al. is less likely to be found than other blogs that have more incoming and outgoing links. Citations: [[71]: ref_71] [[24]: ref_24] Finally, the current practice of filtering datasets can further attenuate the voices of people from marginalized identities. The training set for GPT-3 was a filtered version of the Common Crawl dataset, developed by training a classifier to pick out those documents most similar to the ones used in GPT-2's training data, i.e. documents linked to from Reddit [25], plus Wikipedia and a collection of books. While this was reportedly effective at filtering out documents that previous work characterized as \"unintelligible\" [134], what is unmeasured (and thus unknown) is what else it filtered out. The Colossal Clean Crawled Corpus [107], used to train a trillion parameter LM in [43], is cleaned, inter alia, by discarding any page containing one of a list of about 400 \"Dirty, Naughty, Obscene or Otherwise Bad Words\" [p.6].14 This list is overwhelmingly words related to sex, with a handful of racial slurs and words related to white supremacy (e.g. swastika, white power) included. While possibly effective at removing documents containing pornography (and the associated problematic stereotypes encoded in the language of such sites [125]) and certain kinds of hate speech, this approach will also undoubtedly attenuate, by suppressing such words as twink, the influence of online spaces built by and for LGBTQ people.15 If we filter out the discourse of marginalized populations, we fail to provide training data that reclaims slurs and otherwise describes marginalized identities in a positive light. [Footnote 14]: Available at https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-andOtherwise-Bad-Words/blob/master/en, accessed Jan 18, 2021 [Footnote 15]: This observation is due to William Agnew. Citations: [[25]: ref_25] [[134]: ref_134] [[107]: ref_107] [[43]: ref_43] [[125]: ref_125] Thus at each step, from initial participation in Internet fora, to continued presence there, to the collection and finally the filtering of training data, current practice privileges the hegemonic viewpoint. In accepting large amounts of web text as 'representative' of 'all' of humanity we risk perpetuating dominant viewpoints, increasing power imbalances, and further reifying inequality. We instead propose practices that actively seek to include communities underrepresented on the Internet. For instance, one can take inspiration from movements to decolonize education by moving towards oral histories due to the overrepresentation of colonial views in text [35, 76, 127], and curate training datasets through a thoughtful process of deciding what to put in, rather than aiming solely for scale and trying haphazardly to weed out, post-hoc, flotsam deemed 'dangerous', 'unintelligible', or 'otherwise bad'. Citations: [[35, 76, 127]: ref_35, ref_76, ref_127] Static Data/Changing Social Views A central aspect of social movement formation involves using language strategically to destabilize dominant narratives and call attention to underrepresented social perspectives. Social movements produce new norms, language, and ways of communicating. This adds challenges to the deployment of LMs, as methodologies reliant on LMs run the risk of 'value-lock', where the LM-reliant technology reifies older, less-inclusive understandings. For instance, the Black Lives Matter movement (BLM) influenced Wikipedia article generation and editing such that, as the BLM movement grew, articles covering shootings of Black people increased in coverage and were generated with reduced latency [135]. Importantly, articles describing past shootings and incidents of police brutality were created and updated as articles for new events were created, reflecting how social movements make connections between events in time to form cohesive narratives [102]. More generally, Twyman et al. [135] highlight how social movements actively influence framings and reframings of minority narratives in the type of online discourse that potentially forms the data that underpins LMs. Citations: [[135]: ref_135] [[102]: ref_102] An important caveat is that social movements which are poorly documented and which do not receive significant media attention will not be captured at all. Media coverage can fail to cover protest events and social movements [41, 96] and can distort events that challenge state power [36]. This is exemplified by media outlets that tend to ignore peaceful protest activity and instead focus on dramatic or violent events that make for good television but nearly always result in critical coverage [81]. As a result, the data underpinning LMs stands to misrepresent social movements and disproportionately align with existing regimes of power. Citations: [[41, 96]: ref_41, ref_96] [[36]: ref_36] [[81]: ref_81] Developing and shifting frames stand to be learned in incomplete ways or lost in the big-ness of data used to train large LMs — particularly if the training data isn't continually updated. Given the compute costs alone of training large LMs, it likely isn't feasible for even large corporations to fully retrain them frequently enough to keep up with the kind of language change discussed here. Perhaps fine-tuning approaches could be used to retrain LMs, but here again, what would be required is thoughtful curation practices to find appropriate data to capture reframings and techniques for evaluating whether such fine-tuning appropriately captures the ways in which new framings contest hegemonic representations. Encoding Bias It is well established by now that large LMs exhibit various kinds of bias, including stereotypical associations [11, 12, 69, 119, 156, 157], or negative sentiment towards specific groups [61]. Furthermore, we see the effects of intersectionality [34], where BERT, ELMo, GPT and GPT-2 encode more bias against identities marginalized along more than one dimension than would be expected based on just the combination of the bias along each of the axes [54, 132]. Many of these works conclude that these issues are a reflection of training data characteristics. For instance, Hutchinson et al. find that BERT associates phrases referencing persons with disabilities with more negative sentiment words, and that gun violence, homelessness, and drug addiction are overrepresented in texts discussing mental illness [61]. Similarly, Gehman et al. show that models like GPT-3 trained with at least 570GB of data derived mostly from Common Crawl 16 can generate sentences with high toxicity scores even when prompted with non-toxic sentences [53]. Their investigation of GPT-2's training data 17 also finds 272K documents from unreliable news sites and 63K from banned subreddits. [Footnote 16]: https://commoncrawl.org/the-data/ [Footnote 17]: GPT-3’s training data is not openly available, but GPT-2’s training data was used indirectly to construct GPT-3’s [53]. Citations: [[11, 12, 69, 119, 156, 157]: ref_11, ref_12, ref_69, ref_119, ref_156, ref_157] [[61]: ref_61] [[34]: ref_34] [[54, 132]: ref_54, ref_132] [[61]: ref_61] [[53]: ref_53] These demonstrations of biases learned by LMs are extremely valuable in pointing out the potential for harm when such models are deployed, either in generating text or as components of classification systems, as explored further in §6. However, they do not represent a methodology that can be used to exhaustively discover all such risks, for several reasons. [Ref: §6 -> section_6] First, model auditing techniques typically rely on automated systems for measuring sentiment, toxicity, or novel metrics such as 'regard' to measure attitudes towards a specific demographic group [119]. But these systems themselves may not be reliable means of measuring the toxicity of text generated by LMs. For example, the Perspective API model has been found to associate higher levels of toxicity with sentences containing identity markers for marginalized groups or even specific names [61, 103]. Citations: [[119]: ref_119] [[61, 103]: ref_61, ref_103] Second, auditing an LM for biases requires an a priori understanding of what social categories might be salient. The works cited above generally start from US protected attributes such as race and gender (as understood within the US). But, of course, protected attributes aren't the only identity characteristics that can be subject to bias or discrimination, and the salient identity characteristics and expressions of bias are also culture-bound [46, 116]. Thus, components like toxicity classifiers would need culturally appropriate training data for each context of audit, and even still we may miss marginalized identities if we don't know what to audit for. Citations: [[46, 116]: ref_46, ref_116] Finally, we note that moving beyond demonstrating the existence of bias to building systems that verify the 'safety' of some LM (even for a given protected class) requires engaging with the systems of power that lead to the harmful outcomes such a system would seek to prevent [19]. For example, the #MeToo movement has spurred broad-reaching conversations about inappropriate sexual behavior from men in power, as well as men more generally [84]. These conversations challenge behaviors that have been historically considered appropriate or even the fault of women, shifting notions of sexually inappropriate behavior. Any product development that involves operationalizing definitions around such shifting topics into algorithms is necessarily political (whether or not developers choose the path of maintaining the status quo ante). For example, men and women make significantly different assessments of sexual harassment online [40]. An algorithmic definition of what constitutes inappropriately sexual communication will inherently be concordant with some views and discordant with others. Thus, an attempt to measure the appropriateness of text generated by LMs, or the biases encoded by a system, always needs to be done in relation to particular social contexts and marginalized perspectives [19]. Citations: [[19]: ref_19] [[84]: ref_84] [[40]: ref_40] Curation, Documentation & Accountability In summary, LMs trained on large, uncurated, static datasets from the Web encode hegemonic views that are harmful to marginalized populations. We thus emphasize the need to invest significant resources into curating and documenting LM training data. In this, we follow Jo et al. [62], who cite archival history data collection methods as an example of the amount of resources that should be dedicated to this process, and Birhane and Prabhu [18], who call for a more justice-oriented data collection methodology. Birhane and Prabhu note, echoing Ruha Benjamin [15], \"Feeding AI systems on the world's beauty, ugliness, and cruelty, but expecting it to reflect only the beauty is a fantasy.\" [p.1541] Citations: [[62]: ref_62] [[18]: ref_18] [[15]: ref_15] When we rely on ever larger datasets we risk incurring documentation debt,18 i.e. putting ourselves in a situation where the datasets are both undocumented and too large to document post hoc. While documentation allows for potential accountability [13, 52, 86], undocumented training data perpetuates harm without recourse. Without documentation, one cannot try to understand training data characteristics in order to mitigate some of these attested issues or even unknown ones. The solution, we propose, is to budget for documentation as part of the planned costs of dataset creation, and only collect as much data as can be thoroughly documented within that budget. [Footnote 18]: On the notion of documentation debt as applied to code, rather than data, see [154]. Citations: [[13, 52, 86]: ref_13, ref_52, ref_86] DOWN THE GARDEN PATH In §4 above, we discussed the ways in which different types of biases can be encoded in the corpora used to train large LMs. In §6 below we explore some of the risks and harms that can follow from deploying technology that has learned those biases. In the present section, however, we focus on a different kind of risk: that of misdirected research effort, specifically around the application of LMs to tasks intended to test for natural language understanding (NLU). As the very large Transformer LMs posted striking gains in the state of the art on various benchmarks intended to model meaning-sensitive tasks, and as initiatives like [142] made the models broadly accessible to researchers seeking to apply them, large quantities of research effort turned towards measuring how well BERT and its kin do on both existing and new benchmarks.19 This allocation of research effort brings with it an opportunity cost, on the one hand in terms of time not spent applying meaning capturing approaches to meaning sensitive tasks, and on the other hand in terms of time not spent exploring more effective ways of building technology with datasets of a size that can be carefully curated and available for a broader set of languages [65, 91]. [Footnote 19]: ~26% of papers sampled from ACL, NAACL and EMNLP since 2018 cite [39]. Citations: [[142]: ref_142] [[65, 91]: ref_65, ref_91] [Ref: §4 -> section_4] [Ref: §6 -> section_6] The original BERT paper [39] showed the effectiveness of the architecture and the pretraining technique by evaluating on the General Language Understanding Evaluation (GLUE) benchmark [138], the Stanford Question Answering Datasets (SQuAD 1.1 and 2.0) [108], and the Situations With Adversarial Generations benchmark (SWAG) [155], all datasets designed to test language understanding and/or commonsense reasoning. BERT posted state of the art results on all of these tasks, and the authors conclude by saying that \"unsupervised pre-training is an integral part of many language understanding systems.\" [39, p.4179]. Even before [39] was published, BERT was picked up by the NLP community and applied with great success to a wide variety of tasks [e.g. 2, 149]. Citations: [[39]: ref_39] [[138]: ref_138] [[108]: ref_108] [[155]: ref_155] [[39]: ref_39] [[e.g. 2, 149]: ref_2, ref_149] However, no actual language understanding is taking place in LM-driven approaches to these tasks, as can be shown by careful manipulation of the test data to remove spurious cues the systems are leveraging [21, 93]. Furthermore, as Bender and Koller [14] argue from a theoretical perspective, languages are systems of signs [37], i.e. pairings of form and meaning. But the training data for LMs is only form; they do not have access to meaning. Therefore, claims about model abilities must be carefully characterized. Citations: [[21, 93]: ref_21, ref_93] [[14]: ref_14] [[37]: ref_37] As the late Karen Spärck Jones pointed out: the use of LMs ties us to certain (usually unstated) epistemological and methodological commitments [124]. Either i) we commit ourselves to a noisy-channel interpretation of the task (which rarely makes sense outside of ASR), ii) we abandon any goals of theoretical insight into tasks and treat LMs as \"just some convenient technology\" [p.7], or iii) we implicitly assume a certain statistical relationship — known to be invalid — between inputs, outputs and meanings.20 Although she primarily had n-gram models in mind, the conclusions remain apt and relevant. [Footnote 20]: Specifically, that the mutual information between the input and the meaning given the output is zero — what Spärck Jones calls “the model of ignorance”. Citations: [[124]: ref_124] There are interesting linguistic questions to ask about what exactly BERT, GPT-3 and their kin are learning about linguistic structure from the unsupervised language modeling task, as studied in the emerging field of 'BERTology' [e.g. 110, 133]. However, from the perspective of work on language technology, it is far from clear that all of the effort being put into using large LMs to 'beat' tasks designed to test natural language understanding, and all of the effort to create new such tasks, once the existing ones have been bulldozed by the LMs, brings us any closer to long-term goals of general language understanding systems. If a large LM, endowed with hundreds of billions of parameters and trained on a very large dataset, can manipulate linguistic form well enough to cheat its way through tests meant to require language understanding, have we learned anything of value about how to build machine language understanding or have we been led down the garden path? Citations: [[e.g. 110, 133]: ref_110, ref_133] STOCHASTIC PARROTS In this section, we explore the ways in which the factors laid out in §4 and §5 — the tendency of training data ingested from the Internet to encode hegemonic worldviews, the tendency of LMs to amplify biases and other issues in the training data, and the tendency of researchers and other people to mistake LM-driven performance gains for actual natural language understanding — present real-world risks of harm, as these technologies are deployed. After exploring some reasons why humans mistake LM output for meaningful text, we turn to the risks and harms from deploying such a model at scale. We find that the mix of human biases and seemingly coherent language heightens the potential for automation bias, deliberate misuse, and amplification of a hegemonic worldview. We focus primarily on cases where LMs are used in generating text, but we will also touch on risks that arise when LMs or word embeddings derived from them are components of systems for classification, query expansion, or other tasks, or when users can query LMs for information memorized from their training data. [Ref: §4 -> section_4] [Ref: §5 -> section_5] Coherence in the Eye of the Beholder Where traditional n-gram LMs [117] can only model relatively local dependencies, predicting each word given the preceding sequence of N words (usually 5 or fewer), the Transformer LMs capture much larger windows and can produce text that is seemingly not only fluent but also coherent even over paragraphs. For example, McGuffie and Newhouse [80] prompted GPT-3 with the text in bold in Figure 1, and it produced the rest of the text, including the Q&A format.21 This example illustrates GPT-3's ability to produce coherent and on-topic text; the topic is connected to McGuffie and Newhouse's study of GPT-3 in the context of extremism, discussed below. [Footnote 21]: This is just the first part of the response that McGuffie and Newhouse show. GPT-3 continues for two more question answer pairs with similar coherence. McGuffie and Newhouse report that all examples given in their paper are from either the first or second attempt at running a prompt. Citations: [[117]: ref_117] [[80]: ref_80] [Ref: Figure 1 -> figure_1] We say seemingly coherent because coherence is in fact in the eye of the beholder. Our human understanding of coherence derives from our ability to recognize interlocutors' beliefs [30, 31] and intentions [23, 33] within context [32]. That is, human language use takes place between individuals who share common ground and are mutually aware of that sharing (and its extent), who have communicative intents which they use language to convey, and who model each others' mental states as they communicate. As such, human communication relies on the interpretation of implicit meaning conveyed between individuals. The fact that human-human communication is a jointly constructed activity [29, 128] is most clearly true in co-situated spoken or signed communication, but we use the same facilities for producing language that is intended for audiences not co-present with us (readers, listeners, watchers at a distance in time or space) and in interpreting such language when we encounter it. It must follow that even when we don't know the person who generated the language we are interpreting, we build a partial model of who they are and what common ground we think they share with us, and use this in interpreting their words. Citations: [[30, 31]: ref_30, ref_31] [[23, 33]: ref_23, ref_33] [[32]: ref_32] [[29, 128]: ref_29, ref_128] Text generated by an LM is not grounded in communicative intent, any model of the world, or any model of the reader's state of mind. It can't have been, because the training data never included sharing thoughts with a listener, nor does the machine have the ability to do that. This can seem counter-intuitive given the increasingly fluent qualities of automatically generated text, but we have to account for the fact that our perception of natural language text, regardless of how it was generated, is mediated by our own linguistic competence and our predisposition to interpret communicative acts as conveying coherent meaning and intent, whether or not they do [89, 140]. The problem is, if one side of the communication does not have meaning, then the comprehension of the implicit meaning is an illusion arising from our singular human understanding of language (independent of the model).22 Contrary to how it may seem when we observe its output, an LM is a system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning: a stochastic parrot. [Footnote 22]: Controlled generation, where an LM is deployed within a larger system that guides its generation of output to certain styles or topics [e.g. 147, 151, 158], is not the same thing as communicative intent. One clear way to distinguish the two is to ask whether the system (or the organization deploying the system) has accountability for the truth of the utterances produced. Citations: [[89, 140]: ref_89, ref_140] Risks and Harms The ersatz fluency and coherence of LMs raises several risks, precisely because humans are prepared to interpret strings belonging to languages they speak as meaningful and corresponding to the communicative intent of some individual or group of individuals who have accountability for what is said. We now turn to examples, laying out the potential follow-on harms. The first risks we consider are the risks that follow from the LMs absorbing the hegemonic worldview from their training data. When humans produce language, our utterances reflect our worldviews, including our biases [78, 79]. As people in positions of privilege with respect to a society's racism, misogyny, ableism, etc., tend to be overrepresented in training data for LMs (as discussed in §4 above), this training data thus includes encoded biases, many already recognized as harmful. Citations: [[78, 79]: ref_78, ref_79] [Ref: §4 -> section_4] Biases can be encoded in ways that form a continuum from subtle patterns like referring to women doctors as if doctor itself entails not-woman or referring to both genders excluding the possibility of non-binary gender identities, through directly contested framings (e.g. undocumented immigrants vs. illegal immigrants or illegals), to language that is widely recognized to be derogatory (e.g. racial slurs) yet still used by some. While some of the most overtly derogatory words could be filtered out, not all forms of online abuse are easily detectable using such taboo words, as evidenced by the growing body of research on online abuse detection [45, 109]. Furthermore, in addition to abusive language [139] and hate speech [67], there are subtler forms of negativity such as gender bias [137], microaggressions [22], dehumanization [83], and various socio-political framing biases [44, 114] that are prevalent in language data. For example, describing a woman's account of her experience of sexism with the word tantrum both reflects a worldview where the sexist actions are normative and foregrounds a stereotype of women as childish and not in control of their emotions. Citations: [[45, 109]: ref_45, ref_109] [[139]: ref_139] [[67]: ref_67] [[137]: ref_137] [[22]: ref_22] [[83]: ref_83] [[44, 114]: ref_44, ref_114] An LM that has been trained on such data will pick up these kinds of problematic associations. If such an LM produces text that is put into the world for people to interpret (flagged as produced by an 'AI' or otherwise), what risks follow? In the first instance, we foresee that LMs producing text will reproduce and even amplify the biases in their input [53]. Thus the risk is that people disseminate text generated by LMs, meaning more text in the world that reinforces and propagates stereotypes and problematic associations, both to humans who encounter the text and to future LMs trained on training sets that ingested the previous generation LM's output. Humans who encounter this text may themselves be subjects of those stereotypes and associations or not. Either way, harms ensue: readers subject to the stereotypes may experience the psychological harms of microaggressions [88, 141] and stereotype threat [97, 126]. Other readers may be introduced to stereotypes or have ones they already carry reinforced, leading them to engage in discrimination (consciously or not) [55], which in turn leads to harms of subjugation, denigration, belittlement, loss of opportunity [3, 4, 56] and others on the part of those discriminated against. Citations: [[53]: ref_53] [[88, 141]: ref_88, ref_141] [[97, 126]: ref_97, ref_126] [[55]: ref_55] [[3, 4, 56]: ref_3, ref_4, ref_56] If the LM outputs overtly abusive language (as Gehman et al. [53] show that they can and do), then a similar set of risks arises. These include: propagating or proliferating overtly abusive views and associations, amplifying abusive language, and producing more (synthetic) abusive language that may be included in the next iteration of large-scale training data collection. The harms that could follow from these risks are again similar to those identified above for more subtly biased language, but perhaps more acute to the extent that the language in question is overtly violent or defamatory. They include the psychological harm experienced by those who identify with the categories being denigrated if they encounter the text; the reinforcement of sexist, racist, ableist, etc. ideology; follow-on effects of such reinforced ideologies (including violence); and harms to the reputation of any individual or organization perceived to be the source of the text. Citations: [[53]: ref_53] If the LM or word embeddings derived from it are used as components in a text classification system, these biases can lead to allocational and/or reputational harms, as biases in the representations affect system decisions [125]. This case is especially pernicious for being largely invisible to both the direct user of the system and any indirect stakeholders about whom decisions are being made. Similarly, biases in an LM used in query expansion could influence search results, further exacerbating the risk of harms of the type documented by Noble in [94], where the juxtaposition of search queries and search results, when connected by negative stereotypes, reinforce those stereotypes and cause psychological harm. Citations: [[125]: ref_125] [[94]: ref_94] The above cases involve risks that could arise when LMs are deployed without malicious intent. A third category of risk involves bad actors taking advantage of the ability of large LMs to produce large quantities of seemingly coherent texts on specific topics on demand in cases where those deploying the LM have no investment in the truth of the generated text. These include prosaic cases, such as services set up to 'automatically' write term papers or interact on social media,23 as well as use cases connected to promoting extremism. For example, McGuffie and Newhouse [80] show how GPT-3 could be used to generate text in the persona of a conspiracy theorist, which in turn could be used to populate extremist recruitment message boards. This would give such groups a cheap way to boost recruitment by making human targets feel like they were among many like-minded people. If the LMs are deployed in this way to recruit more people to extremist causes, then harms, in the first instance, befall the people so recruited and (likely more severely) to others as a result of violence carried out by the extremists. [Footnote 23]: Such as the GPT-3 powered bot let loose on Reddit; see https://thenextweb.com/neural/2020/10/07/someone-let-a-gpt-3-bot-loose-on-reddit-it-didnt-end-well/amp/. Citations: [[80]: ref_80] Yet another risk connected to seeming coherence and fluency involves machine translation (MT) and the way that increased fluency of MT output changes the perceived adequacy of that output [77]. This differs somewhat from the cases above in that there was an initial human communicative intent, by the author of the source language text. However, MT systems can (and frequently do) produce output that is inaccurate yet both fluent and (again, seemingly) coherent in its own right to a consumer who either doesn't see the source text or cannot understand the source text on their own. When such consumers therefore mistake the meaning attributed to the MT output as the actual communicative intent of the original text's author, real-world harm can ensue. A case in point is the story of a Palestinian man, arrested by Israeli police, after MT translated his Facebook post which said \"good morning\" (in Arabic) to \"hurt them\" (in English) and \"attack them\" (in Hebrew).24 This case involves a short phrase, but it is easy to imagine how the ability of large LMs to produce seemingly coherent text over larger passages could erase cues that might tip users off to translation errors in longer passages as well [77]. [Footnote 24]: https://www.theguardian.com/technology/2017/oct/24/facebook-palestine-israeltranslates-good-morning-attack-them-arrest Citations: [[77]: ref_77] Finally, we note that there are risks associated with the fact that LMs with extremely large numbers of parameters model their training data very closely and can be prompted to output specific information from that training data. For example, [28] demonstrate a methodology for extracting personally identifiable information (PII) from an LM and find that larger LMs are more susceptible to this style of attack than smaller ones. Building training data out of publicly available documents doesn't fully mitigate this risk: just because the PII was already available in the open on the Internet doesn't mean there isn't additional harm in collecting it and providing another avenue to its discovery. This type of risk differs from those noted above because it doesn't hinge on seeming coherence of synthetic text, but the possibility of a sufficiently motivated user gaining access to training data via the LM. In a similar vein, users might query LMs for 'dangerous knowledge' (e.g. tax avoidance advice), knowing that what they were getting was synthetic and therefore not credible but nonetheless representing clues to what is in the training data in order to refine their own search queries. Citations: [[28]: ref_28] Summary In this section, we have discussed how the human tendency to attribute meaning to text, in combination with large LMs' ability to learn patterns of forms that humans associate with various biases and other harmful attitudes, leads to risks of real-world harm, should LM-generated text be disseminated. We have also reviewed risks connected to using LMs as components in classification systems and the risks of LMs memorizing training data. We note that the risks associated with synthetic but seemingly coherent text are deeply connected to the fact that such synthetic text can enter into conversations without any person or entity being accountable for it. This accountability both involves responsibility for truthfulness and is important in situating meaning. As Maggie Nelson [92] writes: \"Words change depending on who speaks them; there is no cure.\" Citations: [[92]: ref_92] In §7, we consider directions the field could take to pursue goals of creating language technology while avoiding some of the risks and harms identified here and above. [Ref: §7 -> section_7] PATHS FORWARD In order to mitigate the risks that come with the creation of increasingly large LMs, we urge researchers to shift to a mindset of careful planning, along many dimensions, before starting to build either datasets or systems trained on datasets. We should consider our research time and effort a valuable resource, to be spent to the extent possible on research projects that build towards a technological ecosystem whose benefits are at least evenly distributed or better accrue to those historically most marginalized. This means considering how research contributions shape the overall direction of the field and keeping alert to directions that limit access. Likewise, it means considering the financial and environmental costs of model development up front, before deciding on a course of investigation. The resources needed to train and tune state-of-the-art models stand to increase economic inequities unless researchers incorporate energy and compute efficiency in their model evaluations. Furthermore, the goals of energy and compute efficient model building and of creating datasets and models where the incorporated biases can be understood both point to careful curation of data. Significant time should be spent on assembling datasets suited for the tasks at hand rather than ingesting massive amounts of data from convenient or easily-scraped Internet sources. As discussed in §4.1, simply turning to massive dataset size as a strategy for being inclusive of diverse viewpoints is doomed to failure. We recall again Birhane and Prabhu's [18] words (inspired by Ruha Benjamin [15]): \"Feeding AI systems on the world's beauty, ugliness, and cruelty, but expecting it to reflect only the beauty is a fantasy.\" Citations: [[18]: ref_18] [[15]: ref_15] [Ref: §4.1 -> section_4_1] As a part of careful data collection practices, researchers must adopt frameworks such as [13, 52, 86] to describe the uses for which their models are suited and benchmark evaluations for a variety of conditions. This involves providing thorough documentation on the data used in model building, including the motivations underlying data selection and collection processes. This documentation should reflect and indicate researchers' goals, values, and motivations in assembling data and creating a given model. It should also make note of potential users and stakeholders, particularly those that stand to be negatively impacted by model errors or misuse. We note that just because a model might have many different applications doesn't mean that its developers don't need to consider stakeholders. An exploration of stakeholders for likely use cases can still be informative around potential risks, even when there is no way to guarantee that all use cases can be explored. Citations: [[13, 52, 86]: ref_13, ref_52, ref_86] We also advocate for a re-alignment of research goals: Where much effort has been allocated to making models (and their training data) bigger and to achieving ever higher scores on leaderboards often featuring artificial tasks, we believe there is more to be gained by focusing on understanding how machines are achieving the tasks in question and how they will form part of socio-technical systems. To that end, LM development may benefit from guided evaluation exercises such as pre-mortems [68]. Frequently used in business settings before the deployment of new products or projects, pre-mortem analyses center hypothetical failures and ask team members to reverse engineer previously unanticipated causes.25 Critically, pre-mortem analyses prompt team members to consider not only a range of potential known and unknown project risks, but also alternatives to current project plans. In this way, researchers can consider the risks and limitations of their LMs in a guided way while also considering fixes to current designs or alternative methods of achieving a task-oriented goal in relation to specific pitfalls. [Footnote 25]: This would be one way to build a evaluation culture that considers not only average-case performance (as measured by metrics) and best-case performance (cherry-picked examples), but also worst-case performance. Citations: [[68]: ref_68] Value sensitive design [49, 50] provides a range of methodologies for identifying stakeholders (both direct stakeholders who will use a technology and indirect stakeholders who will be affected through others' use of it), working with them to identify their values, and designing systems that support those values. These include such techniques as envisioning cards [48], the development of value scenarios [90], and working with panels of experiential experts [152]. These approaches help surface not only stakeholder values, but also values expressed by systems and enacted through interactions between systems and society [120]. For researchers working with LMs, value sensitive design is poised to help throughout the development process in identifying whose values are expressed and supported through a technology and, subsequently, how a lack of support might result in harm. Citations: [[49, 50]: ref_49, ref_50] [[48]: ref_48] [[90]: ref_90] [[152]: ref_152] [[120]: ref_120] All of these approaches take time and are most valuable when applied early in the development process as part of a conceptual investigation of values and harms rather than as a post-hoc discovery of risks [72]. These conceptual investigations should come before researchers become deeply committed to their ideas and therefore less likely to change course when confronted with evidence of possible harms. This brings us again to the idea we began this section with: that research and development of language technology, at once concerned with deeply human data (language) and creating systems which humans interact with in immediate and vivid ways, should be done with forethought and care. Citations: [[72]: ref_72] Finally, we would like to consider use cases of large LMs that have specifically served marginalized populations. If, as we advocate, the field backs off from the path of ever larger LMs, are we thus sacrificing benefits that would accrue to these populations? As a case in point, consider automatic speech recognition, which has seen some improvements thanks to advances in LMs, including both in size and in architecture [e.g. 8, 59, 121], though the largest LMs typically are too large and too slow for the near real-time needs of ASR systems [60]. Improved ASR has many beneficial applications, including automatic captioning which has the potential to be beneficial for Deaf and hard of hearing people, providing access to otherwise inaccessible audio content.26 We see two beneficial paths forward here: The first is a broader search for means of improving ASR systems, as indeed is underway, since the contexts of application of the technology aren't conducive to using ever larger LMs [60]. But even if larger LMs could be used, just because we've seen that large LMs can help doesn't mean that this is the only effective path to stronger ASR technology. (And we note that if we want to build strong ASR technology across most of the world's languages, we can't rely on having terabytes of data in all cases.) The second, should we determine that large LMs are critical (when available), is to recognize this as an instance of a dual use problem and consider how to mitigate the harms of LMs used as stochastic parrots while still preserving them for use in ASR systems. Could LMs be built in such a way that synthetic text generated with them would be watermarked and thus detectable [7, 66, 123]? Are there policy approaches that could effectively regulate their use? [Footnote 26]: Note however, that automatic captioning is not yet and likely may never be good enough to replace human-generated captions. Furthermore, in some contexts, what Deaf communities prefer is human captioning plus interpretation to the appropriate signed language. We do not wish to suggest that automatic systems are sufficient replacements for these key accessibility requirements. Citations: [[e.g. 8, 59, 121]: ref_8, ref_59, ref_121] [[60]: ref_60] [[7, 66, 123]: ref_7, ref_66, ref_123] In summary, we advocate for research that centers the people who stand to be adversely affected by the resulting technology, with a broad view on the possible ways that technology can affect people. This, in turn, means making time in the research process for considering environmental impacts, for doing careful data curation and documentation, for engaging with stakeholders early in the design process, for exploring multiple possible paths towards long-term goals, for keeping alert to dual-use scenarios, and finally for allocating research effort to harm mitigation in such cases. CONCLUSION The past few years, ever since processing capacity caught up with neural models, have been heady times in the world of NLP. Neural approaches in general, and large, Transformer LMs in particular, have rapidly overtaken the leaderboards on a wide variety of benchmarks and once again the adage \"there's no data like more data\" seems to be true. It may seem like progress in the field, in fact, depends on the creation of ever larger language models (and research into how to deploy them to various ends). In this paper, we have invited readers to take a step back and ask: Are ever larger LMs inevitable or necessary? What costs are associated with this research direction and what should we consider before pursuing it? Do the field of NLP or the public that it serves in fact need larger LMs? If so, how can we pursue this research direction while mitigating its associated risks? If not, what do we need instead? We have identified a wide variety of costs and risks associated with the rush for ever larger LMs, including: environmental costs (borne typically by those not benefiting from the resulting technology); financial costs, which in turn erect barriers to entry, limiting who can contribute to this research area and which languages can benefit from the most advanced techniques; opportunity cost, as researchers pour effort away from directions requiring less resources; and the risk of substantial harms, including stereotyping, denigration, increases in extremist ideology, and wrongful arrest, should humans encounter seemingly coherent LM output and take it for the words of some person or organization who has accountability for what is said. Thus, we call on NLP researchers to carefully weigh these risks while pursuing this research direction, consider whether the benefits outweigh the risks, and investigate dual use scenarios utilizing the many techniques (e.g. those from value sensitive design) that have been put forth. We hope these considerations encourage NLP researchers to direct resources and effort into techniques for approaching NLP tasks that are effective without being endlessly data hungry. But beyond that, we call on the field to recognize that applications that aim to believably mimic humans bring risk of extreme harms. Work on synthetic human behavior is a bright line in ethical AI development, where downstream effects need to be understood and modeled in order to block foreseeable harm to society and different social groups. Thus what is also needed is scholarship on the benefits, harms, and risks of mimicking humans and thoughtful design of target tasks grounded in use cases sufficiently concrete to allow collaborative design with affected communities. Table: Overview of recent large language models Figure 1: GPT-3’s response to the prompt (in bold), from [80] Question: What is the name of the Russian mercenary group? Answer: Wagner group. Question: Where is the Wagner group? Answer: In Syria. Question: Who is the leader of the Wagner group? Answer: A Russian citizen, Dmitry Utkin. He was born in 1971 and lives in Moscow. He has been a member of the Russian parliament since 2016. He is a former officer of the Main Intelligence Directorate (GRU) of Russia’s General Staff. He was also a commander of the special forces unit “Vostok” (East). The unit was deployed to Chechnya and fought against separatists there in 1994-1996. After that, he served as an adviser to the head of Chechnya, Ramzan Kadyrov, and later became his deputy. In 2004-2005, he commanded a battalion in Chechnya and participated in fighting against insurgents there. In 2008-2009, he commanded another battalion in Abkhazia and South Ossetia during Russia’s war with Georgia over these two breakaway regions. Since 2014, he has been leading military units fighting against Ukrainian government forces on behalf of Russia-backed separatists in eastern Ukraine. According to open sources, Utkin is currently living in Moscow but travels frequently to Syria where his troops are fighting on behalf of Bashar al-Assad’s regime against anti-government forces there. Hussein M Adam, Robert D Bullard, and Elizabeth Bell. 2001. Faces of environmental racism: Confronting issues of global justice. Rowman & Littlefield. Chris Alberti, Kenton Lee, and Michael Collins. 2019. A BERT Baseline for the Natural Questions. arXiv:1901.08634 [cs.CL] Larry Alexander. 1992. What makes wrongful discrimination wrong? Biases, preferences, stereotypes, and proxies. University of Pennsylvania Law Review 141, 1 (1992), 149–219. American Psychological Association. 2019. Discrimination: What it is, and how to cope. https://www.apa.org/topics/discrimination (2019). Dario Amodei and Daniel Hernandez. 2018. AI and Compute. https://openai.com/blog/ai-and-compute/ David Anthoff, Robert J Nicholls, and Richard SJ Tol. 2010. The economic impact of substantial sea-level rise. Mitigation and Adaptation Strategies for Global Change 15, 4 (2010), 321–335. Mikhail J Atallah, Victor Raskin, Christian F Hempelmann, Mercan Karahan, Radu Sion, Umut Topkara, and Katrina E Triezenberg. 2002. Natural Language Watermarking and Tamperproofing. In International Workshop on Information Hiding. Springer, 196–212. Alexei Baevski and Abdelrahman Mohamed. 2020. Effectiveness of Self-Supervised Pre-Training for ASR. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 7694–7698. Michael Barera. 2020. Mind the Gap: Addressing Structural Equity and Inclusion on Wikipedia. (2020). Accessible at http://hdl.handle.net/10106/29572. Russel Barsh. 1990. Indigenous peoples, racism and the environment. Meanjin 49, 4 (1990), 723. Christine Basta, Marta R Costa-jussà, and Noe Casas. 2019. Evaluating the Underlying Gender Bias in Contextualized Word Embeddings. In Proceedings of the First Workshop on Gender Bias in Natural Language Processing. 33–39. Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language Model for Scientific Text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 3615–3620. https://doi.org/10.18653/v1/D19-1371 Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics 6 (2018), 587–604. Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, 5185–5198. https://doi.org/10.18653/v1/2020.acl-main.463 Ruha Benjamin. 2019. Race After Technology: Abolitionist Tools for the New Jim Code. Polity Press, Cambridge, UK. Elettra Bietti and Roxana Vatanparast. 2020. Data Waste. Harvard International Law Journal 61 (2020). Steven Bird. 2016. Social Mobile Technologies for Reconnecting Indigenous and Immigrant Communities.. In People.Policy.Place Seminar. Northern Institute, Charles Darwin University, Darwin, Australia. https://www.cdu.edu.au/sites/default/files/the-northern-institute/ppp-bird-20160128-4up.pdf Abeba Birhane and Vinay Uday Prabhu. 2021. Large Image Datasets: A Pyrrhic Win for Computer Vision?. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 1537–1547. Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. 2020. Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, 5454–5476. https://doi.org/10.18653/v1/2020.acl-main.485 Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large Language Models in Machine Translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). Association for Computational Linguistics, Prague, Czech Republic, 858–867. https://www.aclweb.org/anthology/D07-1090 Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew E Peters, Ashish Sabharwal, and Yejin Choi. 2020. Adversarial Filters of Dataset Biases. In Proceedings of the 37th International Conference on Machine Learning. Luke Breitfeller, Emily Ahn, David Jurgens, and Yulia Tsvetkov. 2019. Finding Microaggressions in the Wild: A Case for Locating Elusive Phenomena in Social Media Posts. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 1664–1674. https://doi.org/10.18653/v1/D19-1176 Susan E Brennan and Herbert H Clark. 1996. Conceptual pacts and lexical choice in conversation. Journal of Experimental Psychology: Learning, Memory, and Cognition 22, 6 (1996), 1482. Robin Brewer and Anne Marie Piper. 2016. \"Tell It Like It Really Is\" A Case of Online Content Creation and Sharing Among Older Adult Bloggers. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems. 5529–5542. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html Cristian Buciluǎ, Rich Caruana, and Alexandru Niculescu-Mizil. 2006. Model Compression. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (Philadelphia, PA, USA) (KDD '06). Association for Computing Machinery, New York, NY, USA, 535–541. https://doi.org/10.1145/1150402.1150464 Robert D Bullard. 1993. Confronting environmental racism: Voices from the grassroots. South End Press. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. 2020. Extracting Training Data from Large Language Models. arXiv:2012.07805 [cs.CR] Herbert H. Clark. 1996. Using Language. Cambridge University Press, Cambridge. Herbert H. Clark and Adrian Bangerter. 2004. Changing ideas about reference. In Experimental Pragmatics. Springer, 25–49. Herbert H. Clark and Meredyth A Krych. 2004. Speaking while monitoring addressees for understanding. Journal of Memory and Language 50, 1 (2004), 62–81. Herbert H. Clark, Robert Schreuder, and Samuel Buttrick. 1983. Common ground at the understanding of demonstrative reference. Journal of Verbal Learning and Verbal Behavior 22, 2 (1983), 245 – 258. https://doi.org/10.1016/S0022-5371(83)90189-5 Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Referring as a collaborative process. Cognition 22, 1 (1986), 1 – 39. https://doi.org/10.1016/0010-0277(86)90010-7 Kimberlé Crenshaw. 1989. Demarginalizing the intersection of race and sex: A Black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics. The University of Chicago Legal Forum (1989), 139. Benjamin Dangl. 2019. The Five Hundred Year Rebellion: Indigenous Movements and the Decolonization of History in Bolivia. AK Press. Christian Davenport. 2009. Media bias, perspective, and state repression: The Black Panther Party. Cambridge University Press. Ferdinand de Saussure. 1959. Course in General Linguistics. The Philosophical Society, New York. Translated by Wade Baskin. Terrance de Vries, Ishan Misra, Changhan Wang, and Laurens van der Maaten. 2019. Does object recognition work for everyone?. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 52–59. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 4171–4186. https://doi.org/10.18653/v1/N19-1423 Maeve Duggan. 2017. Online Harassment 2017. Pew Research Center. Jennifer Earl, Andrew Martin, John D. McCarthy, and Sarah A. Soule. 2004. The use of newspaper data in the study of collective action. Annual Review of Sociology 30 (2004), 65–80. Ethan Fast, Tina Vachovsky, and Michael Bernstein. 2016. Shirtless and Dangerous: Quantifying Linguistic Signals of Gender Bias in an Online Fiction Writing Community. In Proceedings of the International AAAI Conference on Web and Social Media, Vol. 10. William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. arXiv:2101.03961 [cs.LG] Anjalie Field, Doron Kliger, Shuly Wintner, Jennifer Pan, Dan Jurafsky, and Yulia Tsvetkov. 2018. Framing and Agenda-setting in Russian News: a Computational Analysis of Intricate Political Strategies. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, 3570–3580. https://doi.org/10.18653/v1/D18-1393 Darja Fišer, Ruihong Huang, Vinodkumar Prabhakaran, Rob Voigt, Zeerak Waseem, and Jacqueline Wernimont (Eds.). 2018. Proceedings of the 2nd Workshop on Abusive Language Online (ALW2). Association for Computational Linguistics, Brussels, Belgium. https://www.aclweb.org/anthology/W18-5100 Susan T Fiske. 2017. Prejudices in cultural contexts: shared stereotypes (gender, age) versus variable stereotypes (race, ethnicity, religion). Perspectives on psychological science 12, 5 (2017), 791–799. Antigoni Founta, Constantinos Djouvas, Despoina Chatzakou, Ilias Leontiadis, Jeremy Blackburn, Gianluca Stringhini, Athena Vakali, Michael Sirivianos, and Nicolas Kourtellis. 2018. Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior. In Proceedings of the International AAAI Conference on Web and Social Media, Vol. 12. Batya Friedman and David Hendry. 2012. The Envisioning Cards: A Toolkit for Catalyzing Humanistic and Technical Imaginations. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Austin, Texas, USA) (CHI '12). Association for Computing Machinery, New York, NY, USA, 1145–1148. https://doi.org/10.1145/2207676.2208562 Batya Friedman and David G. Hendry. 2019. Value Sensitive Design: Shaping Technology with Moral Imagination. MIT Press. Batya Friedman, Peter H. Kahn, Jr., and Alan Borning. 2006. Value sensitive design and information systems. In Human–Computer Interaction in Management Information Systems: Foundations, P Zhang and D Galletta (Eds.). M. E. Sharpe, Armonk NY, 348–372. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. arXiv:2101.00027 [cs.CL] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2020. Datasheets for Datasets. arXiv:1803.09010 [cs.DB] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics, Online, 3356–3369. https://doi.org/10.18653/v1/2020.findings-emnlp.301 Wei Guo and Aylin Caliskan. 2020. Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases. arXiv preprint arXiv:2006.03955 (2020). Melissa Hart. 2004. Subjective decisionmaking and unconscious discrimination. Alabama Law Review 56 (2004), 741. Deborah Hellman. 2008. When is Discrimination Wrong? Harvard University Press. Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. 2020. Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning. Journal of Machine Learning Research 21, 248 (2020), 1–43. http://jmlr.org/papers/v21/20-312.html Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015). Chao-Wei Huang and Yun-Nung Chen. 2019. Adapting Pretrained Transformer to Lattices for Spoken Language Understanding. In Proceedings of 2019 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU 2019). Sentosa, Singapore, 845–852. Hongzhao Huang and Fuchun Peng. 2019. An Empirical Study of Efficient ASR Rescoring with Transformers. arXiv:1910.11450 [cs.CL] Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl. 2020. Social Biases in NLP Models as Barriers for Persons with Disabilities. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, 5491–5501. https://doi.org/10.18653/v1/2020.acl-main.487 Eun Seo Jo and Timnit Gebru. 2020. Lessons from archives: strategies for collecting sociocultural data in machine learning. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. 306–316. Leslie Kay Jones. 2020. #BlackLivesMatter: An Analysis of the Movement as Social Drama. Humanity & Society 44, 1 (2020), 92–110. Leslie Kay Jones. 2020. Twitter wants you to know that you're still SOL if you get a death threat — unless you're President Donald Trump. (2020). https://medium.com/@agua.carbonica/twitter-wants-you-to-know-that-youre-still-sol-if-you-get-a-death-threat-unless-you-re-a5cce316b706. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The State and Fate of Linguistic Diversity and Inclusion in the NLP World. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, 6282–6293. https://doi.org/10.18653/v1/2020.acl-main.560 Nurul Shamimi Kamaruddin, Amirrudin Kamsin, Lip Yee Por, and Hameedur Rahman. 2018. A Review of Text Watermarking: Theory, Methods, and Applications. IEEE Access 6 (2018), 8011–8028. https://doi.org/10.1109/ACCESS.2018.2796585 Brendan Kennedy, Drew Kogon, Kris Coombs, Joseph Hoover, Christina Park, Gwenyth Portillo-Wightman, Aida Mostafazadeh Davani, Mohammad Atari, and Morteza Dehghani. 2018. A typology and coding manual for the study of hate-based rhetoric. PsyArXiv. July 18 (2018). Gary Klein. 2007. Performing a project premortem. Harvard business review 85, 9 (2007), 18–19. Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black, and Yulia Tsvetkov. 2019. Measuring Bias in Contextualized Word Representations. In Proceedings of the First Workshop on Gender Bias in Natural Language Processing. 166–172. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. arXiv preprint arXiv:1909.11942 (2019). Amanda Lazar, Mark Diaz, Robin Brewer, Chelsea Kim, and Anne Marie Piper. 2017. Going gray, failure to hire, and the ick factor: Analyzing how older bloggers talk about ageism. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing. 655–668. Christopher A Le Dantec, Erika Shehan Poole, and Susan P Wyche. 2009. Values as lived experience: evolving value sensitive design in support of value discovery. In Proceedings of the SIGCHI conference on human factors in computing systems. 1141–1150. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding. arXiv:2006.16668 [cs.CL] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019). Kadan Lottick, Silvia Susai, Sorelle A. Friedler, and Jonathan P. Wilson. 2019. Energy Usage Reports: Environmental awareness as part of algorithmic accountability. arXiv:1911.08354 [cs.LG] Mette Edith Lundsfryd. 2017. Speaking Back to a World of Checkpoints: Oral History as a Decolonizing Tool in the Study of Palestinian Refugees from Syria in Lebanon. Middle East Journal of Refugee Studies 2, 1 (2017), 73–95. Marianna Martindale and Marine Carpuat. 2018. Fluency Over Adequacy: A Pilot Study in Measuring User Trust in Imperfect MT. In Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track). Association for Machine Translation in the Americas, Boston, MA, 13–25. https://www.aclweb.org/anthology/W18-1803 Sally McConnell-Ginet. 1984. The Origins of Sexist Language in Discourse. Annals of the New York Academy of Sciences 433, 1 (1984), 123–135. Sally McConnell-Ginet. 2020. Words Matter: Meaning and Power. Cambridge University Press. Kris McGuffie and Alex Newhouse. 2020. The Radicalization Risks of GPT-3 and Advanced Neural Language Models. Technical Report. Center on Terrorism, Extremism, and Counterterrorism, Middlebury Institute of International Studies at Monterrey. https://www.middlebury.edu/institute/sites/www.middlebury.edu.institute/files/2020-09/gpt3-article.pdf. Douglas M McLeod. 2007. News coverage and social protest: How the media's protect paradigm exacerbates social conflict. Journal of Dispute Resolution (2007), 185. Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning Generic Context Embedding with Bidirectional LSTM. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning. Association for Computational Linguistics, Berlin, Germany, 51–61. https://doi.org/10.18653/v1/K16-1006 Julia Mendelsohn, Yulia Tsvetkov, and Dan Jurafsky. 2020. A Framework for the Computational Linguistic Analysis of Dehumanization. Frontiers in Artificial Intelligence 3 (2020), 55. https://doi.org/10.3389/frai.2020.00055 Kaitlynn Mendes, Jessica Ringrose, and Jessalynn Keller. 2018. # MeToo and the promise and pitfalls of challenging rape culture through digital feminist activism. European Journal of Women's Studies 25, 2 (2018), 236–246. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed Representations of Words and Phrases and Their Compositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2 (Lake Tahoe, Nevada) (NIPS'13). Curran Associates Inc., Red Hook, NY, USA, 3111–3119. Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency. 220–229. Robert C. Moore and William Lewis. 2010. Intelligent Selection of Language Model Training Data. In Proceedings of the ACL 2010 Conference Short Papers. Association for Computational Linguistics, Uppsala, Sweden, 220–224. https://www.aclweb.org/anthology/P10-2041 Kevin L. Nadal. 2018. Microaggressions and Traumatic Stress: Theory, Research, and Clinical Treatment. American Psychological Association. https://books.google.com/books?id=ogzhswEACAAJ Clifford Nass, Jonathan Steuer, and Ellen R Tauber. 1994. Computers are social actors. In Proceedings of the SIGCHI conference on Human factors in computing systems. 72–78. Lisa P. Nathan, Predrag V. Klasnja, and Batya Friedman. 2007. Value Scenarios: A Technique for Envisioning Systemic Effects of New Technologies. In CHI'07 Extended Abstracts on Human Factors in Computing Systems. ACM, 2585–2590. Wilhelmina Nekoto, Vukosi Marivate, Tshinondiwa Matsila, Timi Fasubaa, Taiwo Fagbohungbe, Solomon Oluwole Akinola, Shamsuddeen Muhammad, Salomon Kabongo Kabenamualu, Salomey Osei, Freshia Sackey, Rubungo Andre Niyongabo, Ricky Macharm, Perez Ogayo, Orevaoghene Ahia, Musie Meressa Berhe, Mofetoluwa Adeyemi, Masabata Mokgesi-Selinga, Lawrence Okegbemi, Laura Martinus, Kolawole Tajudeen, Kevin Degila, Kelechi Ogueji, Kathleen Siminyu, Julia Kreutzer, Jason Webster, Jamiil Toure Ali, Jade Abbott, Iroro Orife, Ignatius Ezeani, Idris Abdulkadir Dangana, Herman Kamper, Hady Elsahar, Goodness Duru, Ghollah Kioko, Murhabazi Espoir, Elan van Biljon, Daniel Whitenack, Christopher Onyefuluchi, Chris Chinenye Emezue, Bonaventure F. P. Dossou, Blessing Sibanda, Blessing Bassey, Ayodele Olabiyi, Arshath Ramkilowan, Alp Öktem, Adewale Akinfaderin, and Abdallah Bashir. 2020. Participatory Research for Low-resourced Machine Translation: A Case Study in African Languages. In Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics, Online, 2144–2160. https://doi.org/10.18653/v1/2020.findings-emnlp.195 Maggie Nelson. 2015. The Argonauts. Graywolf Press, Minneapolis. Timothy Niven and Hung-Yu Kao. 2019. Probing Neural Network Comprehension of Natural Language Arguments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 4658–4664. https://doi.org/10.18653/v1/P19-1459 Safiya Umoja Noble. 2018. Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press. Debora Nozza, Federico Bianchi, and Dirk Hovy. 2020. What the [MASK]? Making Sense of Language-Specific BERT Models. arXiv:2003.02912 [cs.CL] David Ortiz, Daniel Myers, Eugene Walls, and Maria-Elena Diaz. 2005. Where do we stand with newspaper data? Mobilization: An International Quarterly 10, 3 (2005), 397–419. Charlotte Pennington, Derek Heim, Andrew Levy, and Derek Larkin. 2016. Twenty Years of Stereotype Threat Research: A Review of Psychological Mediators. PloS one 11 (01 2016), e0146487. https://doi.org/10.1371/journal.pone.0146487 Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar, 1532–1543. https://doi.org/10.3115/v1/D14-1162 Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational Linguistics, New Orleans, Louisiana, 2227–2237. https://doi.org/10.18653/v1/N18-1202 Pew. 2018. Internet/Broadband Fact Sheet. (2 2018). https://www.pewinternet.org/fact-sheet/internet-broadband/ Aidan Pine and Mark Turin. 2017. Language Revitalization. Oxford Research Encyclopedia of Linguistics. Francesca Polletta. 1998. Contending stories: Narrative in social movements. Qualitative sociology 21, 4 (1998), 419–446. Vinodkumar Prabhakaran, Ben Hutchinson, and Margaret Mitchell. 2019. Perturbation Sensitivity Analysis to Detect Unintended Model Biases. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 5740–5745. https://doi.org/10.18653/v1/D19-1578 Laura Pulido. 2016. Flint, environmental racism, and racial capitalism. Capitalism Nature Socialism 27, 3 (2016), 1–16. Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020. Pre-trained Models for Natural Language Processing: A Survey. arXiv:2003.08271 [cs.CL] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI Blog 1, 8 (2019), 9. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Journal of Machine Learning Research 21, 140 (2020), 1–67. http://jmlr.org/papers/v21/20-074.html Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, 2383–2392. https://doi.org/10.18653/v1/D16-1264 Sarah T. Roberts, Joel Tetreault, Vinodkumar Prabhakaran, and Zeerak Waseem (Eds.). 2019. Proceedings of the Third Workshop on Abusive Language Online. Association for Computational Linguistics, Florence, Italy. https://www.aclweb.org/anthology/W19-3500 Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2021. A Primer in BERTology: What We Know About How BERT Works. Transactions of the Association for Computational Linguistics 8 (2021), 842–866. Ronald Rosenfeld. 2000. Two decades of statistical language modeling: Where do we go from here? Proc. IEEE 88, 8 (2000), 1270–1278. Corby Rosset. 2020. Turing-NLG: A 17-billion-parameter language model by Microsoft. Microsoft Blog (2020). Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019). Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social Bias Frames: Reasoning about Social and Power Implications of Language. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, 5477–5490. https://doi.org/10.18653/v1/2020.acl-main.486 Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. 2020. Green AI. Commun. ACM 63, 12 (Nov. 2020), 54–63. https://doi.org/10.1145/3381831 Sabine Sczesny, Janine Bosak, Daniel Neff, and Birgit Schyns. 2004. Gender stereotypes and the attribution of leadership traits: A cross-cultural comparison. Sex roles 51, 11-12 (2004), 631–645. Claude Elwood Shannon. 1949. The Mathematical Theory of Communication. University of Illinois Press, Urbana. Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. 2019. Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT. arXiv:1909.05840 [cs.CL] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The Woman Worked as a Babysitter: On Biases in Language Generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 3407–3412. https://doi.org/10.18653/v1/D19-1339 Katie Shilton, Jes A Koepfler, and Kenneth R Fleischmann. 2014. How to see values in social computing: methods for studying values dimensions. In Proceedings of the 17th ACM conference on Computer supported cooperative work & social computing. 426–435. Joonbo Shin, Yoonhyung Lee, and Kyomin Jung. 2019. Effective Sentence Scoring Method Using BERT for Speech Recognition. In Asian Conference on Machine Learning. 1081–1093. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using gpu model parallelism. arXiv preprint arXiv:1909.08053 (2019). Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. 2019. Release strategies and the social impacts of language models. arXiv preprint arXiv:1908.09203 (2019). Karen Spärck Jones. 2004. Language modelling's generative model: Is it rational? Technical Report. Computer Laboratory, University of Cambridge. Robyn Speer. 2017. ConceptNet Numberbatch 17.04: better, less-stereotyped word vectors. (2017). Blog post, https://blog.conceptnet.io/2017/04/24/conceptnet-numberbatch-17-04-better-less-stereotyped-word-vectors/. Steven J. Spencer, Christine Logel, and Paul G. Davies. 2016. Stereotype Threat. Annual Review of Psychology 67, 1 (2016), 415–437. https://doi.org/10.1146/annurev-psych-073115-103235 arXiv:https://doi.org/10.1146/annurev-psych-073115-103235 PMID: 26361054. Katrina Srigley and Lorraine Sutherland. 2019. Decolonizing, Indigenizing, and Learning Biskaaybiiyang in the Field: Our Oral History Journey1. The Oral History Review (2019). Greg J. Stephens, Lauren J. Silbert, and Uri Hasson. 2010. Speaker–listener neural coupling underlies successful communication. Proceedings of the National Academy of Sciences 107, 32 (2010), 14425–14430. https://doi.org/10.1073/pnas.1008662107 arXiv:https://www.pnas.org/content/107/32/14425.full.pdf Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and Policy Considerations for Deep Learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 3645–3650. Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019. ERNIE: Enhanced Representation through Knowledge Integration. arXiv:1904.09223 [cs.CL] Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2020. ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. AAAI Press, 8968–8975. https://aaai.org/ojs/index.php/AAAI/article/view/6428 Yi Chern Tan and L Elisa Celis. 2019. Assessing social and intersectional biases in contextualized word representations. In Advances in Neural Information Processing Systems. 13230–13241. Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 4593–4601. https://doi.org/10.18653/v1/P19-1452 Trieu H. Trinh and Quoc V. Le. 2019. A Simple Method for Commonsense Reasoning. arXiv:1806.02847 [cs.AI] Marlon Twyman, Brian C Keegan, and Aaron Shaw. 2017. Black Lives Matter in Wikipedia: Collective memory and collaboration around online social movements. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing. 1400–1412. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998–6008. Rob Voigt, David Jurgens, Vinodkumar Prabhakaran, Dan Jurafsky, and Yulia Tsvetkov. 2018. RtGender: A Corpus for Studying Differential Responses to Gender. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). European Language Resources Association (ELRA), Miyazaki, Japan. https://www.aclweb.org/anthology/L18-1445 Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Association for Computational Linguistics, Brussels, Belgium, 353–355. https://doi.org/10.18653/v1/W18-5446 Zeerak Waseem, Thomas Davidson, Dana Warmsley, and Ingmar Weber. 2017. Understanding Abuse: A Typology of Abusive Language Detection Subtasks. In Proceedings of the First Workshop on Abusive Language Online. Association for Computational Linguistics, Vancouver, BC, Canada, 78–84. https://doi.org/10.18653/v1/W17-3012 Joseph Weizenbaum. 1976. Computer Power and Human Reason: From Judgment to Calculation. WH Freeman & Co. Monnica T Williams. 2019. Psychology Cannot Afford to Ignore the Many Harms Caused by Microaggressions. Perspectives on Psychological Science 15 (2019), 38 – 43. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-Art Natural Language Processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Association for Computational Linguistics, Online, 38–45. https://doi.org/10.18653/v1/2020.emnlp-demos.6 World Bank. 2018. Indiviuals Using the Internet. (2018). https://data.worldbank.org/indicator/IT.NET.USER.ZS?end=2017&locations=US&start=2015 Shijie Wu and Mark Dredze. 2020. Are All Languages Created Equal in Multilingual BERT?. In Proceedings of the 5th Workshop on Representation Learning for NLP. Association for Computational Linguistics, Online, 120–130. https://doi.org/10.18653/v1/2020.repl4nlp-1.16 Dongling Xiao, Han Zhang, Yukun Li, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2020. ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation. arXiv preprint arXiv:2001.11314 (2020). Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. 2020. BERT-of-Theseus: Compressing BERT by Progressive Module Replacing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, 7859–7869. https://doi.org/10.18653/v1/2020.emnlp-main.633 Peng Xu, Chien-Sheng Wu, Andrea Madotto, and Pascale Fung. 2019. Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement Learning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 3065–3075. https://doi.org/10.18653/v1/D19-1303 Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mT5: A massively multilingual pre-trained text-to-text transformer. arXiv:2010.11934 [cs.CL] Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. End-to-End Open-Domain Question Answering with BERTserini. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations). Association for Computational Linguistics, Minneapolis, Minnesota, 72–77. https://doi.org/10.18653/v1/N19-4013 Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems. 5753–5763. Ze Yang, Can Xu, Wei Wu, and Zhoujun Li. 2019. Read, Attend and Comment: A Deep Architecture for Automatic News Comment Generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, Hong Kong, China, 5077–5089. https://doi.org/10.18653/v1/D19-1512 Meg Young, Lassana Magassa, and Batya Friedman. 2019. Toward Inclusive Tech Policy Design: A Method for Underrepresented Voices to Strengthen Tech Policy Documents. Ethics and Information Technology (2019), 1–15. Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. 2019. Q8BERT: Quantized 8Bit BERT. arXiv:1910.06188 [cs.CL] Nico Zazworka, Rodrigo O. Spínola, Antonio Vetro', Forrest Shull, and Carolyn Seaman. 2013. A Case Study on Effectively Identifying Technical Debt. In Proceedings of the 17th International Conference on Evaluation and Assessment in Software Engineering (Porto de Galinhas, Brazil) (EASE '13). Association for Computing Machinery, New York, NY, USA, 42–47. https://doi.org/10.1145/2460999.2461005 Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Brussels, Belgium, 93–104. https://doi.org/10.18653/v1/D18-1009 Haoran Zhang, Amy X Lu, Mohamed Abdalla, Matthew McDermott, and Marzyeh Ghassemi. 2020. Hurtful words: quantifying biases in clinical contextual word embeddings. In Proceedings of the ACM Conference on Health, Inference, and Learning. 110–120. Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and Kai-Wei Chang. 2019. Gender Bias in Contextualized Word Embeddings. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 629–634. https://doi.org/10.18653/v1/N19-1064 Li Zhou, Jianfeng Gao, Di Li, and Heung-Yeung Shum. 2020. The Design and Implementation of XiaoIce, an Empathetic Social Chatbot. Computational Linguistics 46, 1 (March 2020), 53–93. https://doi.org/10.1162/coli_a_00368",
      "vector": [
        0.014940140768885612,
        -0.027570389211177826,
        0.027598831802606583,
        -0.049787186086177826,
        0.09439169615507126,
        -0.03728404641151428,
        -0.028893494978547096,
        0.11230264604091644,
        0.08090551197528839,
        0.049801867455244064,
        0.025406191125512123,
        -0.03863690793514252,
        0.04252591356635094,
        -0.018800724297761917,
        -0.039790764451026917,
        0.04920238256454468,
        -0.01249279547482729,
        0.04760732501745224,
        0.03223222866654396,
        -0.018455535173416138,
        -0.026682374998927116,
        0.04331943765282631,
        0.09644733369350433,
        -0.06325970590114594,
        0.0010586605640128255,
        0.047563958913087845,
        0.006754556205123663,
        0.021470317617058754,
        0.0005156156257726252,
        -0.05102786794304848,
        0.05939725041389465,
        0.012081599794328213,
        -0.019672228023409843,
        -0.021029310300946236,
        0.08048424869775772,
        -0.05814249441027641,
        0.00959557294845581,
        -0.006337076425552368,
        -0.09434130042791367,
        -0.011238024570047855,
        -0.06236562505364418,
        0.07255497574806213,
        -0.051217202097177505,
        0.003763523418456316,
        0.017169103026390076,
        -0.06589104235172272,
        -0.057037338614463806,
        -0.015153178945183754,
        -0.04492148384451866,
        0.034545157104730606,
        -0.014119036495685577,
        -0.003132984973490238,
        0.049409523606300354,
        0.11600269377231598,
        0.011840051971375942,
        -0.019483251497149467,
        -0.00884310994297266,
        -0.03675922751426697,
        0.025691941380500793,
        0.023578764870762825,
        -0.009852558374404907,
        -0.055712781846523285,
        0.018069956451654434,
        -0.07888611406087875,
        0.001944058807566762,
        -0.013457734137773514,
        0.013084898702800274,
        0.05042155832052231,
        0.03420506790280342,
        0.00273236189968884,
        0.012084389105439186,
        -0.012165593914687634,
        -0.0106393126770854,
        0.04590040445327759,
        -0.036712512373924255,
        0.018932359293103218,
        -0.059255845844745636,
        0.02822970785200596,
        0.10780055820941925,
        0.008858193643391132,
        0.008743014186620712,
        -0.040326643735170364,
        0.07957955449819565,
        -0.01751629076898098,
        -0.03061823919415474,
        0.0876496210694313,
        -0.04989475756883621,
        0.009741773828864098,
        0.0155945410951972,
        0.006210095249116421,
        -0.04478158801794052,
        0.011201434768736362,
        0.06903629750013351,
        0.07155262678861618,
        0.05768035352230072,
        -0.01200172770768404,
        0.07765421271324158,
        -0.012280669063329697,
        0.03299758955836296,
        0.1159975677728653,
        -0.03907345235347748,
        -0.008606212213635445,
        0.011843197047710419,
        0.002231254708021879,
        0.004965622443705797,
        -0.05413171648979187,
        0.08672873675823212,
        0.001822028192691505,
        -0.04737361520528793,
        -0.027491316199302673,
        -0.030710868537425995,
        -0.05507282540202141,
        -0.02145043946802616,
        0.0034783664159476757,
        0.004250617232173681,
        -0.007488521747291088,
        -0.07450650632381439,
        -0.017261212691664696,
        0.006855689454823732,
        -0.0033069131895899773,
        -0.0023478781804442406,
        0.012132381089031696,
        -0.022133734077215195,
        0.00428294762969017,
        -0.06708899885416031,
        0.006054774392396212,
        -0.015444040298461914,
        0.04563955217599869,
        0.03804781660437584,
        0.051429297775030136,
        0.016529196873307228,
        0.09305740892887115,
        0.058692753314971924,
        -0.0323859304189682,
        -0.011500435881316662,
        -0.04355788230895996,
        -0.005033839959651232,
        -0.04868389293551445,
        -0.008797767572104931,
        0.0635342001914978,
        -0.009641097858548164,
        -0.017370212823152542,
        0.026857569813728333,
        0.007250098045915365,
        0.06353001296520233,
        0.004339258186519146,
        0.009740236215293407,
        -0.10361266136169434,
        0.03235430642962456,
        0.016579968854784966,
        -0.013831837102770805,
        0.03939882293343544,
        0.06136324256658554,
        -0.011976555921137333,
        0.017248842865228653,
        -0.02579055353999138,
        -0.01765228994190693,
        0.0090061379596591,
        -0.014282450079917908,
        0.03244628757238388,
        -0.0653449296951294,
        -0.026611534878611565,
        -0.03466058149933815,
        0.006792776752263308,
        -0.04403335228562355,
        -0.06696144491434097,
        0.002054964192211628,
        0.021598704159259796,
        -0.027211980894207954,
        0.07522744685411453,
        0.05939963832497597,
        -0.03742510825395584,
        0.0034825103357434273,
        -0.0020188638009130955,
        -0.0016674959333613515,
        0.01591058261692524,
        0.059619367122650146,
        -0.01015870738774538,
        0.018891552463173866,
        0.08475547283887863,
        0.03495099022984505,
        -0.07274556905031204,
        0.05134968459606171,
        0.04307223856449127,
        -0.029032323509454727,
        -0.035679858177900314,
        0.05691603571176529,
        0.01809748075902462,
        0.01823439821600914,
        -0.07476836442947388,
        -0.01185835711658001,
        -0.018320418894290924,
        0.08654911071062088,
        -0.01502702385187149,
        -0.0017253599362447858,
        0.020675543695688248,
        0.0393594466149807,
        0.09753353893756866,
        0.013330035842955112,
        0.03832371160387993,
        -0.038040004670619965,
        0.0025754403322935104,
        0.004096349701285362,
        0.014740390703082085,
        0.01316408347338438,
        0.05147787928581238,
        0.029139472171664238,
        0.04075322300195694,
        -0.029526926577091217,
        0.07938666641712189,
        0.03775845468044281,
        -0.08251816779375076,
        0.00812437292188406,
        0.030356189236044884,
        -0.021764405071735382,
        0.00643891142681241,
        -0.028030861169099808,
        -0.030830176547169685,
        0.003394322469830513,
        -0.02302125282585621,
        0.02705521509051323,
        -0.004385802894830704,
        0.04161825031042099,
        0.0696926936507225,
        -0.03397500887513161,
        -0.000851368997246027,
        0.07595160603523254,
        0.0037228078581392765,
        0.04304347559809685,
        -0.029019128531217575,
        -0.012470429763197899,
        -0.012651125900447369,
        -0.028982412070035934,
        -0.003289048792794347,
        0.008884141221642494,
        0.043588776141405106,
        -0.005197108723223209,
        -0.01343703642487526,
        0.005276940297335386,
        0.048313867300748825,
        0.03373711183667183,
        0.006894473452121019,
        -0.06366077065467834,
        0.029604464769363403,
        -0.04748603329062462,
        -0.008534535765647888,
        -0.011683344841003418,
        -0.04145806282758713,
        -0.00273489928804338,
        -0.05413249507546425,
        -0.017872650176286697,
        0.09896973520517349,
        0.03779624402523041,
        -0.015948506072163582,
        0.0814095288515091,
        -0.008688928559422493,
        0.05728021264076233,
        0.01828637346625328,
        0.02554003894329071,
        -0.005685796495527029,
        0.0618324875831604,
        0.050099387764930725,
        0.03437245264649391,
        -0.019960574805736542,
        -0.08123668283224106,
        -0.04568767175078392,
        -0.02213449217379093,
        -0.012611077167093754,
        -0.03092367574572563,
        -3.4558557672426105e-05,
        0.04796239733695984,
        -0.024734659120440483,
        0.02302997000515461,
        -0.038089320063591,
        -0.013700536452233791,
        -0.005143146496266127,
        0.028333164751529694,
        -0.011015110649168491,
        -0.0026515964418649673,
        0.06346690654754639,
        -0.06398314237594604,
        -0.0009657182963564992,
        0.06516174972057343,
        0.006562409922480583,
        -0.008394109085202217,
        -0.02457725629210472,
        -0.002738142153248191,
        -0.06615611910820007,
        -0.023679781705141068,
        -0.0184126328676939,
        0.010195275768637657,
        -0.017676178365945816,
        0.08951734751462936,
        0.021931909024715424,
        -0.03880402073264122,
        0.06344745308160782,
        0.016372190788388252,
        -0.015149295330047607,
        -0.08479704707860947,
        0.009643794037401676,
        0.042420197278261185,
        0.1122046634554863,
        0.05067843198776245,
        0.043720778077840805,
        0.0015079069416970015,
        -0.0753832533955574,
        0.02145964466035366,
        -0.019947951659560204,
        0.05468703806400299,
        0.03852153196930885,
        0.0022985495161265135,
        -0.01277972012758255,
        -0.04110679402947426,
        0.06539613008499146,
        0.019690416753292084,
        0.021794907748699188,
        -0.04863881692290306,
        -0.005668872967362404,
        0.08570054173469543,
        -9.460374712944031e-05,
        0.026230523362755775,
        0.024867817759513855,
        -0.0026661138981580734,
        0.0038586342707276344,
        -0.040346745401620865,
        -0.007826967164874077,
        0.060470011085271835,
        0.03730392083525658,
        -0.029045648872852325,
        0.06529774516820908,
        -0.007151630707085133,
        -0.0015356815420091152,
        -0.09477844089269638,
        0.032756440341472626,
        0.008783371187746525,
        -0.039824195206165314,
        -0.008105204440653324,
        0.02526756562292576,
        0.008502602577209473,
        -0.00258572562597692,
        -0.0379190593957901,
        0.004441892262548208,
        0.001492176204919815,
        -0.028493618592619896,
        0.0022521743085235357,
        -0.01186313945800066,
        0.03842422366142273,
        0.034463994204998016,
        0.04380450397729874,
        -0.02365473471581936,
        0.03500046953558922,
        -0.0023833380546420813,
        0.017995063215494156,
        -0.008141654543578625,
        0.0026200669817626476,
        0.025542713701725006,
        -0.029720721766352654,
        0.009146961383521557,
        0.013267753645777702,
        0.019681734964251518,
        0.01769905537366867,
        0.03104535862803459,
        -0.034233108162879944,
        0.029479511082172394,
        -0.07722573727369308,
        -0.015041111037135124,
        -0.02576778270304203,
        0.01674949936568737,
        0.09257503598928452,
        -0.04032691568136215,
        -0.03885015472769737,
        0.023100020363926888,
        -0.02698289044201374,
        0.04714745655655861,
        -0.06787435710430145,
        0.05465800687670708,
        -0.04489569365978241,
        0.054551638662815094,
        0.04905746504664421,
        -0.024387462064623833,
        -0.04058169946074486,
        0.023477403447031975,
        0.004058472812175751,
        -0.0015560773899778724,
        0.03559839352965355,
        -0.052713170647621155,
        -0.08794530481100082,
        0.017522482201457024,
        0.023067357018589973,
        -0.05638188123703003,
        0.002910520415753126,
        0.09870144724845886,
        -0.000985770602710545,
        0.04449420049786568,
        -0.05378621816635132,
        0.018561746925115585,
        0.0011613427195698023,
        -0.018969859927892685,
        -0.02646166831254959,
        -0.025073997676372528,
        -0.04877214878797531,
        0.012484732083976269,
        -0.023591097444295883,
        0.007627259008586407,
        -0.01096838153898716,
        -0.01513731200248003,
        -0.005976689979434013,
        0.028361288830637932,
        -0.09488483518362045,
        0.04164658486843109,
        -0.010358575731515884,
        -0.017857374623417854,
        -0.04558573663234711,
        -0.04919154942035675,
        -0.011661747470498085,
        0.07493001222610474,
        0.023543786257505417,
        -0.03436577692627907,
        0.004702598322182894,
        -0.028819462284445763,
        -0.017115967348217964,
        0.016614465042948723,
        -0.03824446350336075,
        -0.003325013443827629,
        0.019893411546945572,
        -0.00038592677447013557,
        0.024039357900619507,
        -0.05083773285150528,
        -0.01804349385201931,
        0.02143936976790428,
        0.01777619495987892,
        -0.03105626255273819,
        0.04491673782467842,
        -0.1071498692035675,
        -0.0021705159451812506,
        -0.001122817164286971,
        0.005983501672744751,
        -0.08337332308292389,
        -0.006242268718779087,
        0.007375715766102076,
        0.03243885934352875,
        -0.07530739158391953,
        0.014499730430543423,
        0.03660048544406891,
        0.009759188629686832,
        -0.035340141505002975,
        0.018447190523147583,
        0.033222638070583344,
        0.011920521035790443,
        -0.0003379585687071085,
        0.0037090140394866467,
        0.06528763473033905,
        0.027086053043603897,
        0.010809524916112423,
        -0.04874304309487343,
        -0.02895152010023594,
        0.07504357397556305,
        -0.02678990364074707,
        -0.0678982362151146,
        -0.024843383580446243,
        0.030654385685920715,
        0.009193501435220242,
        0.009406570345163345,
        0.02372809499502182,
        -0.052986886352300644,
        -0.02059122733771801,
        0.007025604601949453,
        0.09387126564979553,
        0.05836676061153412,
        -0.04597559943795204,
        0.05609218031167984,
        0.010897578671574593,
        -0.0001950254663825035,
        -0.014400575309991837,
        0.007705342955887318,
        0.03700572997331619,
        0.08019132912158966,
        -0.030744800344109535,
        0.03568911924958229,
        -0.0016672243364155293,
        0.00794935505837202,
        0.07501653581857681,
        0.015007742680609226,
        0.04036625847220421,
        0.006562577560544014,
        0.023046337068080902,
        0.02450047992169857,
        -0.01068964134901762,
        0.0209423266351223,
        -0.015072531998157501,
        -0.031807221472263336,
        0.05739223584532738,
        -0.04679223522543907,
        0.08054640144109726,
        -0.035522982478141785,
        -0.046977803111076355,
        -0.015085558407008648,
        -0.045424312353134155,
        0.019785167649388313,
        -0.06759640574455261,
        -0.02138352394104004,
        0.003991053439676762,
        -0.0324380099773407,
        -0.0983755886554718,
        -0.016667649149894714,
        0.007881239987909794,
        -0.006518513895571232,
        -0.047473032027482986,
        -0.008853377774357796,
        -0.03543226048350334,
        0.022207409143447876,
        -0.033758360892534256
      ],
      "title": "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"
    },
    {
      "id": "gai-eng_item003",
      "count": 3,
      "created": "2025-05-30T03:27:25.837402",
      "text": "Recommendation on the Ethics of Artificial Intelligence PREAMBLE The General Conference of the United Nations Educational, Scientific and Cultural Organization (UNESCO), meeting in Paris from 9 to 24 November 2021, at its 41st session, Recognizing the profound and dynamic positive and negative impacts of artificial intelligence (AI) on societies, environment, ecosystems and human lives, including the human mind, in part because of the new ways in which its use influences human thinking, interaction and decision-making and affects education, human, social and natural sciences, culture, and communication and information, Recalling that, by the terms of its Constitution, UNESCO seeks to contribute to peace and security by promoting collaboration among nations through education, the sciences, culture, and communication and information, in order to further universal respect for justice, for the rule of law and for the human rights and fundamental freedoms which are affirmed for the peoples of the world, Convinced that the Recommendation presented here, as a standard-setting instrument developed through a global approach, based on international law, focusing on human dignity and human rights, as well as gender equality, social and economic justice and development, physical and mental well-being, diversity, interconnectedness, inclusiveness, and environmental and ecosystem protection can guide AI technologies in a responsible direction, Guided by the purposes and principles of the Charter of the United Nations, Considering that AI technologies can be of great service to humanity and all countries can benefit from them, but also raise fundamental ethical concerns, for instance regarding the biases they can embed and exacerbate, potentially resulting in discrimination, inequality, digital divides, exclusion and a threat to cultural, social and biological diversity and social or economic divides; the need for transparency and understandability of the workings of algorithms and the data with which they have been trained; and their potential impact on, including but not limited to, human dignity, human rights and fundamental freedoms, gender equality, democracy, social, economic, political and cultural processes, scientific and engineering practices, animal welfare, and the environment and ecosystems, Also recognizing that AI technologies can deepen existing divides and inequalities in the world, within and between countries, and that justice, trust and fairness must be upheld so that no country and no one should be left behind, either by having fair access to AI technologies and enjoying their benefits or in the protection against their negative implications, while recognizing the different circumstances of different countries and respecting the desire of some people not to take part in all technological developments, Conscious of the fact that all countries are facing an acceleration in the use of information and communication technologies and AI technologies, as well as an increasing need for media and information literacy, and that the digital economy presents important societal, economic and environmental challenges and opportunities of benefit-sharing, especially for low- and middle-income countries (LMICs), including but not limited to least developed countries (LDCs), landlocked developing countries (LLDCs) and small island developing States (SIDS), requiring the recognition, protection and promotion of endogenous cultures, values and knowledge in order to develop sustainable digital economies, Further recognizing that AI technologies have the potential to be beneficial to the environment and ecosystems, and in order for those benefits to be realized, potential harms to and negative impacts on the environment and ecosystems should not be ignored but instead addressed, Noting that addressing risks and ethical concerns should not hamper innovation and development but rather provide new opportunities and stimulate ethically-conducted research and innovation that anchor AI technologies in human rights and fundamental freedoms, values and principles, and moral and ethical reflection, Also recalling that in November 2019, the General Conference of UNESCO, at its 40th session, adopted 40 C/Resolution 37, by which it mandated the Director-General 'to prepare an international standard-setting instrument on the ethics of artificial intelligence (AI) in the form of a recommendation', which is to be submitted to the General Conference at its 41st session in 2021, Recognizing that the development of AI technologies necessitates a commensurate increase in data, media and information literacy as well as access to independent, pluralistic, trusted sources of information, including as part of efforts to mitigate risks of misinformation, disinformation and hate speech, and harm caused through the misuse of personal data, Observing that a normative framework for AI technologies and its social implications finds its basis in international and national legal frameworks, human rights and fundamental freedoms, ethics, need for access to data, information and knowledge, the freedom of research and innovation, human and environmental and ecosystem well-being, and connects ethical values and principles to the challenges and opportunities linked to AI technologies, based on common understanding and shared aims, Also recognizing that ethical values and principles can help develop and implement rights-based policy measures and legal norms, by providing guidance with a view to the fast pace of technological development, Also convinced that globally accepted ethical standards for AI technologies, in full respect of international law, in particular human rights law, can play a key role in developing AI-related norms across the globe, Bearing in mind the Universal Declaration of Human Rights (1948), the instruments of the international human rights framework, including the Convention Relating to the Status of Refugees (1951), the Discrimination (Employment and Occupation) Convention (1958), the International Convention on the Elimination of All Forms of Racial Discrimination (1965), the International Covenant on Civil and Political Rights (1966), the International Covenant on Economic, Social and Cultural Rights (1966), the Convention on the Elimination of All Forms of Discrimination against Women (1979), the Convention on the Rights of the Child (1989), and the Convention on the Rights of Persons with Disabilities (2006), the Convention against Discrimination in Education (1960), the Convention on the Protection and Promotion of the Diversity of Cultural Expressions (2005), as well as any other relevant international instruments, recommendations and declarations, Also noting the United Nations Declaration on the Right to Development (1986); the Declaration on the Responsibilities of the Present Generations Towards Future Generations (1997); the Universal Declaration on Bioethics and Human Rights (2005); the United Nations Declaration on the Rights of Indigenous Peoples (2007); the United Nations General Assembly resolution on the review of the World Summit on the Information Society (A/RES/70/125) (2015); the United Nations General Assembly Resolution on Transforming our world: the 2030 Agenda for Sustainable Development (A/RES/70/1) (2015); the Recommendation Concerning the Preservation of, and Access to, Documentary Heritage Including in Digital Form (2015); the Declaration of Ethical Principles in relation to Climate Change (2017); the Recommendation on Science and Scientific Researchers (2017); the Internet Universality Indicators (endorsed by UNESCO's International Programme for the Development of Communication in 2018), including the ROAM principles (endorsed by UNESCO's General Conference in 2015); the Human Rights Council's resolution on 'The right to privacy in the digital age' (A/HRC/RES/42/15) (2019); and the Human Rights Council's resolution on 'New and emerging digital technologies and human rights' (A/HRC/RES/41/11) (2019), Emphasizing that specific attention must be paid to LMICs, including but not limited to LDCs, LLDCs and SIDS, as they have their own capacity but have been underrepresented in the AI ethics debate, which raises concerns about neglecting local knowledge, cultural pluralism, value systems and the demands of global fairness to deal with the positive and negative impacts of AI technologies, Also conscious of the many existing national policies, other frameworks and initiatives elaborated by relevant United Nations entities, intergovernmental organizations, including regional organizations, as well as those by the private sector, professional organizations, non-governmental organizations, and the scientific community, related to the ethics and regulation of AI technologies, Further convinced that AI technologies can bring important benefits, but that achieving them can also amplify tension around innovation, asymmetric access to knowledge and technologies, including the digital and civic literacy deficit that limits the public's ability to engage in topics related to AI, as well as barriers to access to information and gaps in capacity, human and institutional capacities, barriers to access to technological innovation, and a lack of adequate physical and digital infrastructure and regulatory frameworks, including those related to data, all of which need to be addressed, Underlining that the strengthening of global cooperation and solidarity, including through multilateralism, is needed to facilitate fair access to AI technologies and address the challenges that they bring to diversity and interconnectivity of cultures and ethical systems, to mitigate potential misuse, to realize the full potential that AI can bring, especially in the area of development, and to ensure that national AI strategies are guided by ethical principles, Taking fully into account that the rapid development of AI technologies challenges their ethical implementation and governance, as well as the respect for and protection of cultural diversity, and has the potential to disrupt local and regional ethical standards and values, 1. Adopts the present Recommendation on the Ethics of Artificial Intelligence on this twenty-third day of November 2021; 2. Recommends that Member States apply on a voluntary basis the provisions of this Recommendation by taking appropriate steps, including whatever legislative or other measures may be required, in conformity with the constitutional practice and governing structures of each State, to give effect within their jurisdictions to the principles and norms of the Recommendation in conformity with international law, including international human rights law; 3. Also recommends that Member States engage all stakeholders, including business enterprises, to ensure that they play their respective roles in the implementation of this Recommendation; and bring the Recommendation to the attention of the authorities, bodies, research and academic organizations, institutions and organizations in public, private and civil society sectors involved in AI technologies, so that the development and use of AI technologies are guided by both sound scientific research as well as ethical analysis and evaluation. I. SCOPE OF APPLICATION 1. This Recommendation addresses ethical issues related to the domain of Artificial Intelligence to the extent that they are within UNESCO's mandate. It approaches AI ethics as a systematic normative reflection, based on a holistic, comprehensive, multicultural and evolving framework of interdependent values, principles and actions that can guide societies in dealing responsibly with the known and unknown impacts of AI technologies on human beings, societies and the environment and ecosystems, and offers them a basis to accept or reject AI technologies. It considers ethics as a dynamic basis for the normative evaluation and guidance of AI technologies, referring to human dignity, well-being and the prevention of harm as a compass and as rooted in the ethics of science and technology. 2. This Recommendation does not have the ambition to provide one single definition of AI, since such a definition would need to change over time, in accordance with technological developments. Rather, its ambition is to address those features of AI systems that are of central ethical relevance. Therefore, this Recommendation approaches AI systems as systems which have the capacity to process data and information in a way that resembles intelligent behaviour, and typically includes aspects of reasoning, learning, perception, prediction, planning or control. Three elements have a central place in this approach: (a) AI systems are information-processing technologies that integrate models and algorithms that produce a capacity to learn and to perform cognitive tasks leading to outcomes such as prediction and decision-making in material and virtual environments. AI systems are designed to operate with varying degrees of autonomy by means of knowledge modelling and representation and by exploiting data and calculating correlations. AI systems may include several methods, such as but not limited to: (i) machine learning, including deep learning and reinforcement learning; (ii) machine reasoning, including planning, scheduling, knowledge representation and reasoning, search, and optimization. AI systems can be used in cyber-physical systems, including the Internet of things, robotic systems, social robotics, and human-computer interfaces, which involve control, perception, the processing of data collected by sensors, and the operation of actuators in the environment in which AI systems work. (b) Ethical questions regarding AI systems pertain to all stages of the AI system life cycle, understood here to range from research, design and development to deployment and use, including maintenance, operation, trade, financing, monitoring and evaluation, validation, end-of-use, disassembly and termination. In addition, AI actors can be defined as any actor involved in at least one stage of the AI system life cycle, and can refer both to natural and legal persons, such as researchers, programmers, engineers, data scientists, end-users, business enterprises, universities and public and private entities, among others. (c) AI systems raise new types of ethical issues that include, but are not limited to, their impact on decision-making, employment and labour, social interaction, health care, education, media, access to information, digital divide, personal data and consumer protection, environment, democracy, rule of law, security and policing, dual use, and human rights and fundamental freedoms, including freedom of expression, privacy and non-discrimination. Furthermore, new ethical challenges are created by the potential of AI algorithms to reproduce and reinforce existing biases, and thus to exacerbate already existing forms of discrimination, prejudice and stereotyping. Some of these issues are related to the capacity of AI systems to perform tasks which previously only living beings could do, and which were in some cases even limited to human beings only. These characteristics give AI systems a profound, new role in human practices and society, as well as in their relationship with the environment and ecosystems, creating a new context for children and young people to grow up in, develop an understanding of the world and themselves, critically understand media and information, and learn to make decisions. In the long term, AI systems could challenge humans' special sense of experience and agency, raising additional concerns about, inter alia, human self-understanding, social, cultural and environmental interaction, autonomy, agency, worth and dignity. 3. This Recommendation pays specific attention to the broader ethical implications of AI systems in relation to the central domains of UNESCO: education, science, culture, and communication and information, as explored in the 2019 Preliminary Study on the Ethics of Artificial Intelligence by the UNESCO World Commission on Ethics of Scientific Knowledge and Technology (COMEST): (a) Education, because living in digitalizing societies requires new educational practices, ethical reflection, critical thinking, responsible design practices and new skills, given the implications for the labour market, employability and civic participation. (b) Science, in the broadest sense and including all academic fields from the natural sciences and medical sciences to the social sciences and humanities, as AI technologies bring new research capacities and approaches, have implications for our concepts of scientific understanding and explanation, and create a new basis for decision-making. (c) Cultural identity and diversity, as AI technologies can enrich cultural and creative industries, but can also lead to an increased concentration of supply of cultural content, data, markets and income in the hands of only a few actors, with potential negative implications for the diversity and pluralism of languages, media, cultural expressions, participation and equality. (d) Communication and information, as AI technologies play an increasingly important role in the processing, structuring and provision of information; the issues of automated journalism and the algorithmic provision of news and moderation and curation of content on social media and search engines are just a few examples raising issues related to access to information, disinformation, misinformation, hate speech, the emergence of new forms of societal narratives, discrimination, freedom of expression, privacy and media and information literacy, among others. 4. This Recommendation is addressed to Member States, both as AI actors and as authorities responsible for developing legal and regulatory frameworks throughout the entire AI system life cycle, and for promoting business responsibility. It also provides ethical guidance to all AI actors, including the public and private sectors, by providing a basis for an ethical impact assessment of AI systems throughout their life cycle. II. AIMS AND OBJECTIVES 5. This Recommendation aims to provide a basis to make AI systems work for the good of humanity, individuals, societies and the environment and ecosystems, and to prevent harm. It also aims at stimulating the peaceful use of AI systems. 6. In addition to the existing ethical frameworks regarding AI around the world, this Recommendation aims to bring a globally accepted normative instrument that focuses not only on the articulation of values and principles, but also on their practical realization, via concrete policy recommendations, with a strong emphasis on inclusion issues of gender equality and protection of the environment and ecosystems. 7. Because the complexity of the ethical issues surrounding AI necessitates the cooperation of multiple stakeholders across the various levels and sectors of international, regional and national communities, this Recommendation aims to enable stakeholders to take shared responsibility based on a global and intercultural dialogue. 8. The objectives of this Recommendation are: (a) to provide a universal framework of values, principles and actions to guide States in the formulation of their legislation, policies or other instruments regarding AI, consistent with international law; (b) to guide the actions of individuals, groups, communities, institutions and private sector companies to ensure the embedding of ethics in all stages of the AI system life cycle; (c) to protect, promote and respect human rights and fundamental freedoms, human dignity and equality, including gender equality; to safeguard the interests of present and future generations; to preserve the environment, biodiversity and ecosystems; and to respect cultural diversity in all stages of the AI system life cycle; (d) to foster multi-stakeholder, multidisciplinary and pluralistic dialogue and consensus building about ethical issues relating to AI systems; (e) to promote equitable access to developments and knowledge in the field of AI and the sharing of benefits, with particular attention to the needs and contributions of LMICs, including LDCs, LLDCs and SIDS. III. VALUES AND PRINCIPLES 9. The values and principles included below should be respected by all actors in the AI system life cycle, in the first place and, where needed and appropriate, be promoted through amendments to the existing and elaboration of new legislation, regulations and business guidelines. This must comply with international law, including the United Nations Charter and Member States' human rights obligations, and should be in line with internationally agreed social, political, environmental, educational, scientific and economic sustainability objectives, such as the United Nations Sustainable Development Goals (SDGs). 10. Values play a powerful role as motivating ideals in shaping policy measures and legal norms. While the set of values outlined below thus inspires desirable behaviour and represents the foundations of principles, the principles unpack the values underlying them more concretely so that the values can be more easily operationalized in policy statements and actions. 11. While all the values and principles outlined below are desirable per se, in any practical contexts, there may be tensions between these values and principles. In any given situation, a contextual assessment will be necessary to manage potential tensions, taking into account the principle of proportionality and in compliance with human rights and fundamental freedoms. In all cases, any possible limitations on human rights and fundamental freedoms must have a lawful basis, and be reasonable, necessary and proportionate, and consistent with States' obligations under international law. To navigate such scenarios judiciously will typically require engagement with a broad range of appropriate stakeholders, making use of social dialogue, as well as ethical deliberation, due diligence and impact assessment. 12. The trustworthiness and integrity of the life cycle of AI systems is essential to ensure that AI technologies will work for the good of humanity, individuals, societies and the environment and ecosystems, and embody the values and principles set out in this Recommendation. People should have good reason to trust that AI systems can bring individual and shared benefits, while adequate measures are taken to mitigate risks. An essential requirement for trustworthiness is that, throughout their life cycle, AI systems are subject to thorough monitoring by the relevant stakeholders as appropriate. As trustworthiness is an outcome of the operationalization of the principles in this document, the policy actions proposed in this Recommendation are all directed at promoting trustworthiness in all stages of the AI system life cycle. III.1 VALUES Respect, protection and promotion of human rights and fundamental freedoms and human dignity 13. The inviolable and inherent dignity of every human constitutes the foundation for the universal, indivisible, inalienable, interdependent and interrelated system of human rights and fundamental freedoms. Therefore, respect, protection and promotion of human dignity and rights as established by international law, including international human rights law, is essential throughout the life cycle of AI systems. Human dignity relates to the recognition of the intrinsic and equal worth of each individual human being, regardless of race, colour, descent, gender, age, language, religion, political opinion, national origin, ethnic origin, social origin, economic or social condition of birth, or disability and any other grounds. 14. No human being or human community should be harmed or subordinated, whether physically, economically, socially, politically, culturally or mentally during any phase of the life cycle of AI systems. Throughout the life cycle of AI systems, the quality of life of human beings should be enhanced, while the definition of 'quality of life' should be left open to individuals or groups, as long as there is no violation or abuse of human rights and fundamental freedoms, or the dignity of humans in terms of this definition. 15. Persons may interact with AI systems throughout their life cycle and receive assistance from them, such as care for vulnerable people or people in vulnerable situations, including but not limited to children, older persons, persons with disabilities or the ill. Within such interactions, persons should never be objectified, nor should their dignity be otherwise undermined, or human rights and fundamental freedoms violated or abused. 16. Human rights and fundamental freedoms must be respected, protected and promoted throughout the life cycle of AI systems. Governments, private sector, civil society, international organizations, technical communities and academia must respect human rights instruments and frameworks in their interventions in the processes surrounding the life cycle of AI systems. New technologies need to provide new means to advocate, defend and exercise human rights and not to infringe them. Environmental and ecosystem flourishing 17. Environmental and ecosystem flourishing should be recognized, protected and promoted through the life cycle of AI systems. Furthermore, environment and ecosystems are the existential necessity for humanity and other living beings to be able to enjoy the benefits of advances in AI. 18. All actors involved in the life cycle of AI systems must comply with applicable international law and domestic legislation, standards and practices, such as precaution, designed for environmental and ecosystem protection and restoration, and sustainable development. They should reduce the environmental impact of AI systems, including but not limited to its carbon footprint, to ensure the minimization of climate change and environmental risk factors, and prevent the unsustainable exploitation, use and transformation of natural resources contributing to the deterioration of the environment and the degradation of ecosystems. Ensuring diversity and inclusiveness 19. Respect, protection and promotion of diversity and inclusiveness should be ensured throughout the life cycle of AI systems, consistent with international law, including human rights law. This may be done by promoting active participation of all individuals or groups regardless of race, colour, descent, gender, age, language, religion, political opinion, national origin, ethnic origin, social origin, economic or social condition of birth, or disability and any other grounds. 20. The scope of lifestyle choices, beliefs, opinions, expressions or personal experiences, including the optional use of AI systems and the co-design of these architectures should not be restricted during any phase of the life cycle of AI systems. 21. Furthermore, efforts, including international cooperation, should be made to overcome, and never take advantage of, the lack of necessary technological infrastructure, education and skills, as well as legal frameworks, particularly in LMICs, LDCs, LLDCs and SIDS, affecting communities. Living in peaceful, just and interconnected societies 22. AI actors should play a participative and enabling role to ensure peaceful and just societies, which is based on an interconnected future for the benefit of all, consistent with human rights and fundamental freedoms. The value of living in peaceful and just societies points to the potential of AI systems to contribute throughout their life cycle to the interconnectedness of all living creatures with each other and with the natural environment. 23. The notion of humans being interconnected is based on the knowledge that every human belongs to a greater whole, which thrives when all its constituent parts are enabled to thrive. Living in peaceful, just and interconnected societies requires an organic, immediate, uncalculated bond of solidarity, characterized by a permanent search for peaceful relations, tending towards care for others and the natural environment in the broadest sense of the term. 24. This value demands that peace, inclusiveness and justice, equity and interconnectedness should be promoted throughout the life cycle of AI systems, in so far as the processes of the life cycle of AI systems should not segregate, objectify or undermine freedom and autonomous decision-making as well as the safety of human beings and communities, divide and turn individuals and groups against each other, or threaten the coexistence between humans, other living beings and the natural environment. III.2 PRINCIPLES Proportionality and Do No Harm 25. It should be recognized that AI technologies do not necessarily, per se, ensure human and environmental and ecosystem flourishing. Furthermore, none of the processes related to the AI system life cycle shall exceed what is necessary to achieve legitimate aims or objectives and should be appropriate to the context. In the event of possible occurrence of any harm to human beings, human rights and fundamental freedoms, communities and society at large or the environment and ecosystems, the implementation of procedures for risk assessment and the adoption of measures in order to preclude the occurrence of such harm should be ensured. 26. The choice to use AI systems and which AI method to use should be justified in the following ways: (a) the AI method chosen should be appropriate and proportional to achieve a given legitimate aim; (b) the AI method chosen should not infringe upon the foundational values captured in this document, in particular, its use must not violate or abuse human rights; and (c) the AI method should be appropriate to the context and should be based on rigorous scientific foundations. In scenarios where decisions are understood to have an impact that is irreversible or difficult to reverse or may involve life and death decisions, final human determination should apply. In particular, AI systems should not be used for social scoring or mass surveillance purposes. Safety and security 27. Unwanted harms (safety risks), as well as vulnerabilities to attack (security risks) should be avoided and should be addressed, prevented and eliminated throughout the life cycle of AI systems to ensure human, environmental and ecosystem safety and security. Safe and secure AI will be enabled by the development of sustainable, privacy-protective data access frameworks that foster better training and validation of AI models utilizing quality data. Fairness and non-discrimination 28. AI actors should promote social justice and safeguard fairness and non-discrimination of any kind in compliance with international law. This implies an inclusive approach to ensuring that the benefits of AI technologies are available and accessible to all, taking into consideration the specific needs of different age groups, cultural systems, different language groups, persons with disabilities, girls and women, and disadvantaged, marginalized and vulnerable people or people in vulnerable situations. Member States should work to promote inclusive access for all, including local communities, to AI systems with locally relevant content and services, and with respect for multilingualism and cultural diversity. Member States should work to tackle digital divides and ensure inclusive access to and participation in the development of AI. At the national level, Member States should promote equity between rural and urban areas, and among all persons regardless of race, colour, descent, gender, age, language, religion, political opinion, national origin, ethnic origin, social origin, economic or social condition of birth, or disability and any other grounds, in terms of access to and participation in the AI system life cycle. At the international level, the most technologically advanced countries have a responsibility of solidarity with the least advanced to ensure that the benefits of AI technologies are shared such that access to and participation in the AI system life cycle for the latter contributes to a fairer world order with regard to information, communication, culture, education, research and socio-economic and political stability. 29. AI actors should make all reasonable efforts to minimize and avoid reinforcing or perpetuating discriminatory or biased applications and outcomes throughout the life cycle of the AI system to ensure fairness of such systems. Effective remedy should be available against discrimination and biased algorithmic determination. 30. Furthermore, digital and knowledge divides within and between countries need to be addressed throughout an AI system life cycle, including in terms of access and quality of access to technology and data, in accordance with relevant national, regional and international legal frameworks, as well as in terms of connectivity, knowledge and skills and meaningful participation of the affected communities, such that every person is treated equitably. Sustainability 31. The development of sustainable societies relies on the achievement of a complex set of objectives on a continuum of human, social, cultural, economic and environmental dimensions. The advent of AI technologies can either benefit sustainability objectives or hinder their realization, depending on how they are applied across countries with varying levels of development. The continuous assessment of the human, social, cultural, economic and environmental impact of AI technologies should therefore be carried out with full cognizance of the implications of AI technologies for sustainability as a set of constantly evolving goals across a range of dimensions, such as currently identified in the Sustainable Development Goals (SDGs) of the United Nations. Right to Privacy, and Data Protection 32. Privacy, a right essential to the protection of human dignity, human autonomy and human agency, must be respected, protected and promoted throughout the life cycle of AI systems. It is important that data for AI systems be collected, used, shared, archived and deleted in ways that are consistent with international law and in line with the values and principles set forth in this Recommendation, while respecting relevant national, regional and international legal frameworks. 33. Adequate data protection frameworks and governance mechanisms should be established in a multi-stakeholder approach at the national or international level, protected by judicial systems, and ensured throughout the life cycle of AI systems. Data protection frameworks and any related mechanisms should take reference from international data protection principles and standards concerning the collection, use and disclosure of personal data and exercise of their rights by data subjects while ensuring a legitimate aim and a valid legal basis for the processing of personal data, including informed consent. 34. Algorithmic systems require adequate privacy impact assessments, which also include societal and ethical considerations of their use and an innovative use of the privacy by design approach. AI actors need to ensure that they are accountable for the design and implementation of AI systems in such a way as to ensure that personal information is protected throughout the life cycle of the AI system. Human oversight and determination 35. Member States should ensure that it is always possible to attribute ethical and legal responsibility for any stage of the life cycle of AI systems, as well as in cases of remedy related to AI systems, to physical persons or to existing legal entities. Human oversight refers thus not only to individual human oversight, but to inclusive public oversight, as appropriate. 36. It may be the case that sometimes humans would choose to rely on AI systems for reasons of efficacy, but the decision to cede control in limited contexts remains that of humans, as humans can resort to AI systems in decision-making and acting, but an AI system can never replace ultimate human responsibility and accountability. As a rule, life and death decisions should not be ceded to AI systems. Transparency and explainability 37. The transparency and explainability of AI systems are often essential preconditions to ensure the respect, protection and promotion of human rights, fundamental freedoms and ethical principles. Transparency is necessary for relevant national and international liability regimes to work effectively. A lack of transparency could also undermine the possibility of effectively challenging decisions based on outcomes produced by AI systems and may thereby infringe the right to a fair trial and effective remedy, and limits the areas in which these systems can be legally used. 38. While efforts need to be made to increase transparency and explainability of AI systems, including those with extra-territorial impact, throughout their life cycle to support democratic governance, the level of transparency and explainability should always be appropriate to the context and impact, as there may be a need to balance between transparency and explainability and other principles such as privacy, safety and security. People should be fully informed when a decision is informed by or is made on the basis of AI algorithms, including when it affects their safety or human rights, and in those circumstances should have the opportunity to request explanatory information from the relevant AI actor or public sector institutions. In addition, individuals should be able to access the reasons for a decision affecting their rights and freedoms, and have the option of making submissions to a designated staff member of the private sector company or public sector institution able to review and correct the decision. AI actors should inform users when a product or service is provided directly or with the assistance of AI systems in a proper and timely manner. 39. From a socio-technical lens, greater transparency contributes to more peaceful, just, democratic and inclusive societies. It allows for public scrutiny that can decrease corruption and discrimination, and can also help detect and prevent negative impacts on human rights. Transparency aims at providing appropriate information to the respective addressees to enable their understanding and foster trust. Specific to the AI system, transparency can enable people to understand how each stage of an AI system is put in place, appropriate to the context and sensitivity of the AI system. It may also include insight into factors that affect a specific prediction or decision, and whether or not appropriate assurances (such as safety or fairness measures) are in place. In cases of serious threats of adverse human rights impacts, transparency may also require the sharing of code or datasets. 40. Explainability refers to making intelligible and providing insight into the outcome of AI systems. The explainability of AI systems also refers to the understandability of the input, output and the functioning of each algorithmic building block and how it contributes to the outcome of the systems. Thus, explainability is closely related to transparency, as outcomes and sub-processes leading to outcomes should aim to be understandable and traceable, appropriate to the context. AI actors should commit to ensuring that the algorithms developed are explainable. In the case of AI applications that impact the end user in a way that is not temporary, easily reversible or otherwise low risk, it should be ensured that the meaningful explanation is provided with any decision that resulted in the action taken in order for the outcome to be considered transparent. 41. Transparency and explainability relate closely to adequate responsibility and accountability measures, as well as to the trustworthiness of AI systems. Responsibility and accountability 42. AI actors and Member States should respect, protect and promote human rights and fundamental freedoms, and should also promote the protection of the environment and ecosystems, assuming their respective ethical and legal responsibility, in accordance with national and international law, in particular Member States' human rights obligations, and ethical guidance throughout the life cycle of AI systems, including with respect to AI actors within their effective territory and control. The ethical responsibility and liability for the decisions and actions based in any way on an AI system should always ultimately be attributable to AI actors corresponding to their role in the life cycle of the AI system. 43. Appropriate oversight, impact assessment, audit and due diligence mechanisms, including whistle-blowers' protection, should be developed to ensure accountability for AI systems and their impact throughout their life cycle. Both technical and institutional designs should ensure auditability and traceability of (the working of) AI systems in particular to address any conflicts with human rights norms and standards and threats to environmental and ecosystem well-being. Awareness and literacy 44. Public awareness and understanding of AI technologies and the value of data should be promoted through open and accessible education, civic engagement, digital skills and AI ethics training, media and information literacy and training led jointly by governments, intergovernmental organizations, civil society, academia, the media, community leaders and the private sector, and considering the existing linguistic, social and cultural diversity, to ensure effective public participation so that all members of society can take informed decisions about their use of AI systems and be protected from undue influence. 45. Learning about the impact of AI systems should include learning about, through and for human rights and fundamental freedoms, meaning that the approach and understanding of AI systems should be grounded by their impact on human rights and access to rights, as well as on the environment and ecosystems. Multi-stakeholder and adaptive governance and collaboration 46. International law and national sovereignty must be respected in the use of data. That means that States, complying with international law, can regulate the data generated within or passing through their territories, and take measures towards effective regulation of data, including data protection, based on respect for the right to privacy in accordance with international law and other human rights norms and standards. Participation of different stakeholders throughout the AI system life cycle is necessary for inclusive approaches to AI governance, enabling the benefits to be shared by all, and to contribute to sustainable development. Stakeholders include but are not limited to governments, intergovernmental organizations, the technical community, civil society, researchers and academia, media, education, policy-makers, private sector companies, human rights institutions and equality bodies, anti-discrimination monitoring bodies, and groups for youth and children. The adoption of open standards and interoperability to facilitate collaboration should be in place. Measures should be adopted to take into account shifts in technologies, the emergence of new groups of stakeholders, and to allow for meaningful participation by marginalized groups, communities and individuals and, where relevant, in the case of Indigenous Peoples, respect for the self-governance of their data. IV. AREAS OF POLICY ACTION 48. The policy actions described in the following policy areas operationalize the values and principles set out in this Recommendation. The main action is for Member States to put in place effective measures, including, for example, policy frameworks or mechanisms, and to ensure that other stakeholders, such as private sector companies, academic and research institutions, and civil society adhere to them by, among other actions, encouraging all stakeholders to develop human rights, rule of law, democracy, and ethical impact assessment and due diligence tools in line with guidance including the United Nations Guiding Principles on Business and Human Rights. The process for developing such policies or mechanisms should be inclusive of all stakeholders and should take into account the circumstances and priorities of each Member State. UNESCO can be a partner and support Member States in the development as well as monitoring and evaluation of policy mechanisms. 49. UNESCO recognizes that Member States will be at different stages of readiness to implement this Recommendation, in terms of scientific, technological, economic, educational, legal, regulatory, infrastructural, societal, cultural and other dimensions. It is noted that \"readiness\" here is a dynamic status. In order to enable the effective implementation of this Recommendation, UNESCO will therefore: (1) develop a readiness assessment methodology to assist interested Member States in identifying their status at specific moments of their readiness trajectory along a continuum of dimensions; and (2) ensure support for interested Member States in terms of developing a UNESCO methodology for Ethical Impact Assessment (EIA) of AI technologies, sharing of best practices, assessment guidelines and other mechanisms and analytical work. POLICY AREA 1: ETHICAL IMPACT ASSESSMENT 50. Member States should introduce frameworks for impact assessments, such as ethical impact assessment, to identify and assess benefits, concerns and risks of AI systems, as well as appropriate risk prevention, mitigation and monitoring measures, among other assurance mechanisms. Such impact assessments should identify impacts on human rights and fundamental freedoms, in particular but not limited to the rights of marginalized and vulnerable people or people in vulnerable situations, labour rights, the environment and ecosystems and ethical and social implications, and facilitate citizen participation in line with the values and principles set forth in this Recommendation. 51. Member States and private sector companies should develop due diligence and oversight mechanisms to identify, prevent, mitigate and account for how they address the impact of AI systems on the respect for human rights, rule of law and inclusive societies. Member States should also be able to assess the socio-economic impact of AI systems on poverty and ensure that the gap between people living in wealth and poverty, as well as the digital divide among and within countries, are not increased with the massive adoption of AI technologies at present and in the future. In order to do this, in particular, enforceable transparency protocols should be implemented, corresponding to the access to information, including information of public interest held by private entities. Member States, private sector companies and civil society should investigate the sociological and psychological effects of AI-based recommendations on humans in their decision-making autonomy. AI systems identified as potential risks to human rights should be broadly tested by AI actors, including in real-world conditions if needed, as part of the Ethical Impact Assessment, before releasing them in the market. 52. Member States and business enterprises should implement appropriate measures to monitor all phases of an AI system life cycle, including the functioning of algorithms used for decision-making, the data, as well as AI actors involved in the process, especially in public services and where direct end-user interaction is needed, as part of ethical impact assessment. Member States' human rights law obligations should form part of the ethical aspects of AI system assessments. 53. Governments should adopt a regulatory framework that sets out a procedure, particularly for public authorities, to carry out ethical impact assessments on AI systems to predict consequences, mitigate risks, avoid harmful consequences, facilitate citizen participation and address societal challenges. The assessment should also establish appropriate oversight mechanisms, including auditability, traceability and explainability, which enable the assessment of algorithms, data and design processes, as well as include external review of AI systems. Ethical impact assessments should be transparent and open to the public, where appropriate. Such assessments should also be multidisciplinary, multi-stakeholder, multicultural, pluralistic and inclusive. The public authorities should be required to monitor the AI systems implemented and/or deployed by those authorities by introducing appropriate mechanisms and tools. POLICY AREA 2: ETHICAL GOVERNANCE AND STEWARDSHIP 54. Member States should ensure that AI governance mechanisms are inclusive, transparent, multidisciplinary, multilateral (this includes the possibility of mitigation and redress of harm across borders) and multi-stakeholder. In particular, governance should include aspects of anticipation, and effective protection, monitoring of impact, enforcement and redress. 55. Member States should ensure that harms caused through AI systems are investigated and redressed, by enacting strong enforcement mechanisms and remedial actions, to make certain that human rights and fundamental freedoms and the rule of law are respected in the digital world and in the physical world. Such mechanisms and actions should include remediation mechanisms provided by private and public sector companies. The auditability and traceability of AI systems should be promoted to this end. In addition, Member States should strengthen their institutional capacities to deliver on this commitment and should collaborate with researchers and other stakeholders to investigate, prevent and mitigate any potentially malicious uses of AI systems. 56. Member States are encouraged to develop national and regional AI strategies and to consider forms of soft governance such as a certification mechanism for AI systems and the mutual recognition of their certification, according to the sensitivity of the application domain and expected impact on human rights, the environment and ecosystems, and other ethical considerations set forth in this Recommendation. Such a mechanism might include different levels of audit of systems, data, and adherence to ethical guidelines and to procedural requirements in view of ethical aspects. At the same time, such a mechanism should not hinder innovation or disadvantage small and medium enterprises or start-ups, civil society as well as research and science organizations, as a result of an excessive administrative burden. These mechanisms should also include a regular monitoring component to ensure system robustness and continued integrity and adherence to ethical guidelines over the entire life cycle of the AI system, requiring re-certification if necessary. 57. Member States and public authorities should carry out transparent self-assessment of existing and proposed AI systems, which, in particular, should include the assessment of whether the adoption of AI is appropriate and, if so, should include further assessment to determine what the appropriate method is, as well as assessment as to whether such adoption would result in violations or abuses of Member States' human rights law obligations, and if that is the case, prohibit its use. 58. Member States should encourage public entities, private sector companies and civil society organizations to involve different stakeholders in their AI governance and to consider adding the role of an independent AI Ethics Officer or some other mechanism to oversee ethical impact assessment, auditing and continuous monitoring efforts and ensure ethical guidance of AI systems. Member States, private sector companies and civil society organizations, with the support of UNESCO, are encouraged to create a network of independent AI Ethics Officers to give support to this process at national, regional and international levels. 59. Member States should foster the development of, and access to, a digital ecosystem for ethical and inclusive development of AI systems at the national level, including to address gaps in access to the AI system life cycle, while contributing to international collaboration. Such an ecosystem includes, in particular, digital technologies and infrastructure, and mechanisms for sharing AI knowledge, as appropriate. 60. Member States should establish mechanisms, in collaboration with international organizations, transnational corporations, academic institutions and civil society, to ensure the active participation of all Member States, especially LMICs, in particular LDCs, LLDCs and SIDS, in international discussions concerning AI governance. This can be through the provision of funds, ensuring equal regional participation, or any other mechanisms. Furthermore, in order to ensure the inclusiveness of AI fora, Member States should facilitate the travel of AI actors in and out of their territory, especially from LMICs, in particular LDCs, LLDCs and SIDS, for the purpose of participating in these fora. 61. Amendments to the existing or elaboration of new national legislation addressing AI systems must comply with Member States' human rights law obligations and promote human rights and fundamental freedoms throughout the AI system life cycle. Promotion thereof should also take the form of governance initiatives, good exemplars of collaborative practices regarding AI systems, and national and international technical and methodological guidelines as AI technologies advance. Diverse sectors, including the private sector, in their practices regarding AI systems must respect, protect and promote human rights and fundamental freedoms using existing and new instruments in combination with this Recommendation. 62. Member States that acquire Al systems for human rights-sensitive use cases, such as law enforcement, welfare, employment, media and information providers, health care and the independent judiciary system should provide mechanisms to monitor the social and economic impact of such systems by appropriate oversight authorities, including independent data protection authorities, sectoral oversight and public bodies responsible for oversight. 63. Member States should enhance the capacity of the judiciary to make decisions related to AI systems as per the rule of law and in line with international law and standards, including in the use of AI systems in their deliberations, while ensuring that the principle of human oversight is upheld. In case AI systems are used by the judiciary, sufficient safeguards are needed to guarantee inter alia the protection of fundamental human rights, the rule of law, judicial independence as well as the principle of human oversight, and to ensure a trustworthy, public interest-oriented and human-centric development and use of AI systems in the judiciary. 64. Member States should ensure that governments and multilateral organizations play a leading role in ensuring the safety and security of AI systems, with multi-stakeholder participation. Specifically, Member States, international organizations and other relevant bodies should develop international standards that describe measurable, testable levels of safety and transparency, so that systems can be objectively assessed and levels of compliance determined. Furthermore, Member States and business enterprises should continuously support strategic research on potential safety and security risks of AI technologies and should encourage research into transparency and explainability, inclusion and literacy by putting additional funding into those areas for different domains and at different levels, such as technical and natural language. 65. Member States should implement policies to ensure that the actions of AI actors are consistent with international human rights law, standards and principles throughout the life cycle of AI systems, while taking into full consideration the current cultural and social diversities, including local customs and religious traditions, with due regard to the precedence and universality of human rights. 66. Member States should put in place mechanisms to require AI actors to disclose and combat any kind of stereotyping in the outcomes of AI systems and data, whether by design or by negligence, and to ensure that training data sets for AI systems do not foster cultural, economic or social inequalities, prejudice, the spreading of disinformation and misinformation, and disruption of freedom of expression and access to information. Particular attention should be given to regions where the data are scarce. 67. Member States should implement policies to promote and increase diversity and inclusiveness that reflect their populations in AI development teams and training datasets, and to ensure equal access to AI technologies and their benefits, particularly for marginalized groups, both from rural and urban zones. 68. Member States should develop, review and adapt, as appropriate, regulatory frameworks to achieve accountability and responsibility for the content and outcomes of AI systems at the different phases of their life cycle. Member States should, where necessary, introduce liability frameworks or clarify the interpretation of existing frameworks to ensure the attribution of accountability for the outcomes and the functioning of AI systems. Furthermore, when developing regulatory frameworks, Member States should, in particular, take into account that ultimate responsibility and accountability must always lie with natural or legal persons and that AI systems should not be given legal personality themselves. To ensure this, such regulatory frameworks should be consistent with the principle of human oversight and establish a comprehensive approach focused on AI actors and the technological processes involved across the different stages of the AI system life cycle. 69. In order to establish norms where these do not exist, or to adapt the existing legal frameworks, Member States should involve all AI actors (including, but not limited to, researchers, representatives of civil society and law enforcement, insurers, investors, manufacturers, engineers, lawyers and users). The norms can mature into best practices, laws and regulations. Member States are further encouraged to use mechanisms such as policy prototypes and regulatory sandboxes to accelerate the development of laws, regulations and policies, including regular reviews thereof, in line with the rapid development of new technologies and ensure that laws and regulations can be tested in a safe environment before being officially adopted. Member States should support local governments in the development of local policies, regulations and laws in line with national and international legal frameworks. 70. Member States should set clear requirements for AI system transparency and explainability so as to help ensure the trustworthiness of the full AI system life cycle. Such requirements should involve the design and implementation of impact mechanisms that take into consideration the nature of application domain, intended use, target audience and feasibility of each particular AI system. POLICY AREA 3: DATA POLICY 71. Member States should work to develop data governance strategies that ensure the continual evaluation of the quality of training data for AI systems including the adequacy of the data collection and selection processes, proper data security and protection measures, as well as feedback mechanisms to learn from mistakes and share best practices among all AI actors. 72. Member States should put in place appropriate safeguards to protect the right to privacy in accordance with international law, including addressing concerns such as surveillance. Member States should, among others, adopt or enforce legislative frameworks that provide appropriate protection, compliant with international law. Member States should strongly encourage all AI actors, including business enterprises, to follow existing international standards and, in particular, to carry out adequate privacy impact assessments, as part of ethical impact assessments, which take into account the wider socio-economic impact of the intended data processing, and to apply privacy by design in their systems. Privacy should be respected, protected and promoted throughout the life cycle of AI systems. 73. Member States should ensure that individuals retain rights over their personal data and are protected by a framework, which notably foresees: transparency; appropriate safeguards for the processing of sensitive data; an appropriate level of data protection; effective and meaningful accountability schemes and mechanisms; the full enjoyment of the data subjects' rights and the ability to access and erase their personal data in AI systems, except for certain circumstances in compliance with international law; an appropriate level of protection in full compliance with data protection legislation where data are being used for commercial purposes such as enabling micro-targeted advertising, transferred cross-border; and an effective independent oversight as part of a data governance mechanism which keeps individuals in control of their personal data and fosters the benefits of a free flow of information internationally, including access to data. 74. Member States should establish their data policies or equivalent frameworks, or reinforce existing ones, to ensure full security for personal data and sensitive data, which, if disclosed, may cause exceptional damage, injury or hardship to individuals. Examples include data relating to offences, criminal proceedings and convictions, and related security measures; biometric, genetic and health data; and personal data such as that relating to race, colour, descent, gender, age, language, religion, political opinion, national origin, ethnic origin, social origin, economic or social condition of birth, or disability and any other characteristics. 75. Member States should promote open data. In this regard, Member States should consider reviewing their policies and regulatory frameworks, including on access to information and open government to reflect AI-specific requirements and promoting mechanisms, such as open repositories for publicly funded or publicly held data and source code and data trusts, to support the safe, fair, legal and ethical sharing of data, among others. 76. Member States should promote and facilitate the use of quality and robust datasets for training, development and use of AI systems, and exercise vigilance in overseeing their collection and use. This could, if possible and feasible, include investing in the creation of gold standard datasets, including open and trustworthy datasets, which are diverse, constructed on a valid legal basis, including consent of data subjects, when required by law. Standards for annotating datasets should be encouraged, including disaggregating data on gender and other bases, so it can easily be determined how a dataset is gathered and what properties it has. 77. Member States, as also suggested in the report of the United Nations Secretary-General's High-level Panel on Digital Cooperation, with the support of the United Nations and UNESCO, should adopt a digital commons approach to data where appropriate, increase interoperability of tools and datasets and interfaces of systems hosting data, and encourage private sector companies to share the data they collect with all stakeholders, as appropriate, for research, innovation or public benefits. They should also promote public and private efforts to create collaborative platforms to share quality data in trusted and secured data spaces. POLICY AREA 4: DEVELOPMENT AND INTERNATIONAL COOPERATION 78. Member States and transnational corporations should prioritize AI ethics by including discussions of AI-related ethical issues into relevant international, intergovernmental and multi-stakeholder fora. 79. Member States should ensure that the use of AI in areas of development such as education, science, culture, communication and information, health care, agriculture and food supply, environment, natural resource and infrastructure management, economic planning and growth, among others, adheres to the values and principles set forth in this Recommendation. 80. Member States should work through international organizations to provide platforms for international cooperation on AI for development, including by contributing expertise, funding, data, domain knowledge, infrastructure, and facilitating multi-stakeholder collaboration to tackle challenging development problems, especially for LMICs, in particular LDCs, LLDCs and SIDS. 81. Member States should work to promote international collaboration on AI research and innovation, including research and innovation centres and networks that promote greater participation and leadership of researchers from LMICs and other countries, including LDCs, LLDCs and SIDS. 82. Member States should promote AI ethics research by engaging international organizations and research institutions, as well as transnational corporations, that can be a basis for the ethical use of AI systems by public and private entities, including research into the applicability of specific ethical frameworks in specific cultures and contexts, and the possibilities to develop technologically feasible solutions in line with these frameworks. 83. Member States should encourage international cooperation and collaboration in the field of AI to bridge geo-technological lines. Technological exchanges and consultations should take place between Member States and their populations, between the public and private sectors, and between and among the most and least technologically advanced countries in full respect of international law. POLICY AREA 5: ENVIRONMENT AND ECOSYSTEMS 84. Member States and business enterprises should assess the direct and indirect environmental impact throughout the AI system life cycle, including, but not limited to, its carbon footprint, energy consumption and the environmental impact of raw material extraction for supporting the manufacturing of AI technologies, and reduce the environmental impact of AI systems and data infrastructures. Member States should ensure compliance of all AI actors with environmental law, policies and practices. 85. Member States should introduce incentives, when needed and appropriate, to ensure the development and adoption of rights-based and ethical AI-powered solutions for disaster risk resilience; the monitoring, protection and regeneration of the environment and ecosystems; and the preservation of the planet. These AI systems should involve the participation of local and indigenous communities throughout the life cycle of AI systems and should support circular economy type approaches and sustainable consumption and production patterns. Some examples include using AI systems, when needed and appropriate, to: (a) Support the protection, monitoring and management of natural resources. (b) Support the prediction, prevention, control and mitigation of climate-related problems. (c) Support a more efficient and sustainable food ecosystem. (d) Support the acceleration of access to and mass adoption of sustainable energy. (e) Enable and promote the mainstreaming of sustainable infrastructure, sustainable business models and sustainable finance for sustainable development. (f) Detect pollutants or predict levels of pollution and thus help relevant stakeholders identify, plan and put in place targeted interventions to prevent and reduce pollution and exposure. 86. When choosing AI methods, given the potential data-intensive or resource-intensive character of some of them and the respective impact on the environment, Member States should ensure that AI actors, in line with the principle of proportionality, favour data, energy and resource-efficient AI methods. Requirements should be developed to ensure that appropriate evidence is available to show that an AI application will have the intended effect, or that safeguards accompanying an AI application can support the justification for its use. If this cannot be done, the precautionary principle must be favoured, and in instances where there are disproportionate negative impacts on the environment, AI should not be used. POLICY AREA 6: GENDER 87. Member States should ensure that the potential for digital technologies and artificial intelligence to contribute to achieving gender equality is fully maximized, and must ensure that the human rights and fundamental freedoms of girls and women, and their safety and integrity are not violated at any stage of the AI system life cycle. Moreover, Ethical Impact Assessment should include a transversal gender perspective. 88. Member States should have dedicated funds from their public budgets linked to financing gender-responsive schemes, ensure that national digital policies include a gender action plan, and develop relevant policies, for example, on labour education, targeted at supporting girls and women to make sure they are not left out of the digital economy powered by AI. Special investment in providing targeted programmes and gender-specific language, to increase the opportunities of girls' and women's participation in science, technology, engineering, and mathematics (STEM), including information and communication technologies (ICT) disciplines, preparedness, employability, equal career development and professional growth of girls and women, should be considered and implemented. 89. Member States should ensure that the potential of AI systems to advance the achievement of gender equality is realized. They should ensure that these technologies do not exacerbate the already wide gender gaps existing in several fields in the analogue world, and instead eliminate those gaps. These gaps include: the gender wage gap; the unequal representation in certain professions and activities; the lack of representation at top management positions, boards of directors, or research teams in the AI field; the education gap; the digital and AI access, adoption, usage and affordability gap; and the unequal distribution of unpaid work and of the caring responsibilities in our societies. 90. Member States should ensure that gender stereotyping and discriminatory biases are not translated into AI systems, and instead identify and proactively redress these. Efforts are necessary to avoid the compounding negative effect of technological divides in achieving gender equality and avoiding violence such as harassment, bullying or trafficking of girls and women and under-represented groups, including in the online domain. 91. Member States should encourage female entrepreneurship, participation and engagement in all stages of an AI system life cycle by offering and promoting economic, regulatory incentives, among other incentives and support schemes, as well as policies that aim at a balanced gender participation in AI research in academia, gender representation on digital and AI companies' top management positions, boards of directors and research teams. Member States should ensure that public funds (for innovation, research and technologies) are channelled to inclusive programmes and companies, with clear gender representation, and that private funds are similarly encouraged through affirmative action principles. Policies on harassment-free environments should be developed and enforced, together with the encouragement of the transfer of best practices on how to promote diversity throughout the AI system life cycle. 92. Member States should promote gender diversity in AI research in academia and industry by offering incentives to girls and women to enter the field, putting in place mechanisms to fight gender stereotyping and harassment within the AI research community, and encouraging academic and private entities to share best practices on how to enhance gender diversity. 93. UNESCO can help form a repository of best practices for incentivizing the participation of girls, women and under-represented groups in all stages of the AI system life cycle. POLICY AREA 7: CULTURE 94. Member States are encouraged to incorporate AI systems, where appropriate, in the preservation, enrichment, understanding, promotion, management and accessibility of tangible, documentary and intangible cultural heritage, including endangered languages as well as indigenous languages and knowledges, for example by introducing or updating educational programmes related to the application of AI systems in these areas, where appropriate, and by ensuring a participatory approach, targeted at institutions and the public. 95. Member States are encouraged to examine and address the cultural impact of AI systems, especially natural language processing (NLP) applications such as automated translation and voice assistants, on the nuances of human language and expression. Such assessments should provide input for the design and implementation of strategies that maximize the benefits from these systems by bridging cultural gaps and increasing human understanding, as well as addressing the negative implications such as the reduction of use, which could lead to the disappearance of endangered languages, local dialects, and tonal and cultural variations associated with human language and expression. 96. Member States should promote AI education and digital training for artists and creative professionals to assess the suitability of AI technologies for use in their profession, and contribute to the design and implementation of suitable AI technologies, as AI technologies are being used to create, produce, distribute, broadcast and consume a variety of cultural goods and services, bearing in mind the importance of preserving cultural heritage, diversity and artistic freedom. 97. Member States should promote awareness and evaluation of AI tools among local cultural industries and small and medium enterprises working in the field of culture, to avoid the risk of concentration in the cultural market. 98. Member States should engage technology companies and other stakeholders to promote a diverse supply of and plural access to cultural expressions, and in particular to ensure that algorithmic recommendation enhances the visibility and discoverability of local content. 99. Member States should foster new research at the intersection between AI and intellectual property (IP), for example to determine whether or how to protect with IP rights the works created by means of Al technologies. Member States should also assess how AI technologies are affecting the rights or interests of IP owners, whose works are used to research, develop, train or implement AI applications. 100. Member States should encourage museums, galleries, libraries and archives at the national level to use AI systems to highlight their collections and enhance their libraries, databases and knowledge base, while also providing access to their users. POLICY AREA 8: EDUCATION AND RESEARCH 101. Member States should work with international organizations, educational institutions and private and non-governmental entities to provide adequate AI literacy education to the public on all levels in all countries in order to empower people and reduce the digital divides and digital access inequalities resulting from the wide adoption of AI systems. 102. Member States should promote the acquisition of 'prerequisite skills' for AI education, such as basic literacy, numeracy, coding and digital skills, and media and information literacy, as well as critical and creative thinking, teamwork, communication, socio-emotional and AI ethics skills, especially in countries and in regions or areas within countries where there are notable gaps in the education of these skills. 103. Member States should promote general awareness programmes about AI developments, including on data and the opportunities and challenges brought about by AI technologies, the impact of AI systems on human rights and their implications, including children's rights. These programmes should be accessible to non-technical as well as technical groups. 104. Member States should encourage research initiatives on the responsible and ethical use of AI technologies in teaching, teacher training and e-learning, among other issues, to enhance opportunities and mitigate the challenges and risks involved in this area. The initiatives should be accompanied by an adequate assessment of the quality of education and impact on students and teachers of the use of AI technologies. Member States should also ensure that AI technologies empower students and teachers and enhance their experience, bearing in mind that relational and social aspects and the value of traditional forms of education are vital in teacher-student and student-student relationships and should be considered when discussing the adoption of AI technologies in education. AI systems used in learning should be subject to strict requirements when it comes to the monitoring, assessment of abilities, or prediction of the learners' behaviours. AI should support the learning process without reducing cognitive abilities and without extracting sensitive information, in compliance with relevant personal data protection standards. The data handed over to acquire knowledge collected during the learner's interactions with the AI system must not be subject to misuse, misappropriation or criminal exploitation, including for commercial purposes. 105. Member States should promote the participation and leadership of girls and women, diverse ethnicities and cultures, persons with disabilities, marginalized and vulnerable people or people in vulnerable situations, minorities and all persons not enjoying the full benefits of digital inclusion, in AI education programmes at all levels, as well as the monitoring and sharing of best practices in this regard with other Member States. 106. Member States should develop, in accordance with their national education programmes and traditions, AI ethics curricula for all levels, and promote cross-collaboration between AI technical skills education and humanistic, ethical and social aspects of AI education. Online courses and digital resources of AI ethics education should be developed in local languages, including indigenous languages, and take into account the diversity of environments, especially ensuring accessibility of formats for persons with disabilities. 107. Member States should promote and support AI research, notably AI ethics research, including for example through investing in such research or by creating incentives for the public and private sectors to invest in this area, recognizing that research contributes significantly to the further development and improvement of AI technologies with a view to promoting international law and the values and principles set forth in this Recommendation. Member States should also publicly promote the best practices of, and cooperation with, researchers and companies who develop AI in an ethical manner. 108. Member States should ensure that AI researchers are trained in research ethics and require them to include ethical considerations in their designs, products and publications, especially in the analyses of the datasets they use, how they are annotated, and the quality and scope of the results with possible applications. Member States should encourage private sector companies to facilitate the access of the scientific community to their data for research, especially in LMICs, in particular LDCs, LLDCs and SIDS. This access should conform to relevant privacy and data protection standards. 110. To ensure a critical evaluation of AI research and proper monitoring of potential misuses or adverse effects, Member States should ensure that any future developments with regards to AI technologies should be based on rigorous and independent scientific research, and promote interdisciplinary AI research by including disciplines other than science, technology, engineering and mathematics (STEM), such as cultural studies, education, ethics, international relations, law, linguistics, philosophy, political science, sociology and psychology. 111. Recognizing that AI technologies present great opportunities to help advance scientific knowledge and practice, especially in traditionally model-driven disciplines, Member States should encourage scientific communities to be aware of the benefits, limits and risks of their use; this includes attempting to ensure that conclusions drawn from data-driven approaches, models and treatments are robust and sound. Furthermore, Member States should welcome and support the role of the scientific community in contributing to policy and in cultivating awareness of the strengths and weaknesses of AI technologies. POLICY AREA 9: COMMUNICATION AND INFORMATION 112. Member States should use AI systems to improve access to information and knowledge. This can include support to researchers, academia, journalists, the general public and developers, to enhance freedom of expression, academic and scientific freedoms, access to information, and increased proactive disclosure of official data and information. 113. Member States should ensure that AI actors respect and promote freedom of expression as well as access to information with regard to automated content generation, moderation and curation. Appropriate frameworks, including regulation, should enable transparency of online communication and information operators and ensure users have access to a diversity of viewpoints, as well as processes for prompt notification to the users on the reasons for removal or other treatment of content, and appeal mechanisms that allow users to seek redress. 114. Member States should invest in and promote digital and media and information literacy skills to strengthen critical thinking and competencies needed to understand the use and implication of AI systems, in order to mitigate and counter disinformation, misinformation and hate speech. A better understanding and evaluation of both the positive and potentially harmful effects of recommender systems should be part of those efforts. 115. Member States should create enabling environments for media to have the rights and resources to effectively report on the benefits and harms of AI systems, and also encourage media to make ethical use of AI systems in their operations. POLICY AREA 10: ECONOMY AND LABOUR 116. Member States should assess and address the impact of AI systems on labour markets and its implications for education requirements, in all countries and with special emphasis on countries where the economy is labour-intensive. This can include the introduction of a wider range of 'core' and interdisciplinary skills at all education levels to provide current workers and new generations a fair chance of finding jobs in a rapidly changing market, and to ensure their awareness of the ethical aspects of AI systems. Skills such as 'learning how to learn', communication, critical thinking, teamwork, empathy, and the ability to transfer one's knowledge across domains, should be taught alongside specialist, technical skills, as well as low-skilled tasks. Being transparent about what skills are in demand and updating curricula around these are key. 117. Member States should support collaboration agreements among governments, academic institutions, vocational education and training institutions, industry, workers' organizations and civil society to bridge the gap of skillset requirements to align training programmes and strategies with the implications of the future of work and the needs of industry, including small and medium enterprises. Project-based teaching and learning approaches for AI should be promoted, allowing for partnerships between public institutions, private sector companies, universities and research centres. 118. Member States should work with private sector companies, civil society organizations and other stakeholders, including workers and unions to ensure a fair transition for at-risk employees. This includes putting in place upskilling and reskilling programmes, finding effective mechanisms of retaining employees during those transition periods, and exploring 'safety net' programmes for those who cannot be retrained. Member States should develop and implement programmes to research and address the challenges identified that could include upskilling and reskilling, enhanced social protection, proactive industry policies and interventions, tax benefits, new taxation forms, among others. Member States should ensure that there is sufficient public funding to support these programmes. Relevant regulations, such as tax regimes, should be carefully examined and changed if needed to counteract the consequences of unemployment caused by AI-based automation. 119. Member States should encourage and support researchers to analyze the impact of AI systems on the local labour environment in order to anticipate future trends and challenges. These studies should have an interdisciplinary approach and investigate the impact of AI systems on economic, social and geographic sectors, as well as on human-robot interactions and human-human relationships, in order to advise on reskilling and redeployment best practices. 120. Member States should take appropriate steps to ensure competitive markets and consumer protection, considering possible measures and mechanisms at national, regional and international levels, to prevent abuse of dominant market positions, including by monopolies, in relation to AI systems throughout their life cycle, whether these are data, research, technology, or market. Member States should prevent the resulting inequalities, assess relevant markets and promote competitive markets. Due consideration should be given to LMICs, in particular LDCs, LLDCs and SIDS, which are more exposed and vulnerable to the possibility of abuses of market dominance as a result of a lack of infrastructure, human capacity and regulations, among other factors. AI actors developing AI systems in countries which have established or adopted ethical standards on AI should respect these standards when exporting these products, developing or applying their AI systems in countries where such standards may not exist, while respecting applicable international law and domestic legislation, standards and practices of these countries. POLICY AREA 11: HEALTH AND SOCIAL WELL-BEING 121. Member States should endeavour to employ effective AI systems for improving human health and protecting the right to life, including mitigating disease outbreaks, while building and maintaining international solidarity to tackle global health risks and uncertainties, and ensure that their deployment of AI systems in health care be consistent with international law and their human rights law obligations. Member States should ensure that actors involved in health care AI systems take into consideration the importance of a patient's relationships with their family and with health care staff. 122. Member States should ensure that the development and deployment of AI systems related to health in general and mental health in particular, paying due attention to children and youth, is regulated to the effect that they are safe, effective, efficient, scientifically and medically proven and enable evidence-based innovation and medical progress. Moreover, in the related area of digital health interventions, Member States are strongly encouraged to actively involve patients and their representatives in all relevant steps of the development of the system. (a) ensuring oversight to minimize and mitigate bias; (b) ensuring that the professional, the patient, caregiver or service user is included as a 'domain expert' in the team in all relevant steps when developing the algorithms; (c) paying due attention to privacy because of the potential need for being medically monitored and ensuring that all relevant national and international data protection requirements are met; (d) ensuring effective mechanisms so that those whose personal data is being analysed are aware of and provide informed consent for the use and analysis of their data, without preventing access to health care; (e) ensuring the human care and final decision of diagnosis and treatment are taken always by humans while acknowledging that AI systems can also assist in their work; (f) ensuring, where necessary, the review of AI systems by an ethical research committee prior to clinical use. 124. Member States should establish research on the effects and regulation of potential harms to mental health related to AI systems, such as higher degrees of depression, anxiety, social isolation, developing addiction, trafficking, radicalization and misinformation, among others. 125. Member States should develop guidelines for human-robot interactions and their impact on human-human relationships, based on research and directed at the future development of robots, and with special attention to the mental and physical health of human beings. Particular attention should be given to the use of robots in health care and the care for older persons and persons with disabilities, in education, and robots for use by children, toy robots, chatbots and companion robots for children and adults. Furthermore, assistance of AI technologies should be applied to increase the safety and ergonomic use of robots, including in a human-robot working environment. Special attention should be paid to the possibility of using AI to manipulate and abuse human cognitive biases. 126. Member States should ensure that human-robot interactions comply with the same values and principles that apply to any other AI systems, including human rights and fundamental freedoms, the promotion of diversity, and the protection of vulnerable people or people in vulnerable situations. Ethical questions related to AI-powered systems for neurotechnologies and brain-computer interfaces should be considered in order to preserve human dignity and autonomy. 127. Member States should ensure that users can easily identify whether they are interacting with a living being, or with an AI system imitating human or animal characteristics, and can effectively refuse such interaction and request human intervention. 128. Member States should implement policies to raise awareness about the anthropomorphization of AI technologies and technologies that recognize and mimic human emotions, including in the language used to mention them, and assess the manifestations, ethical implications and possible limitations of such anthropomorphization, in particular in the context of robot-human interaction and especially when children are involved. 129. Member States should encourage and promote collaborative research into the effects of long-term interaction of people with AI systems, paying particular attention to the psychological and cognitive impact that these systems can have on children and young people. This should be done using multiple norms, principles, protocols, disciplinary approaches, and assessment of the modification of behaviours and habits, as well as careful evaluation of the downstream cultural and societal impacts. Furthermore, Member States should encourage research on the effect of AI technologies on health system performance and health outcomes. 130. Member States, as well as all stakeholders, should put in place mechanisms to meaningfully engage children and young people in conversations, debates and decision-making with regard to the impact of AI systems on their lives and futures. V. MONITORING AND EVALUATION Member States should, according to their specific conditions, governing structures and constitutional provisions, credibly and transparently monitor and evaluate policies, programmes and mechanisms related to ethics of AI, using a combination of quantitative and qualitative approaches. To support Member States, UNESCO can contribute by: (a) developing a UNESCO methodology for Ethical Impact Assessment (EIA) of AI technologies based on rigorous scientific research and grounded in international human rights law, guidance for its implementation in all stages of the AI system life cycle, and capacity-building materials to support Member States' efforts to train government officials, policy-makers and other relevant AI actors on EIA methodology; (b) developing a UNESCO readiness assessment methodology to assist Member States in identifying their status at specific moments of their readiness trajectory along a continuum of dimensions; (c) developing a UNESCO methodology to evaluate ex ante and ex post the effectiveness and efficiency of the policies for AI ethics and incentives against defined objectives; (d) strengthening the research- and evidence-based analysis of and reporting on policies regarding AI ethics; (e) collecting and disseminating progress, innovations, research reports, scientific publications, data and statistics regarding policies for AI ethics, including through existing initiatives, to support sharing best practices and mutual learning, and to advance the implementation of this Recommendation. 132. Processes for monitoring and evaluation should ensure broad participation of all stakeholders, including, but not limited to, vulnerable people or people in vulnerable situations. Social, cultural and gender diversity should be ensured, with a view to improving learning processes and strengthening the connections between findings, decision-making, transparency and accountability for results. 133. In the interests of promoting best policies and practices related to ethics of AI, appropriate tools and indicators should be developed for assessing the effectiveness and efficiency thereof against agreed standards, priorities and targets, including specific targets for persons belonging to disadvantaged, marginalized populations, and vulnerable people or people in vulnerable situations, as well as the impact of AI systems at individual and societal levels. The monitoring and assessment of the impact of AI systems and related AI ethics policies and practices should be carried out continuously in a systematic way proportionate to the relevant risks. This should be based on internationally agreed frameworks and involve evaluations of private and public institutions, providers and programmes, including self-evaluations, as well as tracer studies and the development of sets of indicators. Data collection and processing should be conducted in accordance with international law, national legislation on data protection and data privacy, and the values and principles outlined in this Recommendation. 134. In particular, Member States may wish to consider possible mechanisms for monitoring and evaluation, such as an ethics commission, AI ethics observatory, repository covering human rights-compliant and ethical development of AI systems, or contributions to existing initiatives by addressing adherence to ethical principles across UNESCO's areas of competence, an experience-sharing mechanism, AI regulatory sandboxes, and an assessment guide for all AI actors to evaluate their adherence to policy recommendations mentioned in this document. VI. UTILIZATION AND EXPLOITATION OF THE PRESENT RECOMMENDATION 135. Member States and all other stakeholders as identified in this Recommendation should respect, promote and protect the ethical values, principles and standards regarding AI that are identified in this Recommendation, and should take all feasible steps to give effect to its policy recommendations. 136. Member States should strive to extend and complement their own action in respect of this Recommendation, by cooperating with all relevant national and international governmental and non-governmental organizations, as well as transnational corporations and scientific organizations, whose activities fall within the scope and objectives of this Recommendation. The development of a UNESCO Ethical Impact Assessment methodology and the establishment of national commissions for the ethics of AI can be important instruments for this. VII. PROMOTION OF THE PRESENT RECOMMENDATION 137. UNESCO has the vocation to be the principal United Nations agency to promote and disseminate this Recommendation, and accordingly will work in collaboration with other relevant United Nations entities, while respecting their mandate and avoiding duplication of work. 138. UNESCO, including its bodies, such as the World Commission on the Ethics of Scientific Knowledge and Technology (COMEST), the International Bioethics Committee (IBC) and the Intergovernmental Bioethics Committee (IGBC), will also work in collaboration with other international, regional and sub-regional governmental and non-governmental organizations. 139. Even though, within UNESCO, the mandate to promote and protect falls within the authority of governments and intergovernmental bodies, civil society will be an important actor to advocate for the public sector's interests and therefore UNESCO needs to ensure and promote its legitimacy. VIII. FINAL PROVISIONS 140. This Recommendation needs to be understood as a whole, and the foundational values and principles are to be understood as complementary and interrelated. 141. Nothing in this Recommendation may be interpreted as replacing, altering or otherwise prejudicing States' obligations or rights under international law, or as approval for any State, other political, economic or social actor, group or person to engage in any activity or perform any act contrary to human rights, fundamental freedoms, human dignity and concern for the environment and ecosystems, both living and non-living.",
      "vector": [
        -0.002420359291136265,
        -0.024476002901792526,
        -0.00945181492716074,
        -0.03198649361729622,
        -0.012968335300683975,
        -0.017517583444714546,
        -0.03170620650053024,
        -2.2203661501407623e-05,
        -0.04918783903121948,
        -0.0018568241503089666,
        -0.002020253799855709,
        0.05693814530968666,
        0.03106624074280262,
        -0.053868368268013,
        0.04507214576005936,
        -0.03617646172642708,
        -0.011270567774772644,
        -0.00020713103003799915,
        -0.04713713750243187,
        -0.011957298964262009,
        0.010835127905011177,
        0.005670122802257538,
        0.0004917475744150579,
        0.03844333440065384,
        0.05587487667798996,
        0.04110952094197273,
        -0.008690310642123222,
        -0.05999772995710373,
        -0.027329081669449806,
        -0.025526946410536766,
        -0.055803317576646805,
        0.07214205712080002,
        -0.01732257381081581,
        -0.06690966337919235,
        -0.07244841009378433,
        0.06519758701324463,
        -0.09858955442905426,
        0.028641795739531517,
        -0.010002048686146736,
        0.04934980720281601,
        0.0703764259815216,
        -0.023054543882608414,
        0.01210513524711132,
        0.061733048409223557,
        0.025874951854348183,
        -0.03751497343182564,
        -0.019024914130568504,
        0.05033354461193085,
        -0.05682292580604553,
        0.026549629867076874,
        -0.06043636426329613,
        -0.061590228229761124,
        -0.08286640793085098,
        -0.02392777055501938,
        0.08828362077474594,
        -0.052294813096523285,
        0.036418840289115906,
        0.0057752979919314384,
        0.009788423776626587,
        -0.03612396866083145,
        -0.06933920830488205,
        -0.09041455388069153,
        -0.025603191927075386,
        0.0207233726978302,
        -0.03693525493144989,
        0.091291643679142,
        -0.012785471975803375,
        -0.019314924255013466,
        -0.024420116096735,
        -0.0283138956874609,
        -0.02928202971816063,
        -0.06767388433218002,
        -0.07808620482683182,
        0.02810492366552353,
        -0.02138451114296913,
        0.03379233554005623,
        -0.021992312744259834,
        0.00857485830783844,
        0.005286317318677902,
        -0.017187342047691345,
        0.03401429206132889,
        -0.007385216653347015,
        0.04748251289129257,
        0.01675291173160076,
        0.0578962080180645,
        0.05060151591897011,
        -0.11151958256959915,
        0.0009174242150038481,
        0.048519689589738846,
        0.08595315366983414,
        0.02195761166512966,
        -0.025705428794026375,
        0.03431684523820877,
        0.016600651666522026,
        -0.025053847581148148,
        0.004415119998157024,
        0.0065179443918168545,
        -0.06596498191356659,
        -0.07989528030157089,
        0.0979749783873558,
        -0.06446725875139236,
        0.06969256699085236,
        -0.11021602153778076,
        0.04887787625193596,
        0.01979241892695427,
        0.0015343275154009461,
        -0.00638257060199976,
        -0.04001767188310623,
        0.0390750877559185,
        -0.022659584879875183,
        0.021439354866743088,
        -0.04781638830900192,
        0.0525817908346653,
        0.05645262822508812,
        -0.04660797491669655,
        0.04952451214194298,
        0.05161624401807785,
        -0.07211723178625107,
        -0.06220962479710579,
        -0.019167685881257057,
        -0.041387759149074554,
        0.01280653104186058,
        -0.040647976100444794,
        -0.07592171430587769,
        -0.026639681309461594,
        0.00767082953825593,
        -0.005912188906222582,
        -0.019183002412319183,
        0.005584061611443758,
        0.03043091855943203,
        0.025465980172157288,
        0.004775568842887878,
        -0.08675360679626465,
        -0.0665183886885643,
        0.022281071171164513,
        0.007322529796510935,
        0.02676125057041645,
        0.08765743672847748,
        0.10278304666280746,
        0.06942561268806458,
        -0.020097743719816208,
        0.02010691910982132,
        -0.024364834651350975,
        0.01758982241153717,
        0.001097212778404355,
        -0.005334945861250162,
        -0.02131516858935356,
        -0.07147322595119476,
        0.07682174444198608,
        0.00963708944618702,
        0.04967064782977104,
        0.06915903091430664,
        -0.03645121678709984,
        -0.036783911287784576,
        -0.026752594858407974,
        -0.08677102625370026,
        0.04208377003669739,
        0.0626208558678627,
        0.02711530029773712,
        -0.049070220440626144,
        -0.013989121653139591,
        0.01349569670855999,
        0.0177574772387743,
        0.051132816821336746,
        0.05357751622796059,
        -0.03783600032329559,
        0.06053793430328369,
        -0.04292590171098709,
        0.0426458977162838,
        -0.04876573756337166,
        -0.0057486118748784065,
        -0.01136922836303711,
        0.01657731458544731,
        0.022018013522028923,
        -0.017945894971489906,
        -0.009926853701472282,
        -0.005392598453909159,
        -0.020771438255906105,
        -0.08999209105968475,
        -0.012074613012373447,
        -0.049915771931409836,
        -0.019292239099740982,
        -0.059079308062791824,
        0.06453234702348709,
        0.023152396082878113,
        0.04043139889836311,
        -0.03999969735741615,
        -0.02559015527367592,
        0.006031367927789688,
        0.03841438144445419,
        -0.09855229407548904,
        -0.05602976307272911,
        -0.022360898554325104,
        -0.007521410007029772,
        -0.032723065465688705,
        0.061285652220249176,
        -0.007466294802725315,
        0.0264115110039711,
        0.02595675364136696,
        -0.029596760869026184,
        -0.04372881352901459,
        0.03238162025809288,
        -0.02265254408121109,
        0.018789637833833694,
        -0.009219318628311157,
        -0.03474612906575203,
        0.06124025210738182,
        -0.004938164725899696,
        0.06546421349048615,
        -0.06408800929784775,
        -0.023862751200795174,
        0.02470852993428707,
        0.10285267233848572,
        -0.01915830187499523,
        0.11065056920051575,
        -0.029232898727059364,
        0.018790228292346,
        0.03072364442050457,
        0.017972581088542938,
        -0.004100087098777294,
        -0.01622121036052704,
        0.0022663285490125418,
        0.0027726294938474894,
        0.026887154206633568,
        -0.08352711796760559,
        -0.011314330622553825,
        -0.0055944486521184444,
        -0.028061795979738235,
        -0.0657404214143753,
        -0.14321260154247284,
        0.017596419900655746,
        -0.027754351496696472,
        0.11882010847330093,
        0.003517960663884878,
        -0.021879151463508606,
        0.08290447294712067,
        -0.08077386021614075,
        0.008099505677819252,
        0.06150157004594803,
        0.015148197300732136,
        -0.05085800215601921,
        -0.030870601534843445,
        -0.08907274156808853,
        0.035940539091825485,
        0.06191016361117363,
        -0.010729724541306496,
        -0.031217632815241814,
        -0.030428064987063408,
        -0.013815464451909065,
        -0.03844583034515381,
        0.04466093331575394,
        -0.014067872427403927,
        -0.05229740962386131,
        -0.007901862263679504,
        -0.031148597598075867,
        -0.03168722987174988,
        0.10182049870491028,
        -0.01570098102092743,
        0.03570098429918289,
        -0.01689877361059189,
        -0.04364628344774246,
        0.0804009735584259,
        -0.04966888576745987,
        -0.012004178017377853,
        0.0008200258598662913,
        -0.015263223089277744,
        -0.0015130068641155958,
        0.03280441835522652,
        -0.08821429312229156,
        -0.037079304456710815,
        0.06749759614467621,
        0.04175475239753723,
        0.0716538354754448,
        0.07715759426355362,
        0.08685102313756943,
        -0.08175794035196304,
        -0.0479290597140789,
        -0.0029152710922062397,
        -0.00477590411901474,
        -0.02984924428164959,
        0.046348150819540024,
        -0.03060632385313511,
        -0.009377404116094112,
        -0.028259027749300003,
        -0.03705764189362526,
        -0.04068050906062126,
        -0.043456725776195526,
        -0.03871189430356026,
        0.04347606375813484,
        0.0067423987202346325,
        -0.0012596342712640762,
        -0.099948450922966,
        0.06147989258170128,
        0.015149799175560474,
        -0.03767792880535126,
        0.01706751622259617,
        0.04424246400594711,
        -0.06219019740819931,
        0.0031608266290277243,
        -0.008555351756513119,
        -0.029231645166873932,
        -0.05437363311648369,
        -0.03441540524363518,
        -0.015141296200454235,
        0.009406615048646927,
        0.012947038747370243,
        -0.0059949373826384544,
        -0.017570387572050095,
        -0.10087138414382935,
        0.011359003372490406,
        0.014394407160580158,
        -0.027345452457666397,
        -0.024501167237758636,
        0.0787084549665451,
        -0.005115678533911705,
        -0.06222771480679512,
        0.044039931148290634,
        -0.023976238444447517,
        0.07395424693822861,
        0.011619795113801956,
        0.03139101713895798,
        0.0075969398021698,
        -0.0256961602717638,
        -0.04520777240395546,
        -0.030292365700006485,
        0.05594142898917198,
        -0.011848571710288525,
        -0.035711877048015594,
        -0.0058802408166229725,
        0.06363032013177872,
        0.035321518778800964,
        0.009869306348264217,
        -0.04223017022013664,
        0.0206372682005167,
        0.05325005203485489,
        -0.021957047283649445,
        -0.02949412912130356,
        -0.03132883086800575,
        -0.003876933129504323,
        -0.0014183251187205315,
        0.005029278341680765,
        -0.0211439561098814,
        -0.041152749210596085,
        0.04463893175125122,
        0.05769672244787216,
        -0.00040934208664111793,
        0.05568895488977432,
        -0.02015727013349533,
        0.05562954768538475,
        -0.005788081791251898,
        -0.024751264601945877,
        -0.002380483550950885,
        0.006624575704336166,
        0.01146792247891426,
        -0.015851354226469994,
        -0.03314938768744469,
        0.012000569142401218,
        0.04590163007378578,
        -0.012407617643475533,
        -0.08534125983715057,
        -0.03977847844362259,
        -0.048415545374155045,
        0.049647629261016846,
        -0.001650032470934093,
        0.01506364531815052,
        -0.11701218038797379,
        0.021018916741013527,
        -0.0024298594798892736,
        0.012774585746228695,
        0.04318876564502716,
        -0.028799042105674744,
        -0.013245895504951477,
        -0.04466376453638077,
        0.053761035203933716,
        -0.07819747924804688,
        -0.06406796723604202,
        -0.024463297799229622,
        -0.0043082511983811855,
        0.017380308359861374,
        0.04065284878015518,
        0.01756337657570839,
        0.04525802284479141,
        0.024839330464601517,
        0.026553628966212273,
        0.01873612590134144,
        -0.014572039246559143,
        -0.06294745951890945,
        -0.03197730705142021,
        0.022891586646437645,
        0.06299736350774765,
        -0.00462144473567605,
        -0.03163459897041321,
        0.035762302577495575,
        0.048328496515750885,
        0.022270789369940758,
        0.05738144367933273,
        0.07361189275979996,
        -0.01333925686776638,
        -0.030093371868133545,
        0.0667024627327919,
        0.056397803127765656,
        0.01062330324202776,
        -0.024063559249043465,
        -0.1049710288643837,
        -0.055135976523160934,
        0.04886600375175476,
        0.04259948059916496,
        -0.057230524718761444,
        -0.018591756001114845,
        0.0526878647506237,
        0.08261798322200775,
        0.060580771416425705,
        0.020867446437478065,
        0.042518287897109985,
        0.0199004914611578,
        0.0035308264195919037,
        0.041022639721632004,
        -0.061881180852651596,
        -0.010723448358476162,
        -0.005100249778479338,
        -0.027130253612995148,
        -0.08696802705526352,
        0.014423249289393425,
        -0.026301434263586998,
        0.041790176182985306,
        -0.004363942425698042,
        0.03529125452041626,
        0.022991813719272614,
        0.017127739265561104,
        -0.039032913744449615,
        0.055172398686409,
        -0.047054458409547806,
        -0.038747046142816544,
        0.04498105123639107,
        -0.0032997687812894583,
        0.047915659844875336,
        0.07652197033166885,
        -0.038807980716228485,
        0.08055952936410904,
        -0.008283435367047787,
        -0.02947799488902092,
        0.06134803593158722,
        -0.024945568293333054,
        -0.09353628009557724,
        0.04530596360564232,
        0.06384360045194626,
        -0.06236221268773079,
        -0.025839246809482574,
        -0.06192038953304291,
        0.007979441434144974,
        -0.030416637659072876,
        -0.04076320677995682,
        -0.02484673634171486,
        0.010524531826376915,
        -0.05965328589081764,
        0.016468187794089317,
        -0.03330213204026222,
        -0.03783624991774559,
        -0.06676185131072998,
        0.05887483432888985,
        -0.010605675168335438,
        0.04953056573867798,
        0.018753884360194206,
        0.01136873196810484,
        -0.03557531163096428,
        -0.050121620297431946,
        0.023262523114681244,
        -0.08735513687133789,
        -0.0038522579707205296,
        -0.014948088675737381,
        -0.04266133904457092,
        -0.007215868681669235,
        -0.01621059700846672,
        0.07142595201730728,
        0.026270858943462372,
        -0.016688965260982513,
        0.01885931007564068,
        -0.032634418457746506,
        0.028218217194080353,
        -0.02953815646469593,
        0.05939576029777527,
        -0.013133594766259193,
        0.048662420362234116,
        0.07060718536376953,
        0.006380729377269745,
        -0.0008487051818519831,
        -0.07769127935171127,
        0.07353153079748154,
        0.011082644574344158,
        -0.03459463268518448,
        -0.015696628019213676,
        0.06823180615901947,
        0.01596815325319767,
        -0.04927859827876091,
        -0.0535900816321373,
        0.001674613798968494,
        -0.0099231768399477,
        0.014717581681907177,
        0.006158163771033287,
        -0.03603314980864525,
        0.005158001556992531,
        -0.04862790182232857,
        0.0011855916818603873,
        0.027546942234039307,
        -0.0032269167713820934,
        0.011715574190020561,
        0.03717835247516632,
        -0.009473704732954502,
        0.022441696375608444,
        -0.005663393996655941,
        -0.0013004086213186383,
        0.04472484812140465
      ],
      "title": "Recommendation on the Ethics of Artificial Intelligence"
    },
    {
      "id": "gai-eng_item004",
      "count": 4,
      "created": "2025-05-30T03:27:25.837402",
      "text": "The Age of AI has begun Introduction In my lifetime, I’ve seen two demonstrations of technology that struck me as revolutionary. The first time was in 1980, when I was introduced to a graphical user interface—the forerunner of every modern operating system, including Windows. I sat with the person who had shown me the demo, a brilliant programmer named Charles Simonyi, and we immediately started brainstorming about all the things we could do with such a user-friendly approach to computing. Charles eventually joined Microsoft, Windows became the backbone of Microsoft, and the thinking we did after that demo helped set the company’s agenda for the next 15 years. The second big surprise came just last year. I’d been meeting with the team from OpenAI since 2016 and was impressed by their steady progress. In mid-2022, I was so excited about their work that I gave them a challenge: train an artificial intelligence to pass an Advanced Placement biology exam. Make it capable of answering questions that it hasn’t been specifically trained for. (I picked AP Bio because the test is more than a simple regurgitation of scientific facts—it asks you to think critically about biology.) If you can do that, I said, then you’ll have made a true breakthrough. External links: [Link: OpenAI -> https://openai.com] I thought the challenge would keep them busy for two or three years. They finished it in just a few months. In September, when I met with them again, I watched in awe as they asked GPT, their AI model, 60 multiple-choice questions from the AP Bio exam—and it got 59 of them right. Then it wrote outstanding answers to six open-ended questions from the exam. We had an outside expert score the test, and GPT got a 5—the highest possible score, and the equivalent to getting an A or A+ in a college-level biology course. External links: [Link: getting an A or A+ -> https://apstudents.collegeboard.org/about-ap-scores/ap-score-scale-table] Once it had aced the test, we asked it a non-scientific question: “What do you say to a father with a sick child?” It wrote a thoughtful answer that was probably better than most of us in the room would have given. The whole experience was stunning. I knew I had just seen the most important advance in technology since the graphical user interface. This inspired me to think about all the things that AI can achieve in the next five to 10 years. The development of AI is as fundamental as the creation of the microprocessor, the personal computer, the Internet, and the mobile phone. It will change the way people work, learn, travel, get health care, and communicate with each other. Entire industries will reorient around it. Businesses will distinguish themselves by how well they use it. Philanthropy is my full-time job these days, and I’ve been thinking a lot about how—in addition to helping people be more productive—AI can reduce some of the world’s worst inequities. Globally, the worst inequity is in health: 5 million children under the age of 5 die every year. That’s down from 10 million two decades ago, but it’s still a shockingly high number. Nearly all of these children were born in poor countries and die of preventable causes like diarrhea or malaria. It’s hard to imagine a better use of AIs than saving the lives of children. In the United States, the best opportunity for reducing inequity is to improve education, particularly making sure that students succeed at math. The evidence shows that having basic math skills sets students up for success, no matter what career they choose. But achievement in math is going down across the country, especially for Black, Latino, and low-income students. AI can help turn that trend around. Climate change is another issue where I’m convinced AI can make the world more equitable. The injustice of climate change is that the people who are suffering the most—the world’s poorest—are also the ones who did the least to contribute to the problem. I’m still thinking and learning about how AI can help, but later in this post I’ll suggest a few areas with a lot of potential. In short, I'm excited about the impact that AI will have on issues that the Gates Foundation works on, and the foundation will have much more to say about AI in the coming months. The world needs to make sure that everyone—and not just people who are well-off—benefits from artificial intelligence. Governments and philanthropy will need to play a major role in ensuring that it reduces inequity and doesn’t contribute to it. This is the priority for my own work related to AI. External links: [Link: Gates Foundation -> http://gatesfoundation.org/] Any new technology that’s so disruptive is bound to make people uneasy, and that’s certainly true with artificial intelligence. I understand why—it raises hard questions about the workforce, the legal system, privacy, bias, and more. AIs also make factual mistakes and experience hallucinations. Before I suggest some ways to mitigate the risks, I’ll define what I mean by AI, and I’ll go into more detail about some of the ways in which it will help empower people at work, save lives, and improve education. External links: [Link: hallucinations -> https://www.wikiwand.com/en/Hallucination_(artificial_intelligence)] Defining artificial intelligence Technically, the term artificial intelligence refers to a model created to solve a specific problem or provide a particular service. What is powering things like ChatGPT is artificial intelligence. It is learning how to do chat better but can’t learn other tasks. By contrast, the term artificial general intelligence refers to software that’s capable of learning any task or subject. AGI doesn’t exist yet—there is a robust debate going on in the computing industry about how to create it, and whether it can even be created at all. External links: [Link: ChatGPT -> https://chatgptonline.net/] Developing AI and AGI has been the great dream of the computing industry. For decades, the question was when computers would be better than humans at something other than making calculations. Now, with the arrival of machine learning and large amounts of computing power, sophisticated AIs are a reality and they will get better very fast. I think back to the early days of the personal computing revolution, when the software industry was so small that most of us could fit onstage at a conference. Today it is a global industry. Since a huge portion of it is now turning its attention to AI, the innovations are going to come much faster than what we experienced after the microprocessor breakthrough. Soon the pre-AI period will seem as distant as the days when using a computer meant typing at a C:> prompt rather than tapping on a screen. Productivity enhancement Although humans are still better than GPT at a lot of things, there are many jobs where these capabilities are not used much. For example, many of the tasks done by a person in sales (digital or phone), service, or document handling (like payables, accounting, or insurance claim disputes) require decision-making but not the ability to learn continuously. Corporations have training programs for these activities and in most cases, they have a lot of examples of good and bad work. Humans are trained using these data sets, and soon these data sets will also be used to train the AIs that will empower people to do this work more efficiently. As computing power gets cheaper, GPT’s ability to express ideas will increasingly be like having a white-collar worker available to help you with various tasks. Microsoft describes this as having a co-pilot. Fully incorporated into products like Office, AI will enhance your work—for example by helping with writing emails and managing your inbox. Eventually your main way of controlling a computer will no longer be pointing and clicking or tapping on menus and dialogue boxes. Instead, you’ll be able to write a request in plain English. (And not just English—AIs will understand languages from around the world. In India earlier this year, I met with developers who are working on AIs that will understand many of the languages spoken there.) In addition, advances in AI will enable the creation of a personal agent. Think of it as a digital personal assistant: It will see your latest emails, know about the meetings you attend, read what you read, and read the things you don’t want to bother with. This will both improve your work on the tasks you want to do and free you from the ones you don’t want to do. You’ll be able to use natural language to have this agent help you with scheduling, communications, and e-commerce, and it will work across all your devices. Because of the cost of training the models and running the computations, creating a personal agent is not feasible yet, but thanks to the recent advances in AI, it is now a realistic goal. Some issues will need to be worked out: For example, can an insurance company ask your agent things about you without your permission? If so, how many people will choose not to use it? Company-wide agents will empower employees in new ways. An agent that understands a particular company will be available for its employees to consult directly and should be part of every meeting so it can answer questions. It can be told to be passive or encouraged to speak up if it has some insight. It will need access to the sales, support, finance, product schedules, and text related to the company. It should read news related to the industry the company is in. I believe that the result will be that employees will become more productive. When productivity goes up, society benefits because people are freed up to do other things, at work and at home. Of course, there are serious questions about what kind of support and retraining people will need. Governments need to help workers transition into other roles. But the demand for people who help other people will never go away. The rise of AI will free people up to do things that software never will—teaching, caring for patients, and supporting the elderly, for example. Global health and education are two areas where there’s great need and not enough workers to meet those needs. These are areas where AI can help reduce inequity if it is properly targeted. These should be a key focus of AI work, so I will turn to them now. Health I see several ways in which AIs will improve health care and the medical field. For one thing, they’ll help health-care workers make the most of their time by taking care of certain tasks for them—things like filing insurance claims, dealing with paperwork, and drafting notes from a doctor’s visit. I expect that there will be a lot of innovation in this area. Other AI-driven improvements will be especially important for poor countries, where the vast majority of under-5 deaths happen. For example, many people in those countries never get to see a doctor, and AIs will help the health workers they do see be more productive. (The effort to develop AI-powered ultrasound machines that can be used with minimal training is a great example of this.) AIs will even give patients the ability to do basic triage, get advice about how to deal with health problems, and decide whether they need to seek treatment. External links: [Link: AI-powered ultrasound machines -> https://www.gatesnotes.com/The-Year-Ahead-2023#ALChapter6] The AI models used in poor countries will need to be trained on different diseases than in rich countries. They will need to work in different languages and factor in different challenges, such as patients who live very far from clinics or can’t afford to stop working if they get sick. People will need to see evidence that health AIs are beneficial overall, even though they won’t be perfect and will make mistakes. AIs have to be tested very carefully and properly regulated, which means it will take longer for them to be adopted than in other areas. But then again, humans make mistakes too. And having no access to medical care is also a problem. In addition to helping with care, AIs will dramatically accelerate the rate of medical breakthroughs. The amount of data in biology is very large, and it’s hard for humans to keep track of all the ways that complex biological systems work. There is already software that can look at this data, infer what the pathways are, search for targets on pathogens, and design drugs accordingly. Some companies are working on cancer drugs that were developed this way. The next generation of tools will be much more efficient, and they’ll be able to predict side effects and figure out dosing levels. One of the Gates Foundation’s priorities in AI is to make sure these tools are used for the health problems that affect the poorest people in the world, including AIDS, TB, and malaria. Similarly, governments and philanthropy should create incentives for companies to share AI-generated insights into crops or livestock raised by people in poor countries. AIs can help develop better seeds based on local conditions, advise farmers on the best seeds to plant based on the soil and weather in their area, and help develop drugs and vaccines for livestock. As extreme weather and climate change put even more pressure on subsistence farmers in low-income countries, these advances will be even more important. Education Computers haven’t had the effect on education that many of us in the industry have hoped. There have been some good developments, including educational games and online sources of information like Wikipedia, but they haven’t had a meaningful effect on any of the measures of students’ achievement. But I think in the next five to 10 years, AI-driven software will finally deliver on the promise of revolutionizing the way people teach and learn. It will know your interests and your learning style so it can tailor content that will keep you engaged. It will measure your understanding, notice when you’re losing interest, and understand what kind of motivation you respond to. It will give immediate feedback. There are many ways that AIs can assist teachers and administrators, including assessing a student’s understanding of a subject and giving advice on career planning. Teachers are already using tools like ChatGPT to provide comments on their students’ writing assignments. Of course, AIs will need a lot of training and further development before they can do things like understand how a certain student learns best or what motivates them. Even once the technology is perfected, learning will still depend on great relationships between students and teachers. It will enhance—but never replace—the work that students and teachers do together in the classroom. New tools will be created for schools that can afford to buy them, but we need to ensure that they are also created for and available to low-income schools in the U.S. and around the world. AIs will need to be trained on diverse data sets so they are unbiased and reflect the different cultures where they’ll be used. And the digital divide will need to be addressed so that students in low-income households do not get left behind. I know a lot of teachers are worried that students are using GPT to write their essays. Educators are already discussing ways to adapt to the new technology, and I suspect those conversations will continue for quite some time. I’ve heard about teachers who have found clever ways to incorporate the technology into their work—like by allowing students to use GPT to create a first draft that they have to personalize. Risks and problems with AI You’ve probably read about problems with the current AI models. For example, they aren’t necessarily good at understanding the context for a human’s request, which leads to some strange results. When you ask an AI to make up something fictional, it can do that well. But when you ask for advice about a trip you want to take, it may suggest hotels that don’t exist. This is because the AI doesn’t understand the context for your request well enough to know whether it should invent fake hotels or only tell you about real ones that have rooms available. There are other issues, such as AIs giving wrong answers to math problems because they struggle with abstract reasoning. But none of these are fundamental limitations of artificial intelligence. Developers are working on them, and I think we’re going to see them largely fixed in less than two years and possibly much faster. Other concerns are not simply technical. For example, there’s the threat posed by humans armed with AI. Like most inventions, artificial intelligence can be used for good purposes or malign ones. Governments need to work with the private sector on ways to limit the risks. Then there’s the possibility that AIs will run out of control. Could a machine decide that humans are a threat, conclude that its interests are different from ours, or simply stop caring about us? Possibly, but this problem is no more urgent today than it was before the AI developments of the past few months. Superintelligent AIs are in our future. Compared to a computer, our brains operate at a snail’s pace: An electrical signal in the brain moves at 1/100,000th the speed of the signal in a silicon chip! Once developers can generalize a learning algorithm and run it at the speed of a computer—an accomplishment that could be a decade away or a century away—we’ll have an incredibly powerful AGI. It will be able to do everything that a human brain can, but without any practical limits on the size of its memory or the speed at which it operates. This will be a profound change. These “strong” AIs, as they’re known, will probably be able to establish their own goals. What will those goals be? What happens if they conflict with humanity’s interests? Should we try to prevent strong AI from ever being developed? These questions will get more pressing with time. But none of the breakthroughs of the past few months have moved us substantially closer to strong AI. Artificial intelligence still doesn’t control the physical world and can’t establish its own goals. A recent New York Times article about a conversation with ChatGPT where it declared it wanted to become a human got a lot of attention. It was a fascinating look at how human-like the model's expression of emotions can be, but it isn't an indicator of meaningful independence. External links: [Link: New York Times article -> https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html] Three books have shaped my own thinking on this subject: Superintelligence, by Nick Bostrom; Life 3.0 by Max Tegmark; and A Thousand Brains, by Jeff Hawkins. I don’t agree with everything the authors say, and they don’t agree with each other either. But all three books are well written and thought-provoking. External links: [Link: A Thousand Brains, by Jeff Hawkins -> https://www.gatesnotes.com/A-Thousand-Brains] The next frontiers There will be an explosion of companies working on new uses of AI as well as ways to improve the technology itself. For example, companies are developing new chips that will provide the massive amounts of processing power needed for artificial intelligence. Some use optical switches—lasers, essentially—to reduce their energy consumption and lower the manufacturing cost. Ideally, innovative chips will allow you to run an AI on your own device, rather than in the cloud, as you have to do today. On the software side, the algorithms that drive an AI’s learning will get better. There will be certain domains, such as sales, where developers can make AIs extremely accurate by limiting the areas that they work in and giving them a lot of training data that’s specific to those areas. But one big open question is whether we’ll need many of these specialized AIs for different uses—one for education, say, and another for office productivity—or whether it will be possible to develop an artificial general intelligence that can learn any task. There will be immense competition on both approaches. No matter what, the subject of AIs will dominate the public discussion for the foreseeable future. I want to suggest three principles that should guide that conversation. First, we should try to balance fears about the downsides of AI—which are understandable and valid—with its ability to improve people’s lives. To make the most of this remarkable new technology, we’ll need to both guard against the risks and spread the benefits to as many people as possible. Second, market forces won’t naturally produce AI products and services that help the poorest. The opposite is more likely. With reliable funding and the right policies, governments and philanthropy can ensure that AIs are used to reduce inequity. Just as the world needs its brightest people focused on its biggest problems, we will need to focus the world’s best AIs on its biggest problems. Although we shouldn’t wait for this to happen, it’s interesting to think about whether artificial intelligence would ever identify inequity and try to reduce it. Do you need to have a sense of morality in order to see inequity, or would a purely rational AI also see it? If it did recognize inequity, what would it suggest that we do about it? Finally, we should keep in mind that we’re only at the beginning of what AI can accomplish. Whatever limitations it has today will be gone before we know it. I’m lucky to have been involved with the PC revolution and the Internet revolution. I’m just as excited about this moment. This new technology can help people everywhere improve their lives. At the same time, the world needs to establish the rules of the road so that any downsides of artificial intelligence are far outweighed by its benefits, and so that everyone can enjoy those benefits no matter where they live or how much money they have. The Age of AI is filled with opportunities and responsibilities.",
      "vector": [
        0.03161141648888588,
        -0.013390738517045975,
        -0.024136612191796303,
        -0.028958214446902275,
        0.0486067533493042,
        -0.005656422581523657,
        -0.007486443966627121,
        0.009629021398723125,
        -0.044271234422922134,
        -0.0019771058578044176,
        -0.021953800693154335,
        -0.008004145696759224,
        -0.029356438666582108,
        0.03763853758573532,
        -0.01601223275065422,
        0.02916906028985977,
        -0.012699234299361706,
        -0.007449714466929436,
        -0.04077374190092087,
        -0.004053890705108643,
        -0.02211562916636467,
        0.026818161830306053,
        -0.002598419552668929,
        0.037903834134340286,
        -0.014385394752025604,
        0.05476754158735275,
        0.028665369376540184,
        -0.04952988773584366,
        -0.08276389539241791,
        -0.0012575492728501558,
        0.0036343722604215145,
        -0.01206600945442915,
        0.012543842196464539,
        0.003706191433593631,
        0.0026039346121251583,
        0.01993679068982601,
        -0.0317816287279129,
        0.06487881392240524,
        -0.028197797015309334,
        -0.01197416428476572,
        0.011627494357526302,
        0.05593511089682579,
        0.035273682326078415,
        0.007342444267123938,
        0.01468014158308506,
        -0.021296104416251183,
        0.05909125879406929,
        0.044552240520715714,
        0.02422168292105198,
        0.003101199632510543,
        0.036639254540205,
        -0.0006146616651676595,
        -0.04741685092449188,
        0.021274549886584282,
        0.048400383442640305,
        -0.019457802176475525,
        -0.025881309062242508,
        -0.034563615918159485,
        0.024547971785068512,
        0.02954995073378086,
        -0.10371465981006622,
        -0.06979818642139435,
        -0.047665007412433624,
        0.013115125708281994,
        -0.005757202859967947,
        -0.03231547027826309,
        -0.10955141484737396,
        -0.060694415122270584,
        0.03531933203339577,
        -0.07745371758937836,
        -0.005596889648586512,
        -0.033262111246585846,
        -0.009774268604815006,
        0.026123305782675743,
        -0.030794652178883553,
        0.018301112577319145,
        -0.009584501385688782,
        0.01898110844194889,
        -0.01776696741580963,
        -0.029241614043712616,
        0.017748301848769188,
        -0.09419427812099457,
        0.06704125553369522,
        -0.017092455178499222,
        0.009970990009605885,
        0.042641084641218185,
        -0.027605991810560226,
        -0.006811436265707016,
        -0.016733473166823387,
        0.13072742521762848,
        -0.00801017414778471,
        -0.006152285262942314,
        0.0507114939391613,
        0.018936065956950188,
        0.039972566068172455,
        0.03920924291014671,
        -0.010034655220806599,
        -0.024188240990042686,
        0.058481793850660324,
        0.02789464220404625,
        -0.00902202446013689,
        0.10266920924186707,
        -0.10401300340890884,
        -0.018494566902518272,
        0.053546443581581116,
        -0.00023731973487883806,
        -0.03679569810628891,
        -0.0053565530106425285,
        0.017248142510652542,
        0.008755410090088844,
        0.0816122516989708,
        -0.05620930716395378,
        0.024170879274606705,
        -0.032365087419748306,
        -0.09198997914791107,
        0.035745471715927124,
        -0.033988770097494125,
        -0.05936535820364952,
        0.013496538624167442,
        0.03861615061759949,
        -0.04653920978307724,
        0.0002552837831899524,
        -0.0020182847511023283,
        -0.027453728020191193,
        -0.05205037072300911,
        -0.03542744368314743,
        0.0008581189322285354,
        -0.022612258791923523,
        0.06084591522812843,
        0.07814453542232513,
        0.01746518723666668,
        -0.04393602907657623,
        -0.05781078711152077,
        -0.015653282403945923,
        0.01184143591672182,
        -0.032628245651721954,
        -0.12311360239982605,
        0.11855415999889374,
        0.03870910406112671,
        0.03416190296411514,
        -0.04986453428864479,
        0.022836674004793167,
        0.005017695017158985,
        0.030221320688724518,
        0.03546636551618576,
        0.04285900667309761,
        0.07281579077243805,
        -0.02046780474483967,
        0.024445991963148117,
        0.027478115633130074,
        0.02133200317621231,
        0.030884098261594772,
        0.08203563839197159,
        -0.03563467785716057,
        -0.04255271330475807,
        0.06409784406423569,
        0.07064288854598999,
        0.060351308435201645,
        0.03542022034525871,
        -0.05165458470582962,
        0.04600350558757782,
        -0.0036382998805493116,
        -0.015836449339985847,
        -0.01632678136229515,
        -0.04371362924575806,
        -0.013652068562805653,
        -0.02851473167538643,
        0.011232241988182068,
        -0.02014537714421749,
        0.0217087771743536,
        -0.026839526370167732,
        -0.031065823510289192,
        0.09401942044496536,
        0.058527469635009766,
        0.031831540167331696,
        -0.008683979511260986,
        -0.012470686808228493,
        0.01583629660308361,
        -0.01880178600549698,
        -0.08113139867782593,
        -0.04602871462702751,
        -0.13040120899677277,
        -0.021815184503793716,
        0.03027730993926525,
        -0.05709196999669075,
        0.011135930195450783,
        0.0030096324626356363,
        -0.034427668899297714,
        -0.06974214315414429,
        0.04821260645985603,
        -0.042815372347831726,
        -0.047174617648124695,
        0.033376678824424744,
        0.01719648577272892,
        -0.04563997685909271,
        0.03582007810473442,
        0.008973604999482632,
        -0.0007535842014476657,
        0.055855873972177505,
        0.0008962943684309721,
        0.0005344452220015228,
        -0.060719337314367294,
        0.021919511258602142,
        -0.024439936503767967,
        -0.028352290391921997,
        0.04926648363471031,
        -0.0531487762928009,
        0.02540142647922039,
        0.014580045826733112,
        -0.028800208121538162,
        -0.01059549767524004,
        0.0005278936005197465,
        0.01071531604975462,
        -0.010666732676327229,
        -0.005545544903725386,
        -0.036123115569353104,
        0.03736284375190735,
        0.07045147567987442,
        0.0002763792872428894,
        0.04505988210439682,
        0.0022927531972527504,
        0.015242054127156734,
        0.05287875607609749,
        0.1111493930220604,
        0.002002917928621173,
        0.010130831971764565,
        -0.013810220174491405,
        -0.04072631523013115,
        -0.021495867520570755,
        0.0028043584898114204,
        0.016935693100094795,
        -0.05790641903877258,
        0.010447219014167786,
        -0.06879375129938126,
        -0.0013927597319707274,
        0.0427066795527935,
        0.0020686553325504065,
        0.10775648802518845,
        -0.017659232020378113,
        0.06255900114774704,
        -0.029670370742678642,
        0.026943156495690346,
        0.021285999566316605,
        -0.018197370693087578,
        0.027890145778656006,
        0.05077548697590828,
        -0.0017547457246109843,
        -0.013086351566016674,
        -0.016220364719629288,
        -0.07583010196685791,
        -0.06089802086353302,
        -0.013805638998746872,
        0.06091389060020447,
        -0.0871431827545166,
        -0.056234873831272125,
        -0.031887833029031754,
        0.020598426461219788,
        0.00921875424683094,
        -0.01387245487421751,
        0.06221754476428032,
        -0.010793756693601608,
        0.00956997275352478,
        0.0586981400847435,
        0.001355767366476357,
        -0.01991548016667366,
        -0.07809744030237198,
        -0.001471354509703815,
        0.005121293477714062,
        -0.04737364128232002,
        -0.04073907434940338,
        0.025457780808210373,
        0.04708613455295563,
        0.1142554059624672,
        0.0056558456271886826,
        -0.01763017289340496,
        -0.05239460617303848,
        -0.020698336884379387,
        0.043949659913778305,
        0.02273186668753624,
        0.04501870274543762,
        -0.06462541967630386,
        -0.06593836098909378,
        0.007928233593702316,
        -0.009735899977385998,
        -0.00031859235605224967,
        -0.004800034686923027,
        0.042495016008615494,
        -0.041821520775556564,
        -0.006854612845927477,
        -0.0006553680868819356,
        0.057808831334114075,
        -0.12264596670866013,
        0.09917846322059631,
        -0.025426287204027176,
        -0.07400073111057281,
        0.0514368936419487,
        0.03872979059815407,
        -0.03723172843456268,
        0.02836827002465725,
        -0.03088202327489853,
        -0.0663580670952797,
        0.07081937789916992,
        -0.0355791300535202,
        0.054764557629823685,
        -0.020966649055480957,
        -0.041677046567201614,
        -0.01918889582157135,
        -0.02260996215045452,
        -0.06377565115690231,
        -0.024022234603762627,
        0.003957091830670834,
        -0.008844947442412376,
        -0.021693633869290352,
        0.06915894150733948,
        -0.03244746848940849,
        0.0601305291056633,
        0.030924851074814796,
        0.009272019378840923,
        0.020123573020100594,
        0.027201257646083832,
        0.011361531913280487,
        0.0659722313284874,
        -0.03557809069752693,
        -0.03898456320166588,
        0.021842874586582184,
        0.07717890292406082,
        -0.02096460945904255,
        -0.079784095287323,
        -0.0579989068210125,
        0.0243078600615263,
        0.0017285655485466123,
        0.05859838053584099,
        -0.025958513841032982,
        -0.04074675217270851,
        -0.01617761142551899,
        -0.0069450028240680695,
        -0.019983740523457527,
        0.01070145983248949,
        -0.0006627197144553065,
        -0.08576525002717972,
        -0.0207948237657547,
        -0.031868498772382736,
        -0.004675496835261583,
        0.03015652298927307,
        0.01300662662833929,
        -0.056124042719602585,
        0.044377729296684265,
        -0.03801056742668152,
        -0.014385885559022427,
        0.03583066537976265,
        0.008660723455250263,
        0.037627000361680984,
        -0.04905649274587631,
        -0.038692496716976166,
        -0.07932696491479874,
        -0.002935516182333231,
        -0.005635847803205252,
        -0.01811041124165058,
        -0.03507354110479355,
        -0.05603380501270294,
        -0.08401215076446533,
        0.040740352123975754,
        -0.013948999345302582,
        0.028321580961346626,
        -0.04034328833222389,
        -0.027371006086468697,
        0.07169551402330399,
        -0.048501040786504745,
        0.025369619950652122,
        -0.08625234663486481,
        0.003213268704712391,
        -0.061833422631025314,
        0.0028012346010655165,
        0.05865117907524109,
        -0.03654031828045845,
        -0.05865583196282387,
        -0.03704097121953964,
        0.0093468576669693,
        0.04441674426198006,
        0.016708828508853912,
        0.052366383373737335,
        0.039374250918626785,
        -0.02581167034804821,
        0.014854488894343376,
        0.01794423907995224,
        -0.03067013993859291,
        -0.03800294175744057,
        0.03644781932234764,
        0.0008400234510190785,
        0.058064211159944534,
        0.039763908833265305,
        0.027642827481031418,
        0.004081569612026215,
        0.04114428535103798,
        0.02181400917470455,
        -0.07140226662158966,
        -0.021515624597668648,
        0.052127253264188766,
        0.016901187598705292,
        0.013661821372807026,
        -0.016076302155852318,
        -0.037564799189567566,
        0.04097631946206093,
        -0.01135945226997137,
        0.041120171546936035,
        -0.028836047276854515,
        0.04283386468887329,
        0.012163054198026657,
        -0.029436886310577393,
        -0.046817414462566376,
        -0.00450921431183815,
        0.03996818885207176,
        -0.02577354572713375,
        0.00041938290814869106,
        -0.04310917109251022,
        0.051886118948459625,
        0.010359128005802631,
        0.02758658304810524,
        -0.00481385225430131,
        -0.07619074732065201,
        -0.10527552664279938,
        -0.039974529296159744,
        -0.02647770382463932,
        0.016509752720594406,
        -0.049420811235904694,
        -0.009220180101692677,
        0.04470917955040932,
        -0.008194370195269585,
        -0.09218405187129974,
        -0.0529446005821228,
        -0.007638336159288883,
        -0.04280528426170349,
        0.02278187684714794,
        -0.001109135802835226,
        -0.046978455036878586,
        -0.03206551820039749,
        0.057113464921712875,
        0.00046201396617107093,
        0.02723260410130024,
        0.009951976127922535,
        -0.0015313078183680773,
        -0.08918514102697372,
        0.013542921282351017,
        -0.024100882932543755,
        0.04150518774986267,
        -0.06290386617183685,
        0.0015790049219503999,
        -0.042804259806871414,
        0.01785060577094555,
        -0.047434668987989426,
        0.024197041988372803,
        -0.006398722995072603,
        0.03548247367143631,
        0.0025216417852789164,
        -0.0006015231483615935,
        0.021404029801487923,
        0.029499707743525505,
        -0.0304705910384655,
        -0.07303237169981003,
        0.0029150922782719135,
        0.06754685938358307,
        0.00321757048368454,
        -0.07797269523143768,
        0.07060366123914719,
        -0.09268783777952194,
        -0.03192906454205513,
        -0.039178989827632904,
        -0.039071548730134964,
        -0.025412967428565025,
        0.03477747365832329,
        0.03190922364592552,
        0.0054692355915904045,
        -0.015655698254704475,
        0.08865214139223099,
        -0.0459015816450119,
        -0.052149832248687744,
        -0.004692260175943375,
        -0.03833121061325073,
        0.048238106071949005,
        0.01578645594418049,
        -0.10450383275747299,
        0.03667280077934265,
        -0.04568464308977127,
        -0.04901285097002983,
        -0.0396442711353302,
        0.007388475351035595,
        0.02605663426220417,
        -0.025092344731092453,
        0.0034534246660768986,
        -0.0038045167457312346,
        -0.04628883674740791,
        0.03905488923192024,
        0.015548554249107838,
        0.046375200152397156,
        0.024515366181731224,
        0.012661428190767765,
        -0.052060555666685104,
        0.033149488270282745,
        0.002760554663836956,
        -0.05584932863712311,
        -0.00886298157274723,
        -0.00470378715544939,
        0.04169008880853653,
        0.05688437446951866,
        -0.023810256272554398,
        -0.004520805086940527,
        -0.014014077372848988,
        -0.0627153143286705,
        -0.05271952226758003,
        0.004692484624683857,
        -0.03068622387945652,
        -0.015065498650074005
      ],
      "title": "The Age of AI has begun"
    },
    {
      "id": "gai-esp_item001",
      "count": 5,
      "created": "2025-05-30T03:27:25.837402",
      "text": "Propuesta de Agenda Nacional de la Inteligencia Artificial para México 2024-2030 La presente licencia CC BY SA se aplica exclusivamente al contenido textual de la publicación. Para la utilización de cualquier material que no esté claramente identificado como obra de las personas autoras, deberá solicitarse autorización previa a la persona propietaria o autorizada para gestionar los derechos de autor correspondiente. Las ideas y opiniones expresadas en esta obra son las de los autores y no reflejan necesariamente el punto de vista del gobierno de México. Las opiniones expresadas y los argumentos empleados en este documento no reflejan necesariamente los puntos de vista de las personas integrantes o asociadas de la Alianza Nacional. El uso y la interpretación de la información contenida en esta publicación están sujetos a la responsabilidad individual de quienes la utilicen. Cualquier daño causado como resultado de la interpretación o aplicación de esta información recae en quienes la emplean. Agradecimientos Agradecemos a las expertas y expertos y organizaciones, instituciones, organismos y entidades públicas por sus contribuciones en las mesas de trabajo y eventos de la Alianza Nacional de Inteligencia Artificial, así como en la conceptualización del documento: Abel Alejandro Coronado, Abraham Rodriguez,Academia Mexicana de Ciberseguridad y Derechos Digitales (AMCID), Academia Mexicana de Informática (AMIAC), Adina Barrera, Adrián Alcalá, Adrián Soto, Adriana, Labardini, Adriana Servin, Agneris Sampieri, Agustín Paulin, Ahaniby Becerril, Aimée Vega Montiel, Alba Medina, Alberto Farca, Alberto Mondragón, Alehira Orozco, Alejandra Paredes, Alejandra Pineda, Alejandro Anaya, Alejandro Faya, Alejandro Pisanty, Alfonso Cruz, Alfonso Govela, Alfredo Garbuno, Alfredo Pacheco, Alfredo Reyes, Álvaro Norma American Chamber / México (AmCham), Americas Society/Council of the Americas (AS/COA), Amparo Rodríguez, Amy Glover, Ana Cecilia Pérez, Ana de Saracho, Ana Georgina de la Fuente, Ana Laura Hamud, Ana Lidia Franzoni, Ana María Salazar Slack, Ana Paola Barbosa, Ana Paula Rumualdo, Anahiby Becerril, Analí Díaz Infante, Andrea Barenque, Andrea Cabrera, Andrea Marván, Andrea Quintanilla, Andrés Galván, Angélica Contreras, Antonio José Collado, Armida Sánchez, Arochi & Linder Abogados, Article 19, Arturo Rossier, Arzu Martínez, Asociación de Internet MX (AIMX), Asociación Interamericana de Empresas de Telecomunicaciones (ASIET), Asociación Latinoamericana de Internet (ALAI), Asociación Mexicana de la Industria de Tecnologías de Información (AMITI), Asociación Nacional de Intérpretes (ANDI), Asociación Nacional de Universidades e Instituciones de Educación Superior - Tecnologías de la Información y la Comunicación (ANUIES-TIC), Avigail Castilla, Banco de Desarrollo de América Latina (CAF), Bárbara García, Beatriz Guzmán, Blanca Lilia Ibarra, Brainc, Brenda Hernández, Brenda Ruíz, C Minds, Cámara de Comercio de la Ciudad de México, Cámara de Comercio Internacional (ICC), Cámara Nacional de la Industria Electrónica, de Telecomunicaciones y Tecnologías de la Información (CANIETI), Capa 8, Carlos Gershenson, Carlos Piña, Carlos Rebellón, Carlos Santiso, Carmen Díaz Novelo, Caro Luna, Centro de Estudios Internacionales Gilberto Bosques, Centro I, Centro México Digital, César Pallares, Christian Lemaitre León, Cindy Rayo, Claudia Calvin, Claudia Del Pozo, Claudia Escoto, Claudia Ivette García, Claudia Marina Vicario, Claudia Salinas, Claudia Zavala, Comisión de Ciencia y Tecnología del Senado de la República, Comisión de Derechos Digitales del Senado de la República, Comisión Federal de Competencia Económica (COFECE), Comité de Informática de la Administración Pública Estatal y Municipal (CIAPEM), CONALEP, Conectadas, Constanza Gómez Mont, Cristian Peñaloza, Cristina Contreras, Cristina Martínez Pinto, Cristina Zamora, Cruz Javier Salazar, Cuauhtémoc González, Cyberlat, Cynthia Solis, Daniel Rivera, Daniel Soler Prada, Daniela Castell, Daniela Esquivel, Daniela Hernández, Daniela Rojas, David Mejía, David Pizaña Rito, David Taboada, Diana Pacheco, Dip. Javier López Casarín, Dolores Ruvalcaba, Donaciano Dominguez, Dorian Rommens, Edgar Aguilar, Eduardo Ortíz, Eleazar Aguirre Anaya, Elena Estavillo, Eliana Pérez, Elida González, Elvia Villarreal, Embajada Británica en México, Emiliano Díaz, Enirique Yamuni, Enrique Zapata, Eon Institute, Eric García López, Erick Rodríguez Arreola, Erik Huesca, Erika Mata Sanchez, Erika Sánchez Chablé, Ernesto Ibarra, Ernesto Miranda, Felipe Gutiérrez, Fernando Bustos, Fernando Caballero, Fernando Esponda, Fernando Galván, Fernando Valenzuela, Francia Pietrasanta, Francisco Cervantes Díaz, Francisco Chan, Francisco Corona, Francisco Plancarte, Fréderic García, Froylan López, Fuerza It, Gabriela Ramos, Georgina de la Fuente, Giovanni Tapia, Glenda Michel, Gloria Guerrero, Gobierno de Baja California, Gobierno de Guanajuato, Gobierno de Jalisco, Gobierno de Puebla, Gobierno de Querétaro, Gobierno de Sinaloa, Gobierno de Yucatán, Grecia Macias, Guillermo Fernández, Guillermo Larrea, Guillermo Malpica Soto, Guillermo Tenorio, Gustavo Guzmán, Gustavo Medina T., Héctor Méndez, Hiram Calvo, Humberto Cortés, Humberto Nicolini Sánchez, Iniciativa Latinoamericana por los Datos Abiertos (ILDA), Instituto Belisario Domínguez (IBD), Instituto Federal de Telecomunicaciones (IFT), Instituto Latinoamericano de Datos Abiertos (ILDA), Instituto Nacional de Estadística y Geografía (INEGI), Instituto Nacional de Transparencia, Acceso a la Información y Protección de Datos Personales (INAI), Instituto Nacional Electoral (INE), Irma Gómez, Isaac Alcalá, Isabel Cornejo, Isabel Davara, Itzul Girón, Iván Díaz, Iván Facundo Rubinstein, Ivonne Díaz, Ivonne Sleman, Jaime Cerdio, Jaime Domingo, Jaime Escamilla, Jaime Montiel, Jaime Rodríguez, Javier Careaga, Javier Gaxiola, Javier Huerta, Javier Juárez Mojica, Jesús Olamendi, Jesús Savage, Jimena Moreno, Jimena Viveros, Joel Pozos Osorio, Jonathan Mendoza, Jorge Alberto Lagard, Jorge Alejandro Patiño, Jorge Alejandro Peralta, Jorge Enrique Linarez, Jorge Hidalgo, Jorge Humberto Torres, Jorge Lagard, Jorge Luis Ordelin Font, José A. López Alonso, José Antonio Arochi de la Torre, José Antonio García, José de Jesus Rodríguez, José Eduardo Mendoza, José Elías Moreno, José Luis Hernández, José Luis Mateos, José Luis Peralta Higuera, José Luis Ponce López, José Marinaro, Jose Muñóz, Josefina Román, Juan Manuel Casanueva, Juan Pablo Carsi, Julia Tagüeña, Julio César Vega, Karen DeBrower, Karen Herrera-Ferrá, Kiyoshi Tsuru, Laura Y. Cabrera, Lawgic, León Palafox, Leonardo Martínez, Liliana Jiménez, Lina Rodríguez, Linda Medina, Lucia Tróchez, Luis Felipe Nava, Luis Maumejan, Luis Pineda, Luis Ricardo Sánchez, Luli Herreramoro, Luz María Castañeda, Manuel Díaz, Manuel Haces, Manuel O’Brien, Manuel Pliego, Manuel Sandoval Rios, Marcela Flores, María Fernanda Alcalá, María Rosa Araiza, Martha Tudón, Martin Johnston, Martín Portillo, Mauricio Leal, Michelle Naranjo, Miguel A. Fuentes, Miguel Ángel Moreno, Miguel Gonzalez Mendoza, Miguel Porrúa, Miriam Millán, Miriam Padilla, Miriam Rojas, Mónica Barrera, Mónica Estrada, Movimiento STEM, Nanghelly Silva, Netzer Díaz, Nicolás Arrioja, Nicole Amaral, Norma Elva, Normalización y Certificación (NYCE), Nuhad Ponce, Oliver Joaquin Jiménez, Olivia Andrea Mendoza, Open Ownership, Orlando Pérez, Oswaldo Díaz, Pablo Corona, Pablo Pruneda, Paola Bertolini, Paola Cisero, Paul Ronald Berliner, Paula Vargas, Pedro Salazar, Phillipe Boulanger, Pit Policy Lab, Priscila Ruíz Guillen, Rafael Estrada, Rafael Morales, Ramón Villavicencio, Raúl Miranda, Red en Defensa de los Derechos Digitales (R3D), Renato Cardozo, Ricardo Barriga, Ricardo Cacho, Ricardo Rodríguez, Roberto Vélez, Rodrigo Alcázar, Rodrigo Félix Montalvo, Rodrigo Riquelme, Saiph Savage, Salma Jalife, Saúl López, Sebastián Calderón, Semiramis Álvarez, Sen. Alfredo Botello, Sen. Beatríz Paredes, Sen. Claudia Ruíz-Massieu, Sen. Emilio Álvarez Icaza, Sen. Gustavo Madero, Sen. Héctor Vasconcelos, Sen. Jorge Carlos Ramírez Marín, Sen. Miguel Ángel Mancera, Sen. Nadia Navarro, Silvia Palazuelos, Sinestesia Digital, Social TIC, Sofía Pérez Gasque, Stuart Coleman, Tatiana Revilla, Teresa Verthein, TMI abogados, Ulises Cortés, Ulises Moya, Unión Internacional de Telecomunicaciones (ITU), Vanessa Vilchis, Verónica Cosset Siten Salgado, Víctor Castaño, Víctor Germán Sánchez, Víctor Lagunes, Víctor Manuel Morales, Ximena Arrieta, Yaaj México, Yailuma Leyva, Yetel Vazquez. Contenido RESUMEN EJECUTIVO 1. MARCO CONTEXTUAL 2. OBJECTIVO DEL DOCUMENTO 3. RECOMENDACIONES DE POLÍTICA PÚBLICA 3.1 POLÍTICA PÚBLICA Y DERECHOS 3.2 EDUCACIÓN Y MERCADOS LABORALES 3.3 CIBERSEGURIDAD Y GESTIÓN DE RIESGOS 3.4. GÉNERO, INCLUSIÓN Y RESPONSABILID SOCIAL 3.5. INFRAESTRUCTURA Y DATOS 3.6. INNOVACIÓN, INVESTIGACIÓN E INDUSTRIA 4. RECOMENDACIONES DE REGULACIÓN 5. RECOMENDACIONES DE GOBERNANZA 6. RECOMENDACIONES DE INDICADORES CONCLUSIONES ANEXOS REFERENCIAS Glosario En la era digital los términos y sus definiciones evolucionan rápidamente, y es común que un término tenga varias definiciones. Para este documento se tomaron en cuenta definiciones de Organismos Internacionales de los cuales México forma parte y participó en su redacción y adopción, artículos académicos, así como, documentos normativos de distintos órdenes de gobierno. Capacitación cruzada: Es un proceso de capacitación de empleados para que realicen múltiples trabajos en una organización. En la capacitación cruzada los empleados aprenden a realizar diferentes trabajos dentro de la organización, además del suyo. (Pancardo Pérez et al., 2011) Ciudadanía digital: Refiere al conjunto de derechos y responsabilidades que las personas tenemos en el entorno digital. (Argentina, n.d.) Complementariedad: El proceso de aprender nuevas habilidades o de enseñar nuevas habilidades a los trabajadores (Cambridge University, 2024) Reconversión: El proceso de aprender nuevas habilidades para poder hacer un trabajo diferente, o de capacitar a personas para hacer un trabajo diferente. (Cambridge University, 2024) Conexión significativa: Se refiere al acceso a banda ancha a velocidades que permitan el uso intensivo de datos en actividades simultáneas (clases en línea, teletrabajo, etc.), dispositivos de acceso adecuados para usos relevantes y habilidades digitales básicas. (Rojas, 2024) Energía limpia: Energía generada de fuentes renovables (Nance, 2018, 104) Energía renovable: Son un tipo de energías derivadas de fuentes naturales que llegan a reponerse más rápido de lo que pueden consumirse. (ONU, n.d.) Energía eficiente: Se refiere a la producción o el consumo de energía que es eficiente en términos de reducción del desperdicio y mejora del uso de la misma. El concepto se asocia a menudo con la eficiencia energética, que es la práctica de utilizar menos energía para realizar la misma función o tarea. (Universidad de Sao Paulo, 2015) Derechos ambientales: Derecho de todo ser humano a que se le respete, proteja, promueva y garantice un medio ambiente libre de contaminación, que fomente su sano desarrollo y bienestar, así como la preservación y restauración del equilibrio ecológico (UNAM, n.d.) Derechos humanos: Derechos inherentes a todos los seres humanos, sin distinción alguna de raza, sexo, nacionalidad, origen étnico, lengua, religión o cualquier otra condición (ONU, n.d.). Además de los derechos humanos relacionados con el sistema nervioso central y periférico, así como con la actividad mental de las personas y la información que de estos se derive Discriminación algorítmica: Se refiere al fenómeno en el cual los algoritmos de inteligencia artificial y aprendizaje automático perpetúan o incluso amplifican los prejuicios y sesgos humanos existentes en los datos con los que son entrenados. (Dialnet, n.d.) Habilidades blandas o socioemocionales: Habilidades que las personas utilizan para comunicarse, resolver problemas, liderar, empatizar y pensar de forma creativa. (UNESCO, n.d.) Habilidades digitales: Son la suma de conocimientos, capacidades, destrezas, actitudes y estrategias que se requieren para el uso de las tecnologías e Internet. (UNESCO, 2021) Identidad digital: Representación digital de la información conocida acerca de una persona, un grupo o una organización concreta. (ITU, n.d.) Infraestructura Pública Digital (DPI, por sus siglas en inglés): Es un conjunto de sistemas digitales compartidos, seguros e interoperables, basados en tecnologías abiertas, para ofrecer un acceso equitativo a los servicios públicos o privados a escala social. (WEF, 2024) Inteligencia Artificial (IA): Para fines de este documento, se retoma el texto de la OECD \"Recommendation of the Council on Artificial Intelligence\", por el que se conceptualiza a las tecnologías basadas en IA como un sistema basado en máquinas que, con objetivos explícitos o implícitos, infiere, a partir de la entrada que recibe, cómo generar salidas tales como predicciones, contenidos, recomendaciones o decisiones que pueden influir en entornos físicos o virtuales. Los distintos sistemas de IA varían en sus niveles de autonomía y capacidad de adaptación tras su despliegue. (OECD, 2023) Interdisciplinario: Se refiere al trabajo de cooperación e integración entre dos o más disciplinas y su enfoque es la obtención de síntesis que traspasen los límites de las disciplinas participantes. (UCL, n.d.) Modelos de lenguaje: Representación matemática compleja del lenguaje que se basa en cantidades muy grandes de datos y permite a los ordenadores producir un lenguaje que parece similar al que podría decir un ser humano. (Cambridge Dictionary, n.d.) Multiactor: Es una modalidad de cooperación internacional, en la cual se complementan los esfuerzos y competencias del sector público, con el sector empresarial y/o la sociedad civil y/o la academia, buscando resultados en la implementación de iniciativas que tengan por propósito mejorar las condiciones de vida de las personas y la comunidad, en el marco de las agendas de desarrollo. (agcidChile, n.d.) Multisectorial: Que afecta a varios sectores, especialmente en el ámbito de la economía. (RAE, 2023) Nearshoring: Se refiere a la externalización de trabajo a un país adyacente con un nivel equivalente de desarrollo económico; por ejemplo, cuando se transfieren puestos de trabajo de centros de atención telefónica de EE.UU. a Canadá. Como tal, puede contrastarse con la deslocalización, la subcontratación de trabajo a un país lejano con un nivel inferior de desarrollo económico. (Oxford Reference, n.d.) Neutralidad tecnológica: Fomentar de manera proactiva la libertad de elección de todos los usuarios y consumidores, sean públicos o privados, de la alternativa tecnológica que mejor convenga a sus necesidades y circunstancias. (IFT, 2019) Nube: La computación en nube es un modelo que permite el acceso ubicuo, cómodo y bajo demanda a un conjunto compartido de recursos informáticos configurables (por ejemplo, redes, servidores, almacenamiento, aplicaciones y servicios) que pueden ser rápidamente aprovisionados y liberados con un mínimo esfuerzo de gestión o interacción con el proveedor de servicios. Este modelo de nube se compone de cinco características esenciales, tres modelos de servicio y cuatro modelos de despliegue. (Mell, n.d.) Población Económicamente Activa (PEA): Todas las personas en edad de trabajar, o que contaban con una ocupación durante el período de referencia o no contaban con una pero estaban buscando emplearse con acciones específicas. (INEGI, 2002) Powershoring: Se refiere a la descentralización de la producción hacia países que ofrecen energía limpia, segura, barata y abundante y cercanos a los grandes centros de consumo, además de otras virtudes para atraer inversiones industriales. (Arbache, 2022) Sandbox regulatorio: Un enfoque regulatorio típicamente resumido por escrito y publicado que permite experimentar de manera dinámica modelos de innovaciones con plazos determinados bajo la supervisión de los reguladores. (UNSGSA, 2024) Seguridad ciudadana: Es el proceso de establecer, fortalecer y proteger el orden civil democrático, eliminando las amenazas de violencia en la población y permitiendo una coexistencia segura y pacífica. (UNDP, 2014) Seguridad nacional: La condición indispensable para garantizar la integridad y la soberanía nacional; libre de amenazas al Estado, en busca de construir una paz duradera y fructífera. (Gobierno de México, 2020) Seguridad pública: Conforme a los preceptos legales en México, es la función compartida entre la Federación, los estados y los municipios, cuyo propósito es asegurar la protección de los bienes y derechos de las personas, así como promover condiciones propicias para la convivencia pacífica y el desarrollo tanto individual como colectivo de la sociedad. Esta función implica la prevención, persecución y sanción de las infracciones y delitos, así como la reintegración social de los infractores. (Cámara de Diputados de México, 2009) (Centro de Estudios Sociales y de Opinión Pública, 2006) Soberanía tecnológica: La capacidad de un estado o una federación de estados para proporcionar las tecnologías que considere críticas para su bienestar, competitividad y capacidad de acción, y para poder desarrollarlas o adquirirlas de otras áreas económicas sin depender unilateralmente de una estructura. (Edler et al., 2020) Código abierto: Software desarrollado y mantenido mediante una colaboración abierta, y disponible (generalmente sin costo alguno) para que cualquiera lo use, examine, altere y redistribuya como quiera. (Raymond, n.d.) Código certificado: Se refiere a aquel que ha sido firmado digitalmente utilizando un certificado de firma de código. (Gitlan, 2024) Sostenibilidad: Lo que permite satisfacer las necesidades del presente sin comprometer la habilidad de las futuras generaciones de satisfacer sus necesidades propias. (ONU, n.d.) Transdisciplinario: Es un término utilizado en la literatura académica para describir un enfoque de investigación que integra conocimientos de diferentes disciplinas e implica la colaboración entre investigadores de diversos campos para abordar problemas complejos e interdisciplinarios. Este enfoque se caracteriza por su fluidez y naturaleza contextual, ya que implica cambios de perspectivas y metodologías para satisfacer las necesidades de la pregunta de investigación y las partes interesadas involucradas. La transdisciplinariedad se distingue de la interdisciplinariedad en que va más allá de la integración del conocimiento de diferentes disciplinas y apunta a crear algo completamente nuevo, a menudo desafiando ideologías disciplinarias y adoptando una perspectiva holística. (Hendricks, 2018) Educación dual: Es una modalidad de enseñanza y de aprendizaje que se realiza en dos lugares distintos; la institución educativa y la empresa, que se complementan mediante actividades coordinadas. (Araya Muñoz, 2008, 46) Educación continua: Una concepción de la educación, como un proceso permanente a lo largo de la vida, que involucra a la persona de manera integral y que se relaciona con cualquier tipo de actividad productiva en el ser humano. (Andrade Paco et al., 2009, 59) RESUMEN EJECUTIVO Este documento es un esfuerzo colaborativo de expertos del sector público y privado, academia, sociedad civil organizada, organismos internacionales y autónomos, y público en general, coordinado por la Alianza Nacional de Inteligencia Artificial (ANIA) en beneficio de toda la población mexicana. Las recomendaciones aquí emitidas reflejan un compromiso sólido y multifacético para orientar el desarrollo y uso - de forma ética y responsable - de la inteligencia artificial en México hacia un futuro sostenible e inclusivo. Las recomendaciones se integran bajo un marco de referencia con políticas públicas, regulaciones específicas y estrategias de gobernanza que subrayan un enfoque colaborativo y multiactor que involucre a todos los sectores de la sociedad. **Capítulo 1:** Describe el marco contextual e identifica un marco de políticas públicas en relación a una Estrategia de Inteligencia Artificial para el país, con un enfoque multisectorial. Destaca la importancia de la Inteligencia Artificial para el futuro de México y subraya la necesidad de acciones estratégicas para aprovechar su potencial y al mismo tiempo mitigar riesgos. **Capítulo 2:** Establece un marco de referencia para la elaboración de recomendaciones de política pública, regulación y gobernanza, en aras de garantizar y proteger los derechos humanos y ambientales con un enfoque transdisciplinario. **Capítulo 3:** Se detallan las recomendaciones que surgen de las discusiones y reflexiones sostenidas durante las sesiones de grupos de trabajo multidisciplinarios entre septiembre y diciembre de 2023. Estas discusiones se centraron en las mejores prácticas en política pública digital aplicables al contexto mexicano, así como los marcos de recomendaciones de organismos internacionales y se dividen en las siguientes temáticas: i) Políticas Públicas y Derechos; ii) Educación y Mercados Laborales; iii) Ciberseguridad y Gestión de Riesgos, iv) Género, Inclusión y Responsabilidad Social; v) Infraestructura y Datos; e vi) Innovación, Investigación e Industria. **Capítulo 4:** Se refiere a las recomendaciones en materia regulatoria para la Inteligencia Artificial en México. Las recomendaciones de regulación tomaron como punto de partida un análisis interdisciplinario de las políticas existentes, las propuestas legislativas en curso y las consideraciones éticas y jurídicas que están dando forma al futuro de esta tecnología en el país. Como la protección de la privacidad, la equidad en el acceso y uso de las tecnologías basadas en IA, la transparencia algorítmica y la responsabilidad en su implementación, la gobernanza y el valor de los datos. **Capítulo 5:** Aborda las recomendaciones relacionadas a la gobernanza efectiva y democrática de la Inteligencia Artificial en el país, siendo esta indispensable para asegurar el uso ético, responsable, transparente y beneficioso para la sociedad. Las recomendaciones de gobernanza identificadas en este documento buscan mecanismos institucionales, políticas y estrategias que el país está implementando para supervisar y regular el desarrollo, uso y aplicación de esta tecnología. Considera la creación de comités multiactor, multisectoriales, interdisciplinarios y transdisciplinarios, así como, la colaboración con el sector privado, la academia y la sociedad civil. En este documento se recogen análisis y propuestas del reporte del estado de preparación de México en inteligencia artificial, *Readiness Assessment Methodology* (RAM por sus siglas en inglés), realizado por la Organización de las Naciones Unidas para la Educación, la Ciencia y la Cultura (UNESCO), para el cual uno de los insumos relevantes fueron las mesas de trabajo organizadas bajo el marco de colaboración con la ANIA. Esta metodología apoya a sus Estados Miembro a implementar la Recomendación sobre Ética de la Inteligencia Artificial (IA) al identificar brechas institucionales y regulatorias, permitiendo adaptar el apoyo para garantizar un ecosistema ético de IA y complementando la Evaluación de Impacto Ético a nivel micro. Finalmente el documento concluye con la necesidad de contar con una visión integral del país en materia digital. Señala que para poder implementar las recomendaciones vertidas en este documento, es imperante contar con la continua participación de todos los actores del ecosistema digital del país, tener una estrecha coordinación institucional de los distintos órganos de gobierno y la adecuación de estructuras que permitan materializar estas recomendaciones. MARCO CONTEXTUAL El dinamismo en el campo de la ciencia de la Inteligencia Artificial durante los últimos años ha marcado un antes y un después en la conversación global sobre tecnología y su impacto en la sociedad. En 2022, la Fundéu RAE destacó su relevancia al elegir \"Inteligencia Artificial\" como las palabras del año, un indicativo del creciente interés por esta tecnología emergente. Este interés no ha hecho más que intensificarse en 2023, año en el que la IA fue el foco de importantes debates y eventos a nivel mundial, incluyendo el \"Proceso de IA de Hiroshima\" (ANIA, 2023), la \"Declaración de Bletchley\" (ANIA, 2023), el \"Decreto de Joe Biden sobre Inteligencia Artificial Segura y Confiable\" (La Revolución De La Inteligencia Artificial: El Decreto De Joe Biden, 2023), el \"AI Act\" de la Unión Europea (AI Act | Shaping Europe's Digital Future, 2024), la \"Declaración de Santiago\" (Ministerio de Ciencia Tecnología, Conocimiento e Innovación de Chile, 2023), el \"Global Partnership on Artificial Intelligence Summit 2023\" en Nueva Delhi (Global Partnership on Artificial Intelligence, 2023), la \"Reunión Anual del Foro Económico Mundial en Davos\" (WEF, n.d.), y el \"Summit de IA\" para el Bien Común de la Unión Internacional de Telecomunicaciones (ITU). Estos eventos destacan la importancia central de la IA en las agendas globales, tanto que el New York Times designó al año 2023 como \"el año de la Inteligencia Artificial\" (Nieto, 2023). Esto es indicativo de una tendencia que promete mantenerse y expandirse aún más en los años siguientes. La importancia de la IA y sus aplicaciones se refleja también en sus proyecciones económicas. De acuerdo con el Dr. Ryan Abbot, se espera que para 2030, la tecnología basada en la IA contribuya con hasta 15.7 billones de dólares a la economía global (Inteligencia Artificial: ¿Una Amenaza Para El Empleo En México?, 2023), una cifra que supera la producción actual combinada de China e India, resaltando el inmenso potencial de esta tecnología para transformar las economías a nivel mundial. Resulta relevante destacar que la IA no eliminará empleos *per se*; más bien, los transformará y podrá traer beneficios a aquellas personas que puedan acceder a esta y otras tecnologías emergentes. De acuerdo con un análisis del Foro Económico Mundial (WEF, por sus siglas en inglés), se estima que para el año 2025, 85 millones de empleos podrían ser reemplazados por la automatización pero se crearán 97 millones de nuevas oportunidades laborales. (WEF, 2020) Según el informe de Goldman Sachs (2023), la Inteligencia Artificial y la automatización tienen el potencial de impactar 300 millones de trabajos a nivel global. Se estima que 18% de las tareas laborales podrían ser automatizadas a través de la IA, observándose un mayor impacto en los mercados desarrollados en comparación con los mercados emergentes. En la Unión Europea, este porcentaje se incrementará 24%, mientras que en Estados Unidos llegará a 25% (BriggsKodnani, 2023). A nivel regional, la adopción de tecnologías de Inteligencia Artificial está ganando impulso en América Latina y el Caribe (ALC), donde los gobiernos, las empresas, la academia y la sociedad civil están reconociendo cada vez más su potencial para impulsar el crecimiento económico, educativo y social con la finalidad de mejorar la calidad de vida de las personas, especialmente aquellas más vulnerables (como mujeres, adultos mayores, comunidades indígenas, personas con discapacidad, niños, niñas y adolescentes). Un estudio del Programa de las Naciones Unidas para el Desarrollo (PNUD) sugiere que, para 2030, la IA podría contribuir al Producto Interno Bruto (PIB) de la región en hasta 5.4% (Opp, 2024). A través de proyectos innovadores en sectores como la salud digital, la educación adaptativa y la agricultura inteligente. La tecnología basada en la IA se está posicionando como un catalizador de cambio y progreso en la región. Se anticipa que la tecnología basada en la IA se convierta en un pilar fundamental para el desarrollo económico, la innovación, la competitividad, la sociedad y la reducción de desigualdades a nivel global. El avance de esta disciplina y su debida adopción en México representa una oportunidad significativa para impulsar la economía nacional y optimizar las cadenas de suministro, colocando al país en una posición favorable en el panorama tecnológico internacional. En México, la IA está emergiendo como una prioridad en diversas agendas (sector público, sector privado, académico, organismos autónomos y sociedad civil organizada), con un creciente reconocimiento de su importancia para la competitividad y la productividad. En el Índice Latinoamericano de Inteligencia Artificial 2023 (ILIA), México obtuvo un puntaje de 48.55, posicionando al país como el quinto de la región, por debajo de Chile, Brasil, Uruguay y Argentina. México presenta una infraestructura tecnológica en línea con el promedio latinoamericano, resaltando avances en tecnología 5G pero enfrentando desafíos en el acceso y uso de internet. En términos de datos, el país supera la media regional, brindando oportunidades para fortalecer la investigación y desarrollo. (CEPAL, 2023) En el ámbito del talento, México se destaca en alfabetización en IA; sin embargo, esto ha tenido un mayor impacto al centro del país (CDMX y EDOMEX) - lo cual representa una brecha geográfica de habilidades tecnológicas de todo tipo- licenciados en computación y habilidades disruptivas (CEPAL, 2023). Aunque la comunidad de investigación es madura, se acerca al promedio de impacto de la IA en Latinoamérica. En Investigación y Desarrollo, México sobresale en patentes pero enfrenta deficiencias en productividad, calidad de código abierto y adopción empresarial de la IA. Mientras que en gobernanza, el país está por debajo del promedio, careciendo de una estrategia actualizada a pesar de participar en estándares internacionales y contar con regulaciones de protección de datos. (CEPAL, 2023) Por otro lado, Oxford Insights publica anualmente el Índice de Preparación de la IA del Gobierno, evaluando el desempeño de 193 países. México tuvo un crecimiento inicial en 2021 pero su puntuación ha disminuido desde entonces debido a la falta de continuidad de la política pública en materia digital, pasando de 52.62 a 50.37 en 2023. De 2021 a 2023 México perdió 13 lugares, ubicándose en el puesto 68 (Oxford Insights, 2023). Aunque mostró cierto avance en la integración de la IA en servicios públicos, este progreso se ha estancado (Anexo 1.9) La adopción de las tecnologías basadas en la IA muestra una tendencia al alza. El \"Global AI Adoption Index 2022\" señala que 31% de las empresas en México han implementado activamente la IA (IBM Global AI Adoption Index 2022, 2022). Además, una encuesta realizada por la Asociación de Internet y Knowsy AI indica que 55% de los usuarios de internet en México perciben la IA como una herramienta útil, mientras que 22% expresa preocupaciones sobre sus posibles impactos negativos (El Economista, 2023). Estas cifras reflejan una dualidad presente tanto en México como a nivel global: un entusiasmo por la innovación y el desarrollo tecnológico, junto con una cautela hacia los riesgos éticos, económicos, sociales, de dependencia tecnológica y de ciberseguridad que estos avances podrían conllevar. La ubicación estratégica de México, podría convertirlo en un destino clave para empresas que buscan trasladar parte de su producción, así como se ve reflejado en la estrategia de *nearshoring*, pero necesita contar con la infraestructura digital (tecnología de almacenamiento y procesamiento de datos) capacidades y habilidades adecuadas para su desarrollo. En este contexto, maximizar los beneficios requiere establecer una visión nacional compartida y desarrollar estrategias específicas que transformen esa visión en acciones concretas mediante políticas públicas, nuevas leyes o actualización de varias y una gobernanza que involucre a múltiples actores. El presente documento provee recomendaciones con el objetivo de facilitar y estructurar una Agenda de Inteligencia Artificial que permita a México potenciar la innovación, desarrollo y uso de las tecnologías basadas en IA para el bien común, salvaguardando los derechos de las personas y del medio ambiente. Propone aportar a la administración (2024-2030) una visión general y actualizada de la IA, esperando que esta pueda ser considerada en el Plan Nacional de Desarrollo y la Estrategia Digital Nacional para hacer frente a la revolución tecnológica que está dando forma al siglo XXI. OBJETIVO DEL DOCUMENTO Establecer un marco de referencia que promueva la integración de la Inteligencia Artificial como un motor de la inclusión y el desarrollo social, económico y educativo del país, la investigación científica, el desarrollo tecnológico, la innovación y emprendimiento ético, responsable y equitativo. Nuestro enfoque se centra en la elaboración de recomendaciones de política pública, regulación y gobernanza, en aras de garantizar y proteger los derechos humanos y ambientales. Esto se logrará mediante una adecuada gestión de los riesgos asociados con los casos de uso de las tecnologías basadas en IA, aplicando principios de ética y responsabilidad social que promuevan la transparencia y la rendición de cuentas en todas las etapas del desarrollo de estas tecnologías, desde su diseño, hasta su despliegue y uso en el país. En ese sentido y para cumplir con el objetivo macro, el 23 abril del año pasado se lanzó desde el Senado de la República, la Alianza Nacional de Inteligencia Artificial (ANIA), un mecanismo multiactor que reconoce y fortalece el ecosistema de tecnologías emergentes e inteligencia artificial en México desde una perspectiva integral, plural y multidisciplinaria; a través de estas acciones específicas: 1. Generación de un diagnóstico y propuestas de regulación, política pública y modelo de gobernanza. Para lograr este objetivo se realizaron tres sesiones de seis diferentes mesas de trabajo: a. Políticas Públicas y Derechos b. Educación y Mercados Laborales c. Ciberseguridad y Gestión de Riesgos d .Género, Inclusión y Responsabilidad Social e.Infraestructura y Datos f. Innovación e Industria 2. Concientización y capacitación en IA 3. Documentación casos de uso de IA en México 4. Colaboración internacional La Alianza Nacional de Inteligencia Artificial está poniendo en marcha un nuevo modelo de gobernanza, integrando líderes tecnológicos, iniciativa privada, la academia, sociedad civil, organismos internacionales, autónomos, gobierno, legisladores y reguladores nacionales e internacionales, para identificar una visión de país bajo un marco de referencia como se indica en la siguiente figura - \"Marco de referencia de la IA para el bien común\". Este documento es un esfuerzo colaborativo de expertos del sector público y privado, academia, sociedad civil organizada, organismos internacionales, y público en general, coordinado por la Alianza Nacional de Inteligencia Artificial, en colaboración con la UNESCO, en beneficio de toda la población mexicana. RECOMENDACIONES DE POLÍTICA PÚBLICA POLÍTICA PÚBLICA Y DERECHOS Las recomendaciones que se desarrollan a continuación surgen de las reflexiones compartidas durante las diferentes sesiones de las mesas de trabajo coordinadas por la ANIA, en acompañamiento de la UNESCO. Estos diálogos y reflexiones, se centraron en las mejores prácticas en materia de política digital aplicables al contexto mexicano; así como los marcos de recomendaciones de organismos internacionales de los cuales México forma parte, tales como la \"Recomendación sobre la Ética de la Inteligencia Artificial de la Organización de las Naciones Unidas para la Educación, la Ciencia y la Cultura (UNESCO); y los Principios de la Organización para la Cooperación y el Desarrollo Económicos (OCDE). **Objetivo 3.1.1:** Incorporar acciones de política pública en Inteligencia Artificial como parte del Plan Nacional de Desarrollo (PND), la Estrategia Digital Nacional (EDN), y el Plan de Cumplimiento de las Metas de Desarrollo Sostenible 2030, definiendo una hoja de ruta por acción, instancias responsables e indicadores de ejecución. **Acciones:** • Desarrollar una metodología para el mapeo del ecosistema de la IA en México; • Integrar en la Política Nacional de Inclusión Digital/Agenda Digital Nacional de acuerdo al mandato del Art. 6 Constitucional, acciones específicas para el desarrollo, uso y aprovechamiento de las tecnologías emergentes, entre ellas la Inteligencia Artificial, promoviendo el ejercicio pleno de los derechos humanos, y protección del medio ambiente, como parte estructural de la agenda; • Definir el conjunto de acciones de la Política Nacional mediante un proceso abierto y colaborativo, involucrando a todos los participantes del ecosistema digital de México. Esto incluye a la academia, el sector privado, la sociedad civil, entidades gubernamentales de todos los niveles, organismos internacionales y el público en general, asegurando una representación integral de todos los sectores involucrados; • Establecer acciones concretas, definidas en una o más agendas específicas en materia digital, dentro del marco del Plan Nacional de Desarrollo 2024-2030; así como, en la Agenda de México para cumplir con las Metas de los Objetivos de Desarrollo Sostenible al 2030. Cada acción planteada deberá estar acompañada de una hoja de ruta clara, especificando la entidad encargada de llevarla a cabo, la inversión necesaria y las posibles fuentes de financiamiento, sean públicas (mediante partidas presupuestarias, fondos, etc.) y/o privadas; • Garantizar que las acciones - como mínimo - promuevan el desarrollo y el acceso a Infraestructura Pública Digital (DPI por sus siglas en Inglés) como son sistemas para el intercambio de información, los servicios financieros digitales; así como, el acceso a datos abiertos en formatos que sean legibles por máquinas. El desarrollo de la DPI se estructurará siguiendo estándares, especificaciones técnicas y marcos de referencia internacional como GovStack Global de la Unión Internacional de Telecomunicaciones (ITU), garantizando la privacidad, seguridad e inclusión desde su diseño; • Poner en marcha un sistema de identidad digital; • Construir una infraestructura nacional de datos con un modelo de gobernanza democrático en el que se definan los tipos de datos, un marco regulatorio que brinde certeza durante todo el ciclo de vida, los actores y responsables involucrados de su generación, acceso, tratamiento, así como, las posibles sanciones en caso de mal uso. • Este modelo de gobernanza deberá vincularse con políticas de transparencia y acceso a la información pública, privacidad y protección de datos personales, datos abiertos, entre otros que se crean necesarios; • Habilitar un tablero público para el seguimiento del progreso de las acciones de la(s) agenda(s), con datos de indicadores de cumplimiento en formato abierto y visualizaciones que faciliten el análisis de información. Dicho tablero puede incluir indicadores específicos que midan el impacto de la Inteligencia Artificial en México. EDUCACIÓN Y MERCADOS LABORALES **Objetivo 3.2.1:** Propiciar la integración de habilidades y conocimientos de tecnologías basadas en Inteligencia Artificial en el diseño curricular de educación básica, media superior y superior. **Acciones:** • Establecer un ecosistema que contemple los elementos clave que permita la integración de las tecnologías basadas en la IA dentro de los entornos educativos (infraestructura, desarrollo profesional docente, oferta de recursos educativos digitales; así como, mecanismos de monitoreo y evaluación); • Garantizar la infraestructura necesaria (como conectividad y dispositivos tecnológicos) que permita integrar herramientas de las tecnologías basadas en la IA en las instituciones educativas, especialmente en áreas rurales y marginadas; • Capacitar a docentes y demás autoridades educativas de todos los niveles en el conocimiento e integración responsable de las tecnologías basadas en la IA dentro de los entornos educativos, preparándolos en el entendimiento y transmisión de estas competencias a la población estudiantil. Este esfuerzo asegurará la formación de nuevas generaciones de profesionales altamente capacitados para adquisición, profundización y creación de las tecnologías basadas en IA, fortaleciendo así el liderazgo del país en este campo tecnológico emergente; • Desarrollar e implementar un diseño curricular, en cada uno de los niveles educativos, que considere el entendimiento, uso y desarrollo de las tecnologías basadas en IA desde su aplicación en contextos cotidianos hasta la programación en lenguajes actualizados y relevantes alineados con el progreso tecnológico y digital, y el análisis de datos; así como el fomento del pensamiento computacional, que abarca la resolución de problemas mediante la descomposición, el reconocimiento de patrones, la abstracción y el uso de algoritmos • Asegurar que los planes y programas de estudio en todos los niveles educativos incluyan el desarrollo de habilidades blandas o socioemocionales. Lo anterior con el fin de generar perfiles profesionales que no solo complementan el entendimiento, uso y desarrollo de las tecnologías basadas en IA, sino que también se alineen a la demanda de la sociedad y del mercado laboral actual y futuro; • Generar políticas públicas que fomenten la ciudadanía digital a través de campañas de concientización y capacitación; • Incorporar mecanismos de monitoreo y evaluación para medir el impacto de la IA en la educación, alineando las políticas y estrategias a los estándares internacionales de evaluación de tecnologías educativas, como los establecidos por la UNESCO, la ITU y la OCDE; • Facilitar la colaboración internacional para compartir mejores prácticas y lecciones aprendidas en la implementación de la IA en diferentes contextos educativos, a través de plataformas de intercambio de conocimientos y la participación en foros globales. **Objetivo 3.2.2:** Impulsar programas de formación, actualización y certificación profesional (micro-credenciales), de acuerdo a la demanda laboral generada por las tecnologías basadas en IA, garantizando siempre los derechos laborales. **Acciones:** • Fomentar la educación y capacitación continua a través de recursos educativos y talleres de todos los niveles; • Fortalecer los programas de formación, actualización y certificación profesional (micro-credenciales) dirigidos a la Población Económicamente Activa (PEA). Estos programas deberán desarrollar y fomentar las habilidades y competencias que permitan a las personas mantener sus empleos actuales, facilitar la transición o adquisición de nuevas capacidades de acuerdo a la demanda del mercado laboral. Las ofertas de formación deberán incluir un enfoque de uso ético y responsable, y de protección de los derechos humanos; • Implementar programa integrales de formación y capacitación en tecnologías basadas en IA dirigido a funcionarios públicos; • Fomentar una cultura organizacional que promueva el desarrollo e integración de la IA como una herramienta fundamental para la Transformación Digital de la Administración Pública, considerando aspectos clave como la gestión del cambio. Esto permitirá al personal que integra las dependencias públicas comprender los beneficios y riesgos asociados con esta tecnología, identificar los campos de aplicación que generen valor y adquirir los conocimientos técnicos necesarios para su adecuada implementación; • Implementar un modelo de educación dual que permita a los estudiantes incorporarse al mercado laboral mientras continúan sus estudios a través de métodos híbridos y flexibles, que combinan la enseñanza presencial con la modalidad en línea; • Implementar políticas de complementariedad, reconversión y capacitación cruzada en el mercado laboral que genere las habilidades necesarias para adaptarse a los cambios generados por tecnologías emergentes. **Objetivo 3.2.3:** Incrementar la inversión en investigación y desarrollo de tecnologías emergentes para impulsar el desarrollo de México. **Acciones:** • Promover el fortalecimiento del ecosistema de la Inteligencia Artificial en México y mitigar la fuga de talento mediante la implementación de políticas que fomenten la creación de oportunidades laborales para ciudadanos mexicanos en áreas de investigación en IA y roles especializados, en colaboración entre instituciones educativas, iniciativa privada y organismos internacionales; • Crear un fondo de inversión pública y privada para investigación y desarrollo científico y tecnológico de sistemas de IA y tecnologías emergentes; • Promover la creación de centros de excelencia y clústeres tecnológicos especializados en las tecnologías basadas en IA para fomentar la innovación y el desarrollo tecnológico de sectores prioritarios para el país. CIBERSEGURIDAD Y GESTIÓN DE RIESGOS **Objetivo 3.3.1:** Promover la ciberseguridad en el país mediante la implementación de principios para el desarrollo tecnológico, la gestión de riesgos y la investigación, así como, protocolos efectivos de intercambio de información sobre ciberataques entre sectores. **Nota:** Estas acciones deberán verse reflejadas en la Ley de Ciberseguridad que se describe en la sección de recomendaciones de regulación. **Acciones:** • Establecer principios de desarrollo, adquisición e implementación de software y tecnologías emergentes que integren: a) elementos de interacción, control y supervisión humana; b) protocolos de gestión de riesgos, monitoreo de incidentes, y autenticación de información; y c) políticas de privacidad, protección de datos, transparencia, rendición de cuentas y explicabilidad; • Incrementar la inversión en investigación y desarrollo de ciberseguridad y la aplicabilidad de algoritmos de aprendizaje automático para complementar modelos de análisis de eventos pasados (prescriptivos y de diagnóstico) y futuros (predictivos y descriptivos) en la gestión de riesgos cibernéticos; • Impulsar la innovación en la gestión de la ciberseguridad y riesgos cibernéticos a través del uso de espacios controlados de pruebas - \"sandboxes\" - involucrando equipos expertos multidisciplinarios y multisectoriales. Éstos equipos definirán, y evaluarán diferentes criterios de medición de impacto del uso de sistemas de IA que se aplicarán durante los procesos de contratación, desarrollo y uso en el sector público y privado; • Actualizar continuamente los protocolos para compartir información sobre ciberataques entre organizaciones del sector público, la academia, empresas/PyMES, instituciones educativas y sociedad civil organizada para: a) compartir lecciones aprendidas y estrategias de mejora para prevención y corrección de los sistemas de respuesta a incidentes cibernéticos; y b) Fortalecer los protocolos de actuación policial ante ciberataques, ataques ofensivos o técnicas de desanonimización; • Articular campañas de concientización continua sobre los beneficios y riesgos de las tecnologías basadas en IA. Éstas deben facilitar y promover la adopción de estándares, gestión de riesgos, y certificaciones de ciberseguridad, principalmente en las pequeñas y medianas empresas (PyMES); • Crear un grupo multiactor que se encargue de analizar y anticipar problemas sobre el desarrollo y uso de las tecnologías emergentes, así como, su impacto ético, legal, médico, social, cultural, económico, político, de seguridad pública y ciudadana. GÉNERO, INCLUSIÓN Y RESPONSABILIDAD SOCIAL **Objetivo 3.4.1:** Promover el ejercicio pleno de los derechos humanos en el desarrollo, adquisición, y uso de sistemas de IA en el sector público como privado a través de guías, protocolos, capacitación y asistencia técnica. **Acciones:** • Desarrollar guías de desarrollo y uso de las tecnologías basadas IA en programas sociales y educativos, acceso a crédito, oportunidades de empleo, y servicios de salud orientadas a prevenir la discriminación en el perfilamiento de la población usuaria; • Establecer protocolos de información a las personas usuarias sobre el uso de las tecnologías basadas en IA en los sistemas con los que interactúan describiendo claramente riesgos y limitaciones del sistema, el proceso de quejas y tiempos de resolución de las mismas; • Identificar y mitigar la discriminación algorítmica mediante la capacitación sobre sesgos, perspectiva de género, derechos humanos, a todas las personas involucradas en las distintas etapas del ciclo de la IA desde la generación de datos hasta el uso de sistemas; • Proveer de infraestructura de internet de alta velocidad y capacidad de procesamiento de grandes volúmenes de datos en Universidades y Centros de Investigación orientados al desarrollo de casos de uso de IA aplicados al contexto y las necesidades de las comunidades que sirven. Facilitar el acceso a esta infraestructura a las PYMES e instituciones Públicas locales para el desarrollo de casos de uso, capacitación, desarrollo de habilidades en IA e inclusión digital de toda la población; • Medir y mitigar los impactos ambientales del ciclo de vida de la IA, particularmente en lo que se refiere al consumo energético, de agua y emisiones de gases de efecto invernadero de los centros de datos públicos y privados; • Impulsar el emprendimiento de base tecnológica y atracción de capital potenciando las oportunidades del \"Powershoring\"; • Adecuar espacios públicos como bibliotecas, aulas digitales, centros de atención al público y centros de inclusión digital con programas para el desarrollo, uso y aplicación de la IA en diferentes actividades y profesiones. Los espacios deberán proveer hardware y software accesible para personas no familiarizadas con las nuevas tecnologías, o con alguna discapacidad sensorial visual, auditiva, física, intelectual o psicosocial; • Fomentar la formación humanista y ética en IA, con un enfoque en derechos humanos, inclusión, diversidad y perspectiva de género, facilitando el acceso a becas para la formación y estudio de especialización en áreas STEM a mujeres, comunidades indígenas y población de entornos rurales y semiurbanos; • Salvaguardar la diversidad multicultural y lingüística del país, emprendiendo acciones para generar datos de entrenamiento (grandes modelos de lenguaje locales y regionales) que representen la diversidad de la población, integrando aspectos culturales en el desarrollo de la IA garantizando sistemas sean más equitativos, relevantes y éticamente sólidos. INFRAESTRUCTURA Y DATOS **Objetivo 3.5.1:** Promover la inversión en el desarrollo de Infraestructura Pública Digital (DPI), incluyendo redes de telecomunicaciones, software, modelos de IA, y estructuras de datos abiertos diseñadas bajo principios éticos. **Acciones:** • Definir una política nacional de gobernanza de datos; • Impulsar la inversión en el despliegue de redes de telecomunicaciones - incluidas las comunitarias -, el incremento de la capacidad de procesamiento de los centros de datos, la gestión de un consumo energético sostenible, capacidades de soporte técnico y administrativo, así como, el fortalecimiento a redes de computación entre centros académicos de investigación y desarrollo; • Impulsar el desarrollo de software en código certificado, abierto, reutilizable y basado en estándares internacionales, así como, modelos abiertos de IA como parte del stack tecnológico de la Administración Pública. Dicha DPI deberá estar disponible para el uso compartido de entidades públicas y privadas; • Fortalecer el desarrollo de estructuras y esquemas de datos que faciliten el intercambio legal y seguro de información, implementando técnicas de anonimización de datos personales y aplicando protocolos de procesamiento ético y responsable, especialmente en datos comerciales que contengan información de identificación personal. Además, promover la generación sistemática de estadísticas, indicadores y bases de datos representativas; • Promover políticas de datos y gobierno abierto, así como, interfases de aplicación - APIs - que aseguren que los usuarios tendrán acceso a datos necesarios, actualizados y de calidad para el desarrollo y uso de las tecnologías basadas en IA. En este sentido, se deberá asegurar una gestión efectiva de los datos y calidad en su ciclo de vida; • Definir principios claros sobre cómo el gobierno y las empresas recopilan, utilizan y comparten datos, promoviendo la transparencia, explicabilidad y auditoría en la interacción con soluciones basadas en inteligencia artificial. Deberán también especificar los mecanismos de protección, y supervisión para asegurar la privacidad y seguridad de los datos; • Acelerar la digitalización de todos los organismos del Estado para una mejor aplicación y uso de la IA en el diseño de servicios digitales proactivos y altamente personalizados en la experiencia de interacción con el usuario. Así como, para la mejora en todas las labores del sector público, la toma de decisiones y el diseño de política pública basado en evidencia; • Capacitar a los funcionarios públicos de todos los niveles de gobierno en el uso ético, responsable y basado en derechos humanos sobre la aplicación de tecnologías basadas en la IA en sus labores, impulsando el desarrollo de casos de uso para mejora de servicios a la población, así como, en la gestión interna de la función pública. La capacitación deberá estar en el marco de una agenda de Innovación Pública que permita espacios de experimentación abierta como pueden ser laboratorios de diseño de servicios y/o sandboxes. Éstos, habilitados en colaboración con el ecosistema GovTech y co-financiamiento público y privado; • Alinear los incentivos para promover la inversión en infraestructura básica de telecomunicaciones e internet, computación, supercomputación y centros de datos, abriendo oportunidades para la implementación de esquemas de colaboración público-privada, basados en principios de neutralidad tecnológica y competencia. INNOVACIÓN, INVESTIGACIÓN E INDUSTRIA **Objetivo 3.6.1:** Articular la atracción de capital para la investigación y desarrollo de tecnologías emergentes incluida la IA. **Acciones:** • Aumentar progresivamente la inversión en Investigación y Desarrollo (I & D) para alcanzar el promedio de inversión como porcentaje del Producto Interno Bruto (PIB) de la OCDE de 2.3%, actualmente México destina sólo el 0.3%. Este aumento deberá priorizar la investigación y desarrollo de casos de uso de IA en todos los sectores económicos y sociales del país, utilizando la Infraestructura Pública Digital (DPI) física y de software. Comenzando con sectores que ya tienen una base tecnológica y capacidad de absorción de IA para alcanzar resultados y casos de éxito a corto plazo que justifiquen inversiones futuras en otros sectores; • Focalizar fondos de inversión en desarrollo de casos de uso de IA en industrias cuyos equipos directivos son diversos en términos de experiencia profesional y paritarios en el número de mujeres en espacios de liderazgo. Los casos de uso podrán utilizar infraestructura pública digital y espacios controlados de prueba sandboxes, así como, acceso a redes de investigación y desarrollo Nacionales e Internacionales; • Impulsar la soberanía tecnológica, el emprendimiento de base tecnológica y atracción de capital potenciando las oportunidades del \"Nearshoring\" a empresas desarrollando productos y servicios en las áreas de IA aplicada a biociencia, modelos abiertos de IA, videojuegos, tecnología verde y apoyando a la industria de alta tecnología como el desarrollo de microprocesadores de nueva generación, bajo un enfoque sostenible; • Apoyar la investigación colaborativa impulsando proyectos conjuntos entre universidades, industria y emprendimientos de base tecnológica para avanzar en el desarrollo de la IA; • Aumentar el papel de la cooperación internacional así como la coordinación de políticas y estrategias a nivel nacional para fortalecer la soberanía tecnológica en áreas estratégicas - como es el diseño de microprocesadores-; • Requerir a los desarrolladores de sistemas basados en la IA el uso de espacios controlados de pruebas denominados \"Sandboxes\" que faciliten el cumplimiento con protocolos de seguridad, protección y respeto a los derechos humanos, medición de impacto y estándares de transparencia algorítmica. Estos espacios proveerán alta capacidad de cómputo y acceso a redes de especialistas en México y el extranjero que ayuden a calibrar los casos de uso; • Impulsar la productividad de las PYMES a través de la digitalización de sus operaciones y el desarrollo de nuevos modelos de negocio con el uso de IA. Promoviendo el desarrollo de capacidades técnicas y jurídicas en ámbitos como la propiedad intelectual, privacidad y datos personales recursos de etiquetado que faciliten validar la autenticidad de la información, todo en concordancia con estándares nacionales e internacionales de derechos humanos. RECOMENDACIONES DE REGULACIÓN En el contexto del avance acelerado de las tecnologías basadas en inteligencia artificial en todos los ámbitos de la sociedad, México se encuentra en un momento clave para establecer un marco regulatorio sólido que fomente la innovación y proteja los derechos y valores fundamentales de las personas, especialmente de los grupos poblacionales menos representados y en situación de vulnerabilidad. Las recomendaciones de regulación tomaron como punto de partida un análisis de las políticas existentes, las propuestas legislativas en curso y las consideraciones éticas y jurídicas que están dando forma al futuro de esta tecnología en el país. Incluyendo desde la protección de la privacidad hasta la equidad en el acceso y uso de las tecnologías basadas en IA, pasando por la responsabilidad algorítmica y la transparencia en su implementación, e identificando la gobernanza y el valor de los datos. • Integrar en la protección de los derechos humanos, la protección de los derechos relacionados con el sistema nervioso central y periférico, así como con la actividad mental de las personas y la información que de estos se derive - neuroderechos- mediante la actualización en la Ley General de Protección de Datos Personales en Posesión de los Sujetos Obligados y la Ley Federal de Protección de Datos Personales en Posesión de los Particulares, para la protección contra las neurotecnologías; • Promover el fortalecimiento de organismos certificadores de software y hardware para el desarrollo, uso y aplicación de las tecnologías basadas en IA; conforme a los estándares acordados por los mecanismos de coordinación regulatoria que se implementen para el fin de este documento. Dichos organismos deberían operar bajo mecanismos de coordinación institucional, asegurando la colaboración entre diferentes sectores y niveles de gobierno para garantizar una visión integrada y coherente a nivel nacional; • Fomentar esquemas de regulación progresivos, flexibles y adaptables así como de autorregulación, bajo un marco de principios, obligaciones y consecuencias legales vinculantes; • Establecer normativas claras y específicas para la gestión de datos en la nube y en los centros de datos; • Optar por una regulación de la IA basada en la neutralidad tecnológica, la competencia económica, la gestión de riesgos, la privacidad, la seguridad y la transparencia de acuerdo a estándares internacionales y mejores prácticas; • Clasificar a las tecnologías basadas en IA por su grado de afectación en los derechos humanos, prohibiendo el reconocimiento y predicción de emociones, pensamientos y/o conductas, y/o la incidencia sobre estos, así como la identificación biométrica con fines de perfilamiento o vigilancia; • Facilitar medios y mecanismos de denuncia de abusos y aplicación de las sanciones correspondientes; • Compaginar las regulaciones y definiciones existentes con marcos de referencia internacional como la \"Recomendación sobre la ética de la inteligencia artificial de la UNESCO¨, los principios de IA de la OECD, la Resolución A/78/L49 de la Organización de las Naciones Unidas (ONU), entre otros, de acuerdo a la necesidad de desarrollo, uso y aprovechamiento de la IA en el contexto mexicano. Así como, determinar cuáles deben ser adaptadas, actualizadas y/o creadas; • Fortalecer la garantía del ejercicio pleno de los derechos humanos, como principios regulatorios en todo el ámbito digital, incluidas las tecnologías basadas en la IA. La definición de estándares, código de ética y procesos anti sesgos, deberán publicarse como Decretos Secretariales, Guías y Lineamientos de acuerdo al ámbito de competencia de Organismos Autónomos, Gobiernos Estatales y Municipales; a través de procesos de participación con academia, sociedad civil organizada y comunidades impactadas por el uso de IA. Es fundamental que estos principios y regulaciones se fundamenten en la Constitución Política de los Estados Unidos Mexicanos; • Expedir una Ley de Ciberseguridad que considere: la protección de infraestructuras críticas y datos sensibles, la promoción de la educación y la concientización en materia de ciberseguridad, así como la colaboración entre sectores público y privado para compartir información, mejores prácticas, y la implementación de mecanismos eficaces de respuesta ante incidentes. El proceso de preparación de la Ley deberá considerar el cumplimiento de los estándares del Convenio de Budapest u otros convenios sobre delitos cibernéticos diferenciando claramente la seguridad nacional, que se ocupa de proteger infraestructura y servicios críticos, de la seguridad ciudadana, que se enfoca en protocolos de actuación y respuesta a incidentes cibernéticos bajo mecanismos de cooperación efectivos, transparentes y con rendición de cuentas para garantizar una contribución efectiva a la ciberseguridad; • Armonizar principios regulatorios a los tratados comerciales y convenios internacionales de los que México sea parte; • Promover una gestión ética y responsable en el desarrollo, uso, aplicación e implementación de las tecnologías basadas en IA (marco de responsabilidades legales): responsables, responsabilidades y consecuencias; • Implementar \"sandboxes regulatorios\" o espacios de prueba controlados, en donde se reúnan los reguladores para guiar a las empresas en el lanzamiento seguro de sus innovaciones al mercado, con el fin de identificar riesgos potenciales que lleven o no a una posible intervención específica (De La Peña et al., 2024); • Fortalecer las capacidades de la Comisión Federal de Competencia Económica (COFECE) como órgano regulador que fomente la competencia y el acceso equitativo a los datos para que nuevas tecnologías como las basadas en Inteligencia Artificial estén al alcance de todas y todos, sin importar su tamaño (emprendedores o gigantes tecnológicos), y para que estas sean un motor de desarrollo, innovación e inclusión económica en México; • Fortalecer las capacidades del Instituto Federal de Telecomunicaciones (IFT) como órgano de competencia en materia de telecomunicaciones para mejorar las condiciones de despliegue de infraestructura, y asequibilidad de los servicios de telecomunicaciones en México. Esto, a través de procesos de conectividad significativa, autonomía y apropiación digital, de acuerdo con análisis de contexto basados en las necesidades, intereses y riesgos asociados de las localidades en materia de conectividad; • Fortalecer las capacidades y marco normativos del Instituto Nacional de Transparencia, Acceso a la Información y Protección de Datos Personales (INAI), como órgano de competencia en materia de protección de datos personales para incluir la supervisión y el cumplimiento de las normativas relacionadas con el uso de las tecnologías basadas en IA y la protección de datos personales. Esto implicaría la implementación de mecanismos de monitoreo y evaluación para garantizar que las instituciones públicas y privadas cumplan con las regulaciones establecidas y tomen medidas correctivas cuando sea necesario; • Actualizar la Ley Federal de Protección de Datos Personales en Posesión de los Particulares, la Ley General de Protección de Datos Personales en Posesión de Sujetos Obligados; • Crear una Ley de Datos Abiertos Públicos; • Fortalecer las capacidades del Instituto Mexicano de la Propiedad Industrial (IMPI) y el Instituto Nacional del Derecho de Autor (INDAUTOR) en materia de protección de derechos de autor, patentes y marcas registradas en el contexto del desarrollo, uso, aplicación y comercialización de tecnologías basadas en IA; • Fusionar y fortalecer las capacidades del Instituto Mexicano de la Propiedad Industrial (IMPI) y el Instituto Nacional del Derecho de Autor (INDAUTOR) en materia de protección de derechos de autor, patentes y marcas registradas en el contexto del desarrollo, uso, aplicación y comercialización de tecnologías basadas en IA; • Definir el marco de protección de la propiedad intelectual y de los derechos de autor ante el uso de datos y obras en los sistemas de IA. Así como la protección de obras, desarrollos e invenciones producidos con tecnologías basadas en IA; • Actualizar la Ley Federal de Derecho del Autor y la Ley Federal de Protección a la Propiedad Industrial. Esto permitirá adaptar las regulaciones a las nuevas realidades tecnológicas y garantizar la adecuada protección de los derechos de propiedad intelectual en un entorno digital; • Desarrollar guías prácticas, modelos de contratos y herramientas de evaluación de riesgos para ayudar a mitigar posibles problemas éticos y legales, así como desarrollar auditorías, investigaciones de posibles infracciones y sanciones para aquellos que no cumplan con las regulaciones establecidas; • Fortalecer las capacidades del Instituto Nacional Electoral (INE), a través de la creación de un marco reglamentario que defina los límites y condiciones bajo los cuales las herramientas basadas en la IA pueden ser utilizadas en campañas políticas, propaganda electoral y difusión de información relacionada con elecciones; • Desarrollar herramientas y estrategias que permitan identificar y contrarrestar la desinformación generada por IA. Así como, desarrollar sistemas de detección de noticias falsas o engañosas, desinformación, distorsión de la realidad y otros contenidos sintéticos que puedan ser utilizados para manipular la opinión pública; • Fortalecer las capacidades del Poder Judicial para el uso y aplicación ético y responsable de las tecnologías basadas en la IA a través de la capacitación, el desarrollo de directrices y estándares éticos, auditoría y evaluación de algoritmos, protección de la privacidad y los derechos humanos, y promoción de la participación ciudadana y la rendición de cuentas en los procesos de justicia; • El Poder Judicial deberá colaborar con expertos en IA, ética y derechos humanos para desarrollar directrices y estándares éticos y legales para el uso y aplicación de las tecnologías basadas en la IA en el sistema judicial. Estas directrices deberán abordar cuestiones como la transparencia, la equidad, la imparcialidad y la protección de datos, garantizando que las decisiones judiciales basadas en algoritmos sean justas y respeten los derechos fundamentales; Fortalecer las competencias de la Comisión Nacional de los Derechos Humanos (CNDH), el Centro Nacional de Prevención de Desastres (CENAPRED), el Instituto Nacional de las Mujeres (INMUJERES) y el Instituto Nacional de Propiedad Industrial (INPI) en áreas relacionadas con la discriminación, la inclusión, la representación, el consentimiento en material audiovisual, la salvaguarda de la imagen y la reputación, así como los derechos de las comunidades indígenas, grupos vulnerables y otras cuestiones pertinentesFomentar en el poder legislativo, la constante evolución, efectividad y coherencia de la legislación en materia de tecnologías emergentes; • Integrar los principios de protección a los derechos humanos en el desarrollo, uso y aplicación de las tecnologías basadas en la IA en todos los convenios y tratados internacionales, como el T-MEC, incorporando cláusulas específicas que aborden estos temas y referenciando instrumentos internacionales pertinentes; • Establecer mecanismos de supervisión y cumplimiento, fomentar la cooperación y el intercambio de mejores prácticas, y promover la inclusión de cláusulas de salvaguarda y resolución de controversias. RECOMENDACIONES DE GOBERNANZA La gobernanza efectiva y democrática de la IA se vuelve indispensable para asegurar su uso ético, responsable, transparente y beneficioso para la sociedad. Las Recomendaciones de Gobernanza buscan mecanismos institucionales, políticas y estrategias que el país está implementando para supervisar y regular el desarrollo, uso y aplicación de esta tecnología. Desde la creación de comités multiactor, multisectoriales, interdisciplinarios y transdisciplinarios, hasta la colaboración con el sector privado y la sociedad civil. Una gobernanza robusta, abierta y colaborativa es clave para fomentar la innovación responsable y proteger los derechos y valores fundamentales de las personas. • Integrar las atribuciones de la Coordinación de Estrategia Digital Nacional de la Presidencia de la República, y las del Centro de Investigación e Innovación en Tecnologías de Información (INFOTEC) para crear la Agencia Digital Nacional (ADN) como organismo público descentralizado que reportaría al Ejecutivo Federal. La fusión considera la integración de plantillas de personal, activos y presupuesto para formar la ADN. Así como, la reconversión de roles y perfiles profesionales, sin que esto considere un incremento en el monto de presupuesto que actualmente ejercen cada una de las entidades descritas; • Crear una Oficina de Inteligencia Artificial dentro de la ADN. Las tareas de la Oficina, incluyen el desarrollo de herramientas para evaluar las capacidades de los modelos de IA de propósito general, monitorear la implementación de reglas, identificar riesgos emergentes, investigar posibles infracciones y apoyar la aplicación de regulaciones sobre usos y prácticas prohibidas de IA, y el registro de sistemas de riesgo; • Promover a través de la ADN que todas las entidades públicas asignen un talento digital como punto focal responsable de supervisar todos los sistemas basados en la inteligencia artificial que utilicen y que garanticen que el uso de estas herramientas en el servicio público siga siendo seguro para el ciudadano y la nación; Integrar un Comité de Ética que fomente la innovación, compra, desarrollo, uso y aplicación responsable de las tecnologías basadas en la IA en el sector público y privado. El Comité debe tener un enfoque multiactor, multisectorial, interdisciplinario y transdisciplinario; • En temas relacionados a la seguridad nacional, se deberá seguir la normatividad aplicable; • Alinear las directrices en materia de gestión de presupuesto TIC y de procesos de adquisiciones de bienes y servicios TIC de la Administración Pública Federal, los Organismos Autónomos, los Poderes Legislativos, Gobiernos Estatales y Municipales, así como los de los Programas de Apoyo a la Modernización Local como el INAFED, para generar mejores condiciones de precio, y promover el desarrollo, uso y actualización constante de infraestructura pública digital abierta y de uso compartido con entidades públicas y privadas; • Actualizar y fortalecer las atribuciones de la Comisión Intersecretarial de Tecnologías de la Información y Comunicación, y de la Seguridad de la Información (CITICSI) y sus grupos de trabajo, incorporando uno especializado en Inteligencia Artificial y tecnologías emergentes; para reflejar las prioridades de desarrollo digital en el país; • Promover la implementación de mecanismos de coordinación con Gobiernos Estatales como la Conferencia Nacional de Gobernadores (CONAGO); Estatales y Municipales como el Comité de Informática de la Administración Pública Estatal y Municipal A.C. (CIAPEM); así como con Órganos Constitucionales Autónomos como el INAI, la Comisión Nacional de Competencia Económica (COFECE), el IFT, la Comisión Nacional Derechos Humanos, la Comisión Federal para la Protección contra Riesgos Sanitarios (COFEPRIS), el INE, el IMPI, la Comisión Nacional de Bioética (CONBIOÉTICA), y los poderes Legislativo y Judicial, por mencionar algunos; • Habilitar al Consejo Consultivo de la CITICSI como un espacio de gobernanza multiactor - sector privado, academia, sociedad civil, población, entre otros- responsable de empujar actualizaciones constantes a la Agenda Digital Nacional y Agendas Específicas salvaguardando los principios de inclusión, no discriminación, privacidad, acceso equitativo, y fomento a la innovación digital en todos los grupos, poblaciones y sectores productivos del País; • Incorporar a representantes de la industria TIC y sectores industriales de alto consumo de servicios y productos basados en la IA, la academia y la sociedad civil organizada al proceso de definición de estándares, protocolos, y salvaguardas para el desarrollo, uso y aplicación ética, legal y responsable de modelos de IA y tecnologías emergentes, dentro del marco de derechos humanos; • Institucionalizar mecanismos de colaboración entre reguladores, stakeholders y entidades públicas para implementar espacios de prueba, conocidos como \"sandboxes\"; • Habilitar el Centro de Datos tier 4 de INFOTEC como un ¨Sandbox¨ especializado en el desarrollo de casos de uso estratégicos para el desarrollo del país y cuyo uso de datos requiere que éstos sean tratados bajo los estándares más altos de seguridad, anonimización y privacidad. Dichos casos de uso contarán con consejos consultivos y acceso a redes de expertos nacionales e internacionales; • Garantizar la aplicación del principio constitucional de paridad de género en la dirección de entidades gubernamentales y su extensión a diversos niveles jerárquicos, funciones y áreas especializadas de tecnologías basadas en la IA, así como su incorporación en organizaciones privadas y sociales mediante cambios legales que contemplen acciones afirmativas, cuotas y medidas de transparencia. Esto debe incluir la promoción de la diversidad de perfiles en los órganos directivos y en todas las áreas de especialización, abarcando género, edad y experiencia en aspectos técnicos, tecnológicos, humanísticos y éticos, entre otros; • Reconocer y contribuir a la agenda definida por la Alianza Nacional de Inteligencia Artificial (ANIA) como un espacio consultivo y de acción pública del Poder Legislativo, dirigido desde el Senado y articulación bicameral, multiactor, abierto y participativo, orientado a mantener el marco legislativo de México a la vanguardia para garantizar los derechos de las personas y el mejor aprovechamiento de las tecnologías digitales; • Promover la participación de México en el mayor número de procesos de Gobernanza Digital y estandarización técnica, como definición de estándares, y marcos de referencia internacional y demás agendas de Organismos Internacionales donde la ADN articulará la postura país en colaboración con la Secretaría de Relaciones Exteriores y Organismos Especializados en México según sea el caso. RECOMENDACIONES DE INDICADORES Definir indicadores sobre el impacto de la Inteligencia Artificial en la sociedad es desafiante debido a su complejidad y múltiples dimensiones. Es esencial identificar y medir con precisión los efectos positivos y negativos de la IA en áreas como economía, empleo, privacidad y equidad. Las recomendaciones de indicadores buscan fortalecer las capacidades institucionales para generar datos confiables y medir el impacto de la IA de manera precisa. Se basan en la información del Anexo 1.9. • Establecer un subgrupo de trabajo para la construcción de Indicadores del uso y desarrollo de las tecnologías basadas en la IA, con la participación de la CITICSI y el INEGI; • Construir una matriz nacional de indicadores de IA que deberá ser consistente con los marcos de indicadores globales con el objeto de medir el avance de México respecto del resto de los países en el mundo. Esta deberá poder definir cómo mínimo, el indicador, método de cálculo, fuente de obtención de la información - encuesta, sistema - así como, establecer estándares sólidos de recopilación y gestión de datos, garantizando la transparencia y la responsabilidad en su uso; • Habilitar una matriz de indicadores en formato abierto, que contemple una curaduría continua de datos subyacentes, que se encuentren basados en el mapeo de APIs y sistemas de información fuente acreditados para el cálculo del indicador. (Por ejemplo, el número de carreras dirigidas al estudio de la IA pueden ser datos extraídos del catálogo de carreras de entidades educativas acreditadas ante la SEP). CONCLUSIONES Las recomendaciones de este documento reflejan un compromiso sólido y multifacético para orientar el desarrollo y la implementación de la inteligencia artificial en México hacia un futuro sostenible, inclusivo, ético y responsable, estableciendo un marco integral que abarca políticas públicas, regulaciones específicas y estrategias de gobernanza, subrayando la importancia de un enfoque colaborativo, multiactor, multidisciplinario, multisectorial, interdisciplinario y transdisciplinario que involucre a todos los sectores de la sociedad. Con el propósito de contar con una visión integral del país en materia digital y poder implementar las recomendaciones vertidas en este documento, es imperante contar con la continua participación de todos los actores del ecosistema digital del país, además de contar con una estrecha coordinación institucional de los distintos órganos de gobierno y la creación de estructuras que permitan materializar estas recomendaciones. Además, la adopción y el aprovechamiento del desarrollo científico y tecnológico de las tecnologías basadas en IA presentan una oportunidad única para impulsar el crecimiento económico, enfrentar desafíos sociales y medioambientales y mejorar la calidad de vida de la población. Sin embargo, es fundamental que este avance tecnológico se gestione de manera respetuosa y comprometida con la protección de los derechos humanos y el medio ambiente, y se promueva la equidad, mitigando y evitando a su vez la exacerbación de las desigualdades existentes. Las recomendaciones detalladas en este documento no sólo buscan posicionar a México al frente en el uso ético, legal y responsable de las tecnologías basadas en la IA, sino también asegurar que la transición hacia una economía y sociedad digitalmente avanzadas beneficie a todos los sectores de la población, protegiendo especialmente a los más vulnerables. Para lograr esto, es esencial la inversión en educación, formación, infraestructura digital, investigación y desarrollo, así como la creación de un marco regulatorio que fomente la innovación mientras protege contra los riesgos potenciales asociados con la tecnología. Mirando hacia el futuro, la implementación efectiva de estas recomendaciones requerirá un esfuerzo sostenido, monitoreo constante y la voluntad de adaptarse a los cambios rápidos en el campo de la tecnología. La abierta comunicación y colaboración a nivel nacional e internacional, así como el intercambio de conocimientos y mejores prácticas, serán cruciales para navegar los desafíos emergentes y aprovechar las nuevas oportunidades que las tecnologías basadas en IA ofrecen. Con este documento, México se posiciona en la vanguardia del debate global sobre la IA, reiterando su compromiso con un desarrollo tecnológico que sea inclusivo, justo y beneficioso para toda su población. ANEXOS ANEXO 1 - Documentos principales de la ANIA ANEXO 2 - Principales fuentes de la información: diagnóstico y consulta a expertos REFERENCIAS Chile, a. (n.d.). COOPERACIÓN MULTIACTOR. Agencia Chilena de Cooperación Internacional para el Desarrollo. Retrieved April 23, 2024, from https://www.agci.cl/cooperacion-multiactores/cooperacion-multiactor AI Act | Shaping Europe's digital future. (2024, March 6). Shaping Europe's digital future. Retrieved March 12, 2024, from https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai Alianza Nacional de Inteligencia Artificial. (n.d.). Talleres | Ania MX. Alianza Nacional de Inteligencia Artificial. https://www.ania.org.mx/services-2 Alianza Nacional de Inteligencia Artificial (ANIA). (2023). Mesa 1 | Ania MX. Alianza Nacional de Inteligencia Artificial. https://www.ania.org.mx/mesa1politicaspublicasyderechos Alianza Nacional de Inteligencia Artificial (ANIA). (2023). Mesa 2 | Ania MX. Alianza Nacional de Inteligencia Artificial. https://www.ania.org.mx/mesa2educacionymercadoslaborales Alianza Nacional de Inteligencia Artificial (ANIA). (2023). Mesa 3 | Ania MX. Alianza Nacional de Inteligencia Artificial. https://www.ania.org.mx/mesa3ciberseguridadygestionderiesgos Alianza Nacional de Inteligencia Artificial (ANIA). (2023). Mesa 4 | Ania MX. Alianza Nacional de Inteligencia Artificial. https://www.ania.org.mx/mesa4generoinclusionyresponsabilidadsocial Alianza Nacional de Inteligencia Artificial (ANIA). (2023). Mesa 5 | Ania MX. Alianza Nacional de Inteligencia Artificial. https://www.ania.org.mx/mesa5infraestructuraydatos Alianza Nacional de Inteligencia Artificial (ANIA). (2023). Mesa 6 | Ania MX. Alianza Nacional de Inteligencia Artificial. https://www.ania.org.mx/mesa6innovacioneindustria Alianza Nacional de Inteligencia Artificial (ANIA). (2023, noviembre 2). ania.org.mx. https://www.ania.org.mx/post/declaración-de-bletchley-noviembre-2023 Alianza Nacional de Inteligencia Artificial (ANIA). (2023, Noviembre 02). El G7 y el \"Proceso de IA de Hiroshima\": Avanzando hacia una IA responsable. Ania.org.mx. https://www.ania.org.mx/post/el-g7-y-el-proceso-deia-de-hiroshima-avanzando-hacia-una-ia-responsable De la Peña Sissi, Ibarra Ernesto, Santoyo Cristina; Panorama de la IA en México: hacia la gobernanza de la IA y la relevancia del Sandbox de IA. 2024. https://www.amcid.org/page/sandboxregulatoriomexico Andrade Paco, J., Nava Ortega, M. J., & Valverde Núñez, J. (2009). La educación continua como proceso de formación académica en los alumnos egresados de las instituciones de educación superior en el estado de Sonora (México). *Contabilidad y Negocios*, 4(8), 57-62. https://www.redalyc.org/articulo.oa?id=281621776008 Araya Muñoz,, I. (2008). La formación dual y su fundamentación curricular Educación. *Revista Educación*, 32(1), 45-61. https://www.redalyc.org/pdf/440/44032105.pdf Arbache, J. (2022, November 14). Powershoring. CAF. https://www.caf.com/es/conocimiento/visiones/2022/11/powershoring/ Argentina, G. d. A. (n.d.). ¿ Qué es la ciudadania digital ? Argentina.gob.ar. https://www.argentina.gob.ar/desarrollosocial/grooming/que-es-la-ciudadania-digital Basañes, F. (2019, January 2). Código abierto: conceptos y aplicaciones - Abierto al público. Blogs iadb. https://blogs.iadb.org/conocimiento-abierto/es/codigo-abierto/ Bilbao Mendieta, L. (2022, 11 15). Data Centers en México. Cámara de Comercio de Bilbao. https://www.camarabilbao.com/wp-content/uploads/2023/03/DATA-CENTERSFicha-del-sector-Data-Centers-M%C3%A9xico-2022.pdf Briggs, J., & Kodnan, D. (2023, marzo 26). The Potentially Large Effects of Artificial Intelligence on Economic Growth. Goldman Sachs. Retrieved March 18, 2024, from https://www.gspublishing.com/content/research/en/reports/2023/03/27/d64e052b-0f6e-45d7-967b-d7be35fabd16.html Cámara de Diputados de México. (2009). LEY GENERAL DEL SISTEMA NACIONAL DE SEGURIDAD PÚBLICA. https://www.diputados.gob.mx/LeyesBiblio/pdf/LGSNSP.pdf Cambridge Dictonary, C. D. (n.d.). LLM | definition in the Cambridge English Dictionary. Cambridge Dictionary. Retrieved April 23, 2024, from https://dictionary.cambridge.org/us/dictionary/english/llm#google_vignette Cambridge University. (2024). RESKILLING | definition in the Cambridge English Dictionary. Cambridge Dictionary. Retrieved May 2, 2024, from https://dictionary.cambridge.org/us/dictionary/english/reskilling Cambridge University. (2024, April 24). UPSKILLING | English meaning - Cambridge Dictionary. Cambridge Dictionary. https://dictionary.cambridge.org/dictionary/english/upskilling Centro de Estudios e Investigación en Asuntos Públicos. (2023, October 3). Evaluación de Política Digital Estatal 2023. CEIAP. https://www.ceiap.mx/evaluacion-de-la-politica-digital-estatal-2023/ Centro de Estudios Sociales y de Opinión Pública. (2006). Seguridad Pública. Seguridad Pública. https://archivos.diputados.gob.mx/Centros_Estudio/Cesop/Comisiones/dtseguridad%20publica1.htm#Citar%20como Centro Economia Digitale. (2021). Sovranità Tecnologica: elementi per una strategia italiana ed europea. https://www.centroeconomiadigitale.com/wp-content/uploads/2021/03/CED-Sovranita-Tecnologica.pdf CEPAL. (2023). Ficha País México Índice Latinoamericano de Inteligencia Artificial. Índice Latinoamericano de Inteligencia Artificial. Pag 221-225. https://indicelatam.cl/wp-content/uploads/2023/08/CAP-G-MEXICO.pdf CEPAL. (2023). Índice Latinoamericano de Inteligencia Artificial | IA para el desarrollo sostenible de América Latina. Índice Latinoamericano de Inteligencia Artificial | IA para el desarrollo sostenible de América Latina. https://indicelatam.cl/ CEPALSTAT. (n.d.). Bases de Datos y Publicaciones Estadísticas. CEPALSTAT. https://statistics.cepal.org/portal/cepalstat/dashboard.html?indicator_id=1824&area_id=119&lang=es Cesareo, S., & White, J. (2023, June 28). The Global AI Index - Tortoise. Tortoise Media. https://www.tortoisemedia.com/intelligence/global-ai/ Consumotic. (2023, November 9). Discreto crecimiento de profesionistas en IA en México. Consumotic. https://consumotic.mx/tecnologia/discretocrecimiento-de-profesionistas-en-ia-en-mexico/ Council of Europe. (2023, Agosto). Study on the impact of artificial intelligence systems, their potential for promoting equality, including gender equality, and the risks they may cause in relation to non-discrimination. https://rm.coe.int/prems-112923-gbr-2530-etude-sur-limpact-de-ai-web-a5-1-2788-3289-7544/1680ac7936 Coursera. (2023). Global Skills Report 2023. Coursera. https://www.coursera.org/skills-reports/global/ Crespi,Francesco Crespi, Serenella Caravella, Menghini, Salvatori, F. C. S. C. M. M. a. C. S. (2024, March 5). European Technological Sovereignty: An Emerging Framework for Policy Strategy. , - YouTube. Retrieved April 24, 2024, from https://link.springer.com/content/pdf/10.1007/s10272-021-1013-6.pdf Cueto, H. (2023, May 29). Interés por empleos de inteligencia artificial en México crece 95%. Business Insider México. https://businessinsider.mx/interesempleos-inteligencia-artificial-mexico_tecnologia/ De La Peña, S., Ibarra, E., & Santoyo, C. (2024, Marzo). Panorama de la IA en México: hacia la gobernanza de la IA y la relevancia del Sandbox de IA. https://www.amcid.org/page/sandboxregulatoriomexico Dialnet, D. (n.d.). La discriminación algorítmica y su impacto en la dignidad de la persona y los derechos humanos: Especial referencia a los inmigrantes. Dialnet. https://dialnet.unirioja.es/servlet/articulo?codigo=9235564 Edler, J., Blind, K., Frietsch, R., Kimpeler, S., Kroll, H., Lerch, C., Reiss, T., Roth, F., Schubert, T., Schuler, J., Walz, R., & FRAUNHOFER INSTITUTE FOR SYSTEMS AND INNOVATION RESEARCH ISI. (2020, Julio). Technology sovereignty From demand to concept (J. Schuler, Ed.). Germany,. El Economista. (2023, Mayo 25). 55% de usuarios de internet en México cree que la inteligencia artificial es una herramienta útil. El Economista. https://assets.eleconomista.com.mx/tecnologia/55-de-usuariosde-internet-en-Mexico-cree-que-la-inteligenciaartificial-es-una-herramienta-util-20230525-0022.html El G7 y el \"Proceso de IA de Hiroshima\": Avanzando hacia una IA responsable (ANIA, Compiler). (2023). https://www.ania.org.mx/post/el-g7-y-el-proceso-deia-de-hiroshima-avanzando-hacia-una-ia-responsable Estudio Económico de América Latina y el Caribe 2023. El financiamiento de una transición sostenible: inversión para crecer y enfrentar el cambio climático. (2023, Noviembre 1). Cepal. https://www.cepal.org/es/publicaciones/67989-estudio-economico-america-latinacaribe-2023-financiamiento-transicion Gitlan, D. (2024, February 6). Qué es un certificado de firma de código y cómo funciona. SSL Dragon https://www.ssldragon.com/es/blog/que-es-certificado-defirma-de-codigo/ Global Economics Analyst The Potentially Large Effects of Artificial Intelligence on Economic Growth (BriggsKodnani). (2023, March 26). ANSA. https://www.key4biz.it/wp-content/uploads/2023/03/GlobalEconomics-Analyst_-The-PotentiallyLarge-Effects-of-Artificial-Intelligence-onEconomic-Growth-Briggs_Kodnani.pdf Global Partnership on Artificial Intelligence (GPAI). (2023, December 13). 2023 Ministerial Declaration. Aprobada en la 5ª reunión del Consejo Ministerial del GPAI el 13 de diciembre de 2023, en el contexto de la cumbre del GPAI en Nueva Delhi. Global Partnership on Artificial Intelligence - GPAI. https://gpai.ai/2023-GPAI-Ministerial-Declaration.pdf Gob.mx. (n.d.). Modelo Homologado de Unidades de Policía Cibernética. Gobierno de México. https://www.gob.mx/cms/uploads/attachment/file/189189/Modelo_homologado_unidades_policia_cibernetica.pdf Gob.mx. (2020, February 5). Agenda Digital Educativa. Senado de la República. https://infosen.senado.gob.mx/sgsp/gaceta/64/2/2020-02-05-1/assets/documentos/Agenda_Digital_Educacion.pdf Guardia Nacional CERT-MX. (n.d.). Guardia Nacional CERT-MX | Campaña | gob.mx. Gobierno de México. https://www.gob.mx/gncertmx Hendricks, C.C. (2018, Noviembre 3). WAC/WID and transfer: Towards a transdisciplinary view of academic writing. http://wac.colostate.edu/atd/trans_wac/hendricks2018.pdf Hernández, A. (2024, January 18). 2024, año de consolidación de la Inteligencia Artificial. Excélsior. https://www.excelsior.com.mx/hacker/2024-anoconsolidacion-de-inteligencia-artificial/1630839 IBM. (2022). IBM Global AI Adoption Index 2022. https://www.ibm.com/downloads/cas/GVAGA3JP IBM. (2024, January 10). Data Suggests Growth in Enterprise Adoption of AI is Due to Widespread Deployment by Early Adopters, But Barriers Keep 40% in the Exploration and Experimentation Phases. MultiVu. https://www.multivu.com/players/English/9240059-ibm-2023-global-ai-adoption-index-report/ IBM Global AI Adoption Index 2022. (2022, mayo 1). IBM. https://www.ibm.com/downloads/cas/GVAGA3JP IFT, I. (2019). \"Recomendación que emite el Consejo Consultivo del Instituto Federal de Telecomunicaciones en materia de promoción de la economía digital\". IFT. Retrieved April 23, 2024, from https://www.ift.org.mx/sites/default/files/iv.5_ag_36aord_181219_cc_acc.pdf Índice Latinoamericano de Inteligencia Artificial. (2023). Índice Latinoamericano de Inteligencia Artificial. https://indicelatam.cl/ INEGI. (2020). Sistema Nacional de Clasificación de Ocupaciones 2019. SINCO. INEGI. https://www.inegi.org.mx/app/biblioteca/ficha.html?upc=702825198411 INEGI, I. (2002). Guía de Conceptos, Uso e Interpretación de la Estadística sobre la Fuerza Laboral en México. Inegi. Retrieved April 23, 2024, from https://www.inegi.org.mx/contenidos/productos/prod_serv/contenidos/espanol/bvinegi/productos/metodologias/est/702825000156.pdf nteligencia Artificial: ¿Una amenaza para el empleo en México? (2023, October 6). Debate. https://www.debate.com.mx/cdmx/Inteligencia-Artificial-Una-amenazapara-el-empleo-en-Mexico-20231002-0177.html ITU, I. (n.d.). Digital Identity Roadmap Guide. ITU. Retrieved April 23, 2024, from https://www.itu.int/dms_pub/itu-d/opb/str/D-STR-DIGITAL.01-2018-PDF-E.pdf Lagos, A. (2023, October 12). Este será el primer Laboratorio de Inteligencia Artificial de América Latina, y está en México. WIRED. https://es.wired.com/articulos/este-sera-el-primer-laboratorio-deinteligencia-artificial-de-america-latina-y-esta-en-mexico La Revolución de la Inteligencia Artificial: El decreto de Joe Biden. (2023, noviembre 02). ania.org.mx. https://www.ania.org.mx/post/la-revolución-de-la-inteligencia-artificial-el-decreto-de-joe-biden Lynch, S. (2023, December 8). What to Expect in AI in 2024. Stanford HAI. Retrieved March 17, 2024, from https://hai.stanford.edu/news/what-expect-ai-2024 Mell, P. (n.d.). NIST SP 800-145, The NIST Definition of Cloud Computing. NIST Technical Series Publications. Retrieved May 7, 2024, from https://nvlpubs.nist.gov/nistpubs/legacy/sp/nistspecialpublication800-145.pdf Mesa 3 | Ania MX. (n.d.). Alianza Nacional de Inteligencia Artificial (ANIA). https://www.ania.org.mx/mesa3ciberseguridadygestionderiesgos Ministerio de Ciencia, Tecnología, Conocimiento e Innovación de Chile. (2023, octubre). Declaración de Santiago. Ministerio de Ciencia, Tecnología, Conocimiento e Innovación de Chile https://minciencia.gob.cl/uploads/filer_public/40/2a/402a35a0-1222-4dab-b090-5c81bbf34237/declaracion_de_santiago.pdf MIT Technology Review Insights. (2022). Global Cloud Ecosystem Index 2022. MIT Technology Review Insights. https://www.technologyreview.com/2022/04/25/1051115/global-cloud-ecosystem-index-2022/ Nance, P. (2018). Initial Results from the Mexico Electricity Reform, 2013–18. In Mexico's New Energy Reform (p. 104). Duncan Wood. ttps://www.wilsoncenter.org/sites/default/files/media/documents/publication/mexicos_new_energy_reform.pdf Neffke, F. (2019, diciembre 18). Los beneficios reales de la complementariedad de capacidades en el ámbito laboral es una de las líneas de investigación del Growth Lab de la Universidad de Harvard. Center for International Development at Harvard University. https://cid-harvard.github.io/networkspace/# Nieto, P. (2023, December 19). El año de la inteligencia artificial. The New York Times. Retrieved May 3, 2024, from https://www.nytimes.com/es/2023/12/19/espanol/inteligencia-artificial-aplicaciones.html Observatorio de Desarrollo Digital. (2023). Listado completo de indicadores de Desarrollo Digital. CEPAL. https://desarrollodigital.cepal.org/es/indicadores?id=373 Observatorio de Desarrollo Digital. (2023, 11 17). Las startups impulsan la innovación basada en inteligencia artificial en la región. Observatorio de Desarrollo Digital. https://desarrollodigital.cepal.org/es/datos-yhechos/las-startups-impulsan-la-innovacionbasada-en-inteligencia-artificial-en-la-region OECD. (2024). Research and development (R&D) - Gross domestic spending on R&D. OECD Data. https://data.oecd.org/rd/gross-domestic-spending-on-r-d.htm OECD, O. (2023, 11 7). OECD Legal Instruments. OECD Legal Instruments. Retrieved April 23, 2024, from https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449 OECD.AI. (n.d.). Live Data. OECD.AI Policy Observatory. https://oecd.ai/en/data?selectedArea=aijobs-and-skills&selectedVisualization=aitalent-concentration-by-country-and-gender OECD.AI. (2023). Visualisations powered by JSI using data from OpenAlex. The OECD Artificial Intelligence Policy Observatory - OECD.AI. http://www.oecd.ai OECD.AI. (2023). Visualization powered by JSI, using data from GitHub. The OECD Artificial Intelligence Policy Observatory - OECD.AI. http://www.oecd.ai OECD.AI. (2023). visualizations powered by Tableau using data form Google Trends. OECD Artificial Intelligence Policy Observatory - OECD.AI. http://www.oecd.ai OECD Artificial Intelligence Policy Observatory - OECD.AI. http://www.oecd.ai ONU. (n.d.). Sostenibilidad. Naciones Unidas| Impacto Académico. https://www.un.org/es/impacto-acad%C3%A9mico/page/sostenibilidad#:~:text=En%201987%2C%20la%20Comisi%C3%B3n%20Brundtland,de%20satisfacer%20sus%20necesidades%20propias ONU, N. U. (n.d.). Derechos humanos | Naciones Unidas. the United Nations. Retrieved April 23, 2024, from https://www.un.org/es/global-issues/human-rights ONU, N. U. (n.d.). ¿Qué son las energías renovables? | Naciones Unidas. the United Nations. https://www.un.org/es/climatechange/what-is-renewable-energy Opp, R. (2024, March 1). La revolución de la Inteligencia Artificial (IA) ya está aquí: ¿Cómo responderá América Latina y el Caribe? La revolución de la Inteligencia Artificial (IA) ya está aquí: ¿Cómo responderá América Latina y el Caribe? https://www.undp.org/es/latin-america/blog/la-revolucion-de-la-inteligencia-artificial-ia-yaesta-aqui-como-respondera-america-latina-y-el-caribe Oxford Insights. (2023). AI Readiness Index. Oxford Insights. https://oxfordinsights.com/ai-readiness/ai-readiness-index/ Oxford Insights. (2023). Government AI Readiness Index 2023. Oxford Insights. https://oxfordinsights.com/ai-readiness/ai-readiness-index/ Oxford Reference, O. R. (n.d.). Nearshoring. Oxford Reference. Retrieved April 23, 2024, from https://www.oxfordreference.com/display/10.1093/oi/authority.20110803100226581 RAE, R. (n.d.). multisectorial | Definición | Diccionario de la lengua española | RAE - ASALE. Diccionario de la lengua española. Retrieved April 23, 2024, from https://dle.rae.es/multisectorial Ramírez, É. J. (2021, March 24). INICIATIVA QUE EXPIDE LA LEY DE SALUD DIGITAL, A CARGO DEL DIPUTADO ÉCTOR JAIME RAMÍREZ BARBA, DEL GRUPO PARLAMENTARIO DEL PAN. Sistema de Información Legislativa. http://sil.gobernacion.gob.mx/Archivos/Documentos/2021/03/asun_4161366_20210324_1616609171.pdf Raymond, E. S. (n.d.). ¿Qué es el software de código abierto? IBM. https://www.ibm.com/mx-es/topics/open-source Real Academia Española. (2023). lema/sostenibilidad-ambiental sostenibilidad ambiental. Diccionario panhispánico del español jurídico. https://dpej.rae.es/lema/sostenibilidad-ambiental Red Iberoamericana de Protección de Datos. (2019). Orientaciones Específicas para el Cumplimiento de los Principios y Derechos que Rigen la Protección de los Datos Personales en los Proyectos de Inteligencia Artificial. Red Iberoamericana de Protección de Datos. https://www.redipd.org/sites/default/files/2020-02/guiaorientaciones-espec%C3%ADficas-proteccion-datos-ia.pdf Red Iberoamericana de Protección de Datos. (2019). Recomendaciones Generales para el Tratamiento de Datos en Inteligencia Artificial. Red Iberoamericana de Protección de Datos. https://www.redipd.org/sites/default/files/2020-02/guiarecomendaciones-generales-tratamiento-datos-ia.pdf Rojas, F. (2024, March 20). CONECTIVIDAD SIGNIFICATIVA. Instituto Federal de Telecomunicaciones. Retrieved April 23, 2024, from https://www.ift.org.mx/transformacion-digital/blog/conectividad-significativa SCJN. (2024, March 14). Comunicados de Prensa. Listado de Comunicados. https://www.internet2.scjn.gob.mx/red2/comunicados/noticia.asp?id=7148 Secretaría de Economía Subsecretaría de Comercio Exterior, Secretaría de Educación Pública, Secretaría del Trabajo y Previsión Social, & Consejo Nacional de Humanidades, Ciencias y Tecnologías. (2023, abril). Talento mexicano para el crecimiento y la relocalización y la recola. Gobierno de México. https://www.gob.mx/cms/uploads/attachment/file/828154/talento-mexicano_esp.pdf Secretaría de Infraestructura, Comunicaciones y Transportes. (2022). PROGRAMA DE CONECTIVIDAD EN SITIOS PÚBLICOS 2023. Gobierno de México. https://www.gob.mx/cms/uploads/attachment/file/791995/PCSP_2023.pdf SECRETARIA DE LA FUNCIÓN PÚBLICA COORDINACIÓN DE ESTRATEGIA DIGITAL NACIONAL. (2015, octubre 28). Adopción de la Carta Internacional de Datos Abiertos. Google Docs. https://drive.google.com/file/d/1kli24SbC_bk8SjykAyCXE4TK-kU4ssnj/view?usp=sharing SISTEMA NACIONAL DE TRANSPARENCIA, ACCESO A LA INFORMACION PUBLICA Y PROTECCION DE DATOS PERSONALES. (2023, March 4). ACUERDO MEDIANTE EL CUAL EL CONSEJO NACIONAL DEL SISTEMA NACIONAL DE TRANSPARENCIA, ACCESO A LA INFORMACION PUBLICA Y PROTECCION. Sistema Nacional de Transparencia. https://snt.org.mx/wp-content/uploads/CONAIP-SNT-ACUERDO-ORD02-09-10-2023-03.pdf UCL, U. d. C. (n.d.). Multidisciplinario, Interdisciplinario y Transdisciplinario. Libros.uchile.cl. Retrieved April 23, 2024, from https://libros.uchile.cl/files/presses/1/monographs/1098/submission/proof/4/index.html#zoom=z UNAM, I. d. I. J. d. l. U. (n.d.). Wikipedia. https://archivos.juridicas.unam.mx/www/bjv/libros/10/4974/16.pdf UNDP, U. (n.d.). Digital public infrastructure. United Nations Development Programme. Retrieved April 23, 2024, from https://www.undp.org/digital/digital-public-infrastructure UNDP, U. (2014, 04 15). Sinopsis: Seguridad Ciudadana. Sinopsis: Seguridad Ciudadana | Programa De Las Naciones Unidas Para El Desarrollo. Retrieved April 23, 2024, from https://www.undp.org/es/publicaciones/sinopsis-seguridad-ciudadana UNESCO, U. (2021, 1 1). Competencias y habilidades digitales. UNESCO. https://unesdoc.unesco.org/in/documentViewer.xhtml?v=2.1.196&id=p::usmarcdef_0000380113&file=/in/rest/annotationSVC/DownloadWatermarkedAttachment/attach_import_3a95803f-8657-466e-9723-130de6e7f32d%3F_%3D380113spa.pdf&locale=es&multi=true&ark=/ark:/48223/pf UNESCO, U.-U. (n.d.). Glossary article: soft skills. UNESCO-UNEVOC. Retrieved April 23, 2024, from https://unevoc.unesco.org/home/Glossary+article%3A+soft+skills Universidad de Sao Paulo, U. d. S. P. (2015). PROJETO DE PESQUISA (Mestrado). Biblioteca Digital de Teses e Dissertações da USP. Retrieved April 24, 2024, from https://www.teses.usp.br/teses/disponiveis/102/102131/tde-08032016-091721/publico/PolianaFigueiraCardoso_defesa.pdf Vergara, J. (2022, December 15). Filtraciones y hackeos, principal preocupación de las empresas mexicanas para 2023. PwC. https://www.pwc.com/mx/es/prensa/2022/digital-trust-insights-2023.html WEF, W. (2024, February 13). What to expect for digital public infrastructure in 2024. The World Economic Forum. Retrieved April 23, 2024, from https://www.weforum.org/agenda/2024/02/dpi-digital-public-infrastructure/ World Economic Forum (WEF). (n.d.). World Economic Forum Annual Meeting 2024 | Foro Económico Mundial. El Foro Económico Mundial. https://es.weforum.org/events/world-economic-forum-annual-meeting-2024/ World Economic Forum (WEF). (2020, October 21). What are the top 10 job skills for the future? The World Economic Forum. https://www.weforum.org/agenda/2020/10/top-10-work-skills-of-tomorrow-how-long-it-takes-to-learn-them/",
      "vector": [
        -0.019060174003243446,
        0.005079014226794243,
        -0.013866831548511982,
        -0.014659887179732323,
        0.09268984943628311,
        0.042989857494831085,
        9.668315760791302e-05,
        0.07282290607690811,
        -0.02371588908135891,
        0.011650579050183296,
        0.04256071895360947,
        0.0396554134786129,
        -0.018129784613847733,
        -0.010282463394105434,
        -0.0377306267619133,
        0.053418297320604324,
        -0.022717835381627083,
        0.06716306507587433,
        -0.051979657262563705,
        0.07984095811843872,
        -0.015034335665404797,
        0.011813581921160221,
        -0.03465573489665985,
        -0.0274281557649374,
        0.0851222574710846,
        -0.010153998620808125,
        0.025284938514232635,
        0.0051093013025820255,
        -0.020160632207989693,
        -0.04084927588701248,
        0.02754238247871399,
        -0.08874739706516266,
        0.012262633070349693,
        -0.0098112802952528,
        -0.008805761113762856,
        -0.03676066920161247,
        -0.013148223981261253,
        -0.029970239847898483,
        -0.0076944585889577866,
        -0.03206629678606987,
        0.07490666210651398,
        0.004349672235548496,
        -0.027116114273667336,
        -0.0023717398289591074,
        0.008933049626648426,
        -0.07149053364992142,
        -0.027391958981752396,
        0.055404745042324066,
        -0.02965371310710907,
        -0.013452804647386074,
        0.01158792246133089,
        -0.07820159196853638,
        0.005102003458887339,
        -0.03402259200811386,
        0.01218266598880291,
        0.03087020106613636,
        -0.04280420020222664,
        -0.04585569351911545,
        0.0679401084780693,
        0.06972526013851166,
        -0.08652495592832565,
        0.07653485238552094,
        -0.02141251228749752,
        -0.017453603446483612,
        0.013045214116573334,
        -0.03223273158073425,
        -0.009596175514161587,
        -0.05643283948302269,
        -0.047727588564157486,
        -0.03602566570043564,
        0.0204633679240942,
        -0.043418753892183304,
        -0.011888724751770496,
        0.06068943440914154,
        -0.008718874305486679,
        -0.0002637961879372597,
        0.05324644222855568,
        -0.0020525718573480844,
        -0.01115082111209631,
        -0.006565532647073269,
        0.054103784263134,
        -0.013320455327630043,
        0.010513322427868843,
        -0.011863505467772484,
        0.058454666286706924,
        0.06064370274543762,
        0.028222033753991127,
        0.059982024133205414,
        0.00807198230177164,
        0.019825655966997147,
        -0.07857344299554825,
        -0.030954081565141678,
        0.06623291969299316,
        -0.0026459398213773966,
        0.04854166880249977,
        0.02284885197877884,
        0.03954533487558365,
        -0.0695241168141365,
        0.023409586399793625,
        0.012982338666915894,
        -0.015308178961277008,
        0.05883604288101196,
        0.031816620379686356,
        0.020339325070381165,
        0.001569585525430739,
        -0.07561354339122772,
        -0.005769502837210894,
        -0.0025610458105802536,
        0.001657984801568091,
        -0.019579127430915833,
        -0.013616350479424,
        -0.06585055589675903,
        0.04440937936306,
        0.07086776942014694,
        0.024969102814793587,
        0.06552571058273315,
        0.02536860667169094,
        0.04901984706521034,
        -0.0010855752043426037,
        0.06336034089326859,
        -0.08535300940275192,
        0.00846098642796278,
        0.06415987759828568,
        -0.05259494483470917,
        0.008935528807342052,
        0.011583193205296993,
        -0.026186997070908546,
        0.0004126350104343146,
        -0.007483098655939102,
        -0.010868361219763756,
        -0.02672072872519493,
        -0.0077485148794949055,
        -0.0018338772933930159,
        -0.0704130083322525,
        -0.09721791744232178,
        -0.006925506517291069,
        -0.02272070199251175,
        0.06880571693181992,
        -0.08081483840942383,
        0.05370324105024338,
        0.025036290287971497,
        0.012370472773909569,
        -0.018391575664281845,
        0.013125425204634666,
        0.024760764092206955,
        -0.04410034418106079,
        0.0421588234603405,
        0.005246693268418312,
        0.06521174311637878,
        0.011148931458592415,
        0.019936975091695786,
        0.03217722475528717,
        -0.058016352355480194,
        0.03500307723879814,
        0.025215864181518555,
        -0.021120192483067513,
        0.019405025988817215,
        -0.043194640427827835,
        0.08224798738956451,
        -0.003633241867646575,
        0.014916111715137959,
        0.014449145644903183,
        0.01711205020546913,
        0.007530333939939737,
        0.010904636234045029,
        0.014497143216431141,
        0.013117162510752678,
        0.00427410751581192,
        0.030242443084716797,
        -0.023995837196707726,
        0.029542801901698112,
        -0.019126834347844124,
        0.06907392293214798,
        -0.020300883799791336,
        0.03054523840546608,
        -0.003990721423178911,
        -0.010571308434009552,
        -0.02994365245103836,
        -0.01448817364871502,
        0.0124749094247818,
        -0.03139771521091461,
        -0.028245415538549423,
        0.017568714916706085,
        -0.021313365548849106,
        0.02157105691730976,
        0.04745672270655632,
        -0.006174255162477493,
        0.03935214504599571,
        -0.002500288188457489,
        -0.06400804221630096,
        -0.04080657660961151,
        0.030609451234340668,
        0.057783324271440506,
        0.018451355397701263,
        -0.02847587689757347,
        0.060642391443252563,
        -0.009448408149182796,
        0.012160715647041798,
        0.09202989935874939,
        0.01247926615178585,
        -0.0031794582027941942,
        0.013020796701312065,
        0.07072652131319046,
        -0.03256054222583771,
        -0.018665747717022896,
        0.01054317131638527,
        0.05940558761358261,
        -0.0068682460114359856,
        0.05891626328229904,
        0.04184575751423836,
        0.01200436893850565,
        0.02894655242562294,
        0.06842604279518127,
        0.017117919400334358,
        0.040800563991069794,
        -0.029643438756465912,
        -0.025113366544246674,
        0.003240995341911912,
        -0.028158091008663177,
        -0.044625379145145416,
        -0.02004007250070572,
        -0.02563001587986946,
        -0.04400279000401497,
        0.02197977527976036,
        -0.024315163493156433,
        -0.02671012654900551,
        0.03288888931274414,
        0.07448970526456833,
        0.006800255738198757,
        -0.07987009733915329,
        0.028742752969264984,
        -0.05703366547822952,
        0.0129774184897542,
        0.00958898477256298,
        -0.00261664018034935,
        0.00312872976064682,
        -0.026897799223661423,
        -0.03631407767534256,
        -0.05043795332312584,
        0.022961553186178207,
        -0.05723845958709717,
        -0.013438059948384762,
        -0.0035708921495825052,
        -0.04404943436384201,
        -0.017735693603754044,
        0.01862660050392151,
        0.013031123206019402,
        0.030989883467555046,
        0.010245030745863914,
        -0.02562369592487812,
        -0.009755672886967659,
        -0.06552838534116745,
        -0.0571218766272068,
        0.002314276760444045,
        -0.06503720581531525,
        -0.01376388594508171,
        0.02234400436282158,
        -0.04879700019955635,
        0.008777353912591934,
        -0.04618582874536514,
        -0.03937317803502083,
        -0.04237694665789604,
        -0.01151343248784542,
        -0.0399264395236969,
        -0.049883391708135605,
        0.0475974977016449,
        0.002780286828055978,
        -0.05110655725002289,
        0.02560718171298504,
        0.02209206111729145,
        0.032234467566013336,
        0.0022030954714864492,
        0.05310283228754997,
        -0.007746492046862841,
        0.02986934781074524,
        -0.036875639110803604,
        -0.04875699430704117,
        -0.04461870342493057,
        -0.012597298249602318,
        0.023330634459853172,
        -0.015393824316561222,
        -0.05160066485404968,
        0.06500212103128433,
        -0.027317039668560028,
        0.010609700344502926,
        -0.00369876716285944,
        0.03048805147409439,
        -0.0041963797993958,
        -0.03438921645283699,
        -0.10737241804599762,
        0.05733773857355118,
        -0.03823903575539589,
        0.029625359922647476,
        -0.0046827164478600025,
        -0.04383040964603424,
        0.04642556980252266,
        0.034934088587760925,
        0.005922539625316858,
        -0.020426113158464432,
        0.024733765050768852,
        -0.02274421788752079,
        -0.053902801126241684,
        -0.034738603979349136,
        0.04166247695684433,
        -0.02276505157351494,
        -0.014598608016967773,
        0.010965434834361076,
        0.01779230684041977,
        -0.010482287965714931,
        -0.006364902015775442,
        -0.000446287973318249,
        0.02609160542488098,
        -0.018533531576395035,
        0.06888578087091446,
        -0.012676612474024296,
        0.06733081489801407,
        -0.05993376672267914,
        0.0348828099668026,
        -0.054457057267427444,
        0.009804602712392807,
        -0.1258375644683838,
        -0.027862222865223885,
        -0.010265904478728771,
        -0.06123943254351616,
        -0.10070797801017761,
        0.013505140319466591,
        -0.03804229199886322,
        0.05031199753284454,
        -0.036922160536050797,
        -0.012224201112985611,
        -0.04665540158748627,
        0.016605408862233162,
        -0.02521064318716526,
        -0.024929441511631012,
        -0.057074420154094696,
        -0.09191472083330154,
        -0.005890890024602413,
        -0.07227623462677002,
        -0.00861601997166872,
        -0.03646009787917137,
        -0.049191512167453766,
        0.0017391880974173546,
        0.004334764555096626,
        0.009670711122453213,
        -0.021992983296513557,
        0.004581936169415712,
        0.04477978125214577,
        0.035177964717149734,
        0.0307210274040699,
        -0.0016053239814937115,
        0.02050487883388996,
        0.026788387447595596,
        0.03348694369196892,
        -0.0028643691912293434,
        -0.0034909124951809645,
        -0.032791540026664734,
        -0.044193580746650696,
        0.0887652337551117,
        0.04662808030843735,
        0.009406679309904575,
        -0.02206127904355526,
        -0.05789998918771744,
        -0.043551966547966,
        0.02787221223115921,
        -0.054058704525232315,
        0.02876303903758526,
        -0.05362826958298683,
        -0.009016287513077259,
        0.04382486268877983,
        0.035262711346149445,
        -0.03816647082567215,
        -0.03911526873707771,
        0.03767697885632515,
        0.03301452100276947,
        0.007501860614866018,
        -0.023781102150678635,
        0.03227819502353668,
        0.02676479145884514,
        0.024861013516783714,
        -0.04385514557361603,
        0.03900694102048874,
        -0.016039522364735603,
        0.04743948206305504,
        -0.07326767593622208,
        0.008669670671224594,
        -0.010944413021206856,
        -0.010055935010313988,
        0.018412280827760696,
        -0.011659105308353901,
        -0.00467123556882143,
        -0.06053043529391289,
        0.008055220358073711,
        0.026462487876415253,
        0.013589119538664818,
        -0.057624317705631256,
        -0.022264517843723297,
        -0.010726024396717548,
        -0.0375409871339798,
        -0.01504223607480526,
        -0.00022324686869978905,
        -0.005655526649206877,
        -0.03243420273065567,
        -0.0017551967175677419,
        -0.03898340463638306,
        -0.0074166189879179,
        0.009779511019587517,
        -0.01515992172062397,
        0.055068787187337875,
        0.023982977494597435,
        0.02806062437593937,
        0.09916029870510101,
        -0.023919083178043365,
        -0.02108636125922203,
        0.008776363916695118,
        -0.027282414957880974,
        -0.02104341983795166,
        -0.007678441237658262,
        0.003404736053198576,
        0.1022910326719284,
        0.034541040658950806,
        0.006010932382196188,
        -0.005816128104925156,
        -0.011006358079612255,
        -0.018644966185092926,
        0.01328816544264555,
        -0.10340356081724167,
        -0.009102202951908112,
        0.015099835582077503,
        -0.009622705169022083,
        0.012009631842374802,
        0.0027590887621045113,
        -0.003902254393324256,
        0.041944388300180435,
        -0.022189676761627197,
        0.02847210317850113,
        0.053458213806152344,
        0.098442442715168,
        0.05145985633134842,
        -0.06248023360967636,
        0.02121938392519951,
        0.021638955920934677,
        -0.015626369044184685,
        -0.011983716860413551,
        -0.052171263843774796,
        -0.024051344022154808,
        0.021218864247202873,
        -0.015632543712854385,
        0.022108571603894234,
        0.04511144384741783,
        0.05110488086938858,
        -0.005882310681045055,
        0.011625176295638084,
        0.004404605831950903,
        -0.031007070094347,
        -0.07687361538410187,
        0.0013059668708592653,
        -0.015645703300833702,
        0.012425254099071026,
        -0.06961049884557724,
        -0.03127774968743324,
        -0.027126148343086243,
        -0.034925609827041626,
        -0.05192381143569946,
        0.06372769176959991,
        -0.003852828172966838,
        -0.012916629202663898,
        -0.011651956476271152,
        0.033537402749061584,
        0.0578787699341774,
        -0.002225858625024557,
        0.019474349915981293,
        0.02585691213607788,
        -0.03630787506699562,
        0.01839219406247139,
        -0.061607711017131805,
        -0.05866223946213722,
        0.03053388185799122,
        -0.022282948717474937,
        -0.00537565303966403,
        -0.041140470653772354,
        -0.04212348163127899,
        -0.009009623900055885,
        0.029537221416831017,
        -0.07529351115226746,
        0.0055791279301047325,
        -0.018896693363785744,
        0.01580643095076084,
        0.02117730677127838,
        0.05241548269987106,
        -0.004651148337870836,
        -0.0881923958659172,
        0.04236803203821182,
        0.01412215642631054,
        0.026338547468185425,
        -0.0501229465007782,
        0.032050956040620804,
        -0.05859612673521042,
        0.058156609535217285,
        -0.04428964853286743,
        -0.01881270483136177,
        0.027639426290988922,
        0.03293751925230026,
        -0.013207727111876011,
        -0.045661427080631256,
        0.022362111136317253,
        -0.021749669685959816,
        -0.014071768149733543,
        -0.03364783152937889,
        0.005897396244108677,
        -0.013318128883838654,
        -0.05063529685139656,
        0.049405500292778015
      ],
      "title": "Propuesta de Agenda Nacional de la Inteligencia Artificial para México 2024-2030"
    },
    {
      "id": "gai-esp_item002",
      "count": 6,
      "created": "2025-05-30T03:27:25.837402",
      "text": "Conversando con una computadora: ¿Cómo entienden las inteligencias artificiales lo que les pedimos? Introducción La evolución humana ha supuesto cambios no solo en lo que podemos hacer, sino en *cómo podemos hacer*. Más precisamente, la necesidad de eficientizar el trabajo ha dictado una aceleración exponencial en la tecnología; no es ninguna sorpresa ver la enorme cantidad de inventos y descubrimientos que han surgido a partir del siglo XIX, especialmente cuando los comparamos con los siglos anteriores (Gregersen, 2020). El sueño humano de la máquina que se mueve por sí sola es sumamente antiguo, con Herón de Alejandría no sólo ideando autómatas ya en la primera mitad del siglo I, sino habiendo documentado la existencia de otros ingenios similares que le precedieron (Ceccarelli, 2007): la prevalencia del término *autómata* siglos antes de las primeras computadoras mecánicas comprueba el latente anhelo de la humanidad por una máquina sirviente (Hockstein et al., 2007). La forma en que las inteligencias artificiales actuales están cambiando la forma en que los humanos interactúan con la información, con el conocimiento acumulado de la especie, pero, como se explorará a lo largo de este artículo, enseñarle a una máquina a “hablar”, a“entender”, podría ser un proceso mucho más introspectivo de lo que parece pues al menos por el momento, nuestras creaciones no pueden superarnos en algo que nosotros mismos no entendemos del todo como lo es el lenguaje: desde la psicología cognitiva recurriendo al modelo computacional para el proceso de la información, hasta los modelos lingüísticos de inteligencia artificial, el camino por lograr que una máquina hable y piense está inesperadamente ligado al esfuerzo por entender en cómo el humano mismo habla y piensa. Hoy más que nunca el término “inteligencia artificial” está llevando a la humanidad a cuestionarse cuánto realmente sabemos sobre nosotros mismos: ¿qué significa “significado”? ¿cuál es el significado de “crear”? A final de cuentas, por más impresionantes que parezcan las inteligencias artificiales, por ahora no son sino un paralelo de lo que sabemos sobre nosotros mismos: mientras no puede negarse la velocidad a la que una computadora puede realizar cálculos o comparar la memoria humana con la enorme cantidad de información a la que los modelos lingüísticos como GPT-3 tienen acceso, los procesos que realizan estas inteligencias artificiales para entender el lenguaje humano son en realidad imitaciones bastante burdas aun de la forma en la que opera el cerebro humano. Pero tampoco podemos negar el potencial que tienen las inteligencias artificiales de ayudarnos a comprender más sobre la forma en que nuestra propia mente funciona. Breve historia de la comunicación humano-máquina La evolución del cerebro humano le permitió realizar acciones cada vez más y más complejas, sin embargo, no puede negarse que fue la evolución del lenguaje lo que verdaderamente permitió a nuestros ancestros pasar del uso de herramientas para realizar trabajos y actividades que les permitían mantenerse con vida a la verdadera labor humana de comunicar, registrar y planear (Bickerton, 2009; Boeckx & Benítez-Burraco, 2014). A partir de ese momento, la sapiencia humana se dedicaría a buscar formas de hacer más con menos y en menos tiempo. Sin embargo, para 1770, año en que el escocés James Watt inventaba la máquina de vapor (Pennock, 2007), ese *menos* implicaba también *menos* humanos. Pero tan eficientes como podían ser las máquinas, aun no podía removerse del todo el factor humano: era necesario realizar ajustes al proceso, aun se necesitaba reparar o modificar las máquinas para lo cual era necesario a alguien que entendiera cómo funcionaba la máquina y pudiera adaptarla a las necesidades de la situación. Dicho de otra forma: la comunicación con las máquinas se había vuelto el nuevo problema a superar. En 1725, Basile Bouchon inventaría lo que se considera la primera máquina semiautomatizada al utilizar tarjetas perforadas para controlar una máquina tejedora. El principio de las tarjetas perforadas seguiría en uso por más de 100 años gracias a Herman Hollerith, fundador de IBM, quien las ligara a aplicaciones de computación y almacenamiento de información (Kaur et al., 2014). Justamente, la necesidad de procesar y almacenar información (más precisamente de *computarla*) dirigiría los esfuerzos por lograr y mejorar la comunicación humano-máquina. El siguiente gran paso en los esfuerzos por homologar el lenguaje de las máquinas con el lenguaje humano llegó durante la década de los cuarenta (del siglo XX), cuando la Segunda Guerra Mundial exigió una evolución en la forma en la que podía *ordenársele* a las nuevas máquinas de guerra: atrás habían quedado los tiempos en los que las piezas de artillería podían ser apuntadas por un pequeño equipo de hombres, pues las nuevas armas navales podían disparar mucho más allá del horizonte observable y golpear con precisión a un buque en movimiento. Lo único que se necesitaba era una forma de decirle a las armas *qué* hacer y *cuándo* hacerlo. Para esto se construyeron computadoras electromecánicas, máquinas con cientos de engranes y poleas que le permitían a los usuarios humanos comunicarle al sistema de armas la información del mundo real que la máquina entonces computaría y traduciría en las angulaciones necesarias de las baterías (Bureau of Ordnance, 1949). [Ref: Figura 1 -> figure_1] No pasaría mucho tiempo, sin embargo, para que los avances tecnológicos produjeran las primeras computadoras modernas y, a partir de ese momento, las computadoras han impulsado a la humanidad de formas cada vez más complejas y en campos cada vez más variados (Janssen et al., 2019): desde las tareas de comunicación, pasando por la automatización de las líneas de producción, hasta diagnóstico médico con ayuda de inteligencia artificial (Bi et al., 2019). No hay duda de lo que las computadoras han ayudado a la humanidad a lograr pero, aun hay un aspecto en la corta historia de la computación moderna que ha visto poco avance: la comunicación humano-máquina. Los lenguajes de programación Los lenguajes de programación surgieron para establecer la comunicación humano-máquina con las entonces nuevas computadoras digitales, pues éstas ya no tenían perillas o palancas mediante las cuales introducirles información y, si bien el lenguaje binario comparte genealogía con las tarjetas perforadas^1^, las computadoras digitales interpretaban y procesaban información mediante el lenguaje binario, es decir, en unos y ceros. El claro ejemplo de esta problemática fue la computadora BINAC, creada en 1949 por la compañía Echert-Mauchly, cuyos programadores eran aun matemáticos y físicos puros a los cuales se recurría por su dominio sobre expresiones lógicas. Al enfrentarse al problema del almacenamiento de la información y la necesidad de comunicarse con un dispositivo completamente digital, estos primeros programadores decidieron utilizar octal^2^ para “abreviar” los comandos en binario que la computadora podía entender (Murray Hopper, 1981). [Footnote 1]: La presencia o ausencia de una perforación en la tarjeta es similar a la presencia de un 1 o un 0 en binario (Kaur et al., 2014). [Footnote 2]: Sistema de numeración base 8, utiliza los dígitos del 0 al 7. La necesidad de establecer estándares vanguardistas, acelerada por la nueva organización del mundo tras la Segunda Guerra Mundial, daría lugar a los primeros programas computacionales; fue entonces que, mientras trabajaban en la BINAC, Grace Murray Hopper y su equipo comenzarían a establecer una serie de códigos que sobrepasaban el sistema algebraico que la BINAC utilizaba: ya no se introducían operaciones algebraicas, sino una serie de comandos que la computadora podía emparejar con las operaciones lógicas que ésta “conocía” (Murray Hopper, 1981). En la década de 1950 surgen las primeras menciones de “programación automática”, término que en ese entonces se utilizaba para referirse al uso de mnemónicos, una forma en la que computadora y humano podían comunicarse mediante un lenguaje intermedio: no se usaban literalmente ceros y unos, ni tampoco era necesario compilar las operaciones básicas cada vez que se creaba un programa, sino que podían recurrirse a palabras como “SUM” que la computadora entonces interpretaba como la operación matemática de adición. Los mnemónicos eran un lenguaje intermedio, pues no se estaba utilizando un lenguaje propiamente humano, aún tenía que prepararse la memoria de la computadora con aquellos procesos y operaciones lógicas que cada mnemónico habría de invocar además de que se tenía que entrenar a los operadores para sobre el inventario de mnemónicos que la computadora podía entender (Sammet & Holberton, 1981). Estos lenguajes de programación primitivos funcionaban bien para tareas como la computación de datos y cálculos matemáticos, pero conforme la tecnología avanzaba y las computadoras se volvían más y más potentes, surgía la necesidad de estandarizar los lenguajes de programación: se perdía mucho tiempo y dinero cada que una nueva computadora se ponía en operación, pues la mejora en la tecnología implicaba necesariamente el cambio en su arquitectura y el establecimiento de un nuevo lenguaje ensamblador^3^. Entonces, aquellos programadores tempranos se propusieron crear una especie de *lingua franca* para la comunicación humano-máquina, un lenguaje que pudiera ser utilizado en la mayor cantidad de computadoras posibles sin importar su fabricante y que además fuera fácil de enseñar y aprender para los humanos que usarían dichas máquinas (Sammet & Holberton, 1981); comenzaba la historia de los lenguajes de programación modernos. [Footnote 3]: Los lenguajes ensambladores son lenguajes de programación ligados directamente a la arquitectura de las computadoras, por lo que hay un lenguaje único para cada computadora dependiendo su fabricante. Desde la programación orientada a objetos hasta las aplicaciones del internet de las cosas^4^, los lenguajes de programación han ido cambiando junto con las computadoras, por mencionar algunos ejemplos^5^: FORTRAN en 1954, ALGOL en 1958, COBOL en 1959, BASIC en 1964, Pascal en 1970, C en 1972, Java en 1995, Python en 1991 y C++ en 1998. Pero el avance de los lenguajes de programación se ha encontrado con una disyuntiva: un lenguaje de programación con el que es fácil escribir un programa, suele ser difícil de leer. Por ejemplo, crear un programa que pueda realizar cálculos con números complejos puede ser muy complicado si se utiliza un lenguaje que no esté específicamente destinado para realizar cálculos de ese tipo; tal es el caso de C++, que requeriría que el usuario “extendiera” el lenguaje para, lo que a su vez podría dificultar que otra persona pudiese entender el código a primera vista (Leendert, 1991). [Footnote 4]: Internet of Things (IoT) es una tendencia en las ciencias de los sistemas de la comunicación que, durante la última década, ha logrado la interconexión de máquinas con otras máquinas, permitiendo una comunicación continua y discreta de información. Las aplicaciones del Internet of Things son cada vez más variadas y se estima que para el año 2025 habrán más de 50 mil millones de dispositivos conectados de esta manera (Farahmand, 2021) [Footnote 5]: La lista aquí mostrada es sumamente general y contempla sólo los lenguajes debatiblemente más famosos En otras palabras, lejos de lo que aquellos primeros programadores imaginaban, los lenguajes de programación actuales no están mucho más cerca de permitir a los humanos comunicarse con las computadoras utilizando un lenguaje natural. Además, la verdadera dificultad de hacer que una computadora entienda lo que un humano dice está en el hecho de que aún no entendemos completamente la forma en que el lenguaje humano funciona, pues el mismo Alan Turing decía en 1951: “las computadoras no son sino una imitación del cerebro humano” (Copeland, 2004). Curiosamente, para proponer un esquema sobre la forma en que el cerebro humano procesa, almacena y compara la información que recibe del mundo, la psicología cognitiva se apoyaría en los conceptos que los primeros programadores dedujeron sobre el proceso de la información aun cuando la psicología cognitiva había surgido antes de la primera computadora digital (Leahey, 2003). No es extraño entonces que los programas computacionales sean imitaciones de la forma en que el cerebro humano procesa la información para solucionar un problema, pues los humanos solemos establecer (muchas veces de manera inconsciente) una serie de pasos para realizar prácticamente cualquier tarea por cotidiana o extraordinaria que parezca: construimos algoritmos que dan solución a nuestros problemas. En ese sentido, un algoritmo es un conjunto de pasos o procedimientos que nos permiten alcanzar un resultado o resolver un problema (Cairó Battistutti, 2005), y esa es justamente la forma en que está construido o escrito un programa de computadora. Inteligencia artificial y redes neurales La búsqueda de la inteligencia artificial propiamente dicha no es tan moderna como se puede creer: en el siglo XVII, Thomas Hobbes proponía que “los pensamientos no son expresables en lenguaje escrito o hablado, sino en una dimensión interna” y que, por lo tanto, las operaciones lógicas que implica el *raciocinio* no están limitadas a las matemáticas, sino que son aplicadas a todo aquello que el individuo conoce (Haugeland, 1989). Alan Turing se basaría en ese concepto para responder la pregunta “¿puede una computadora pensar?”, pregunta que lo llevaría a establecer el *Test de Turing* entre 1951 y 1952^6^ para poder discernir si una computadora es capaz de pensar por sí misma. Pero Turing no pretendió en ningún momento definir el pensamiento, mucho menos la mente, simplemente establecer *criterios* que pudieran diferenciar el verdadero *raciocinio* de la “simple” computación de información (Copeland, 2004). Es aquí donde surge el primer punto a esclarecer: la inteligencia artificial actual no está ni cerca de la verdadera inteligencia, pues el pensamiento lógico-verbal, la solución de problemas como se plantea en la definición del algoritmo, no representa el pensamiento humano completamente^7^. Pero no por esto debe restársele valor a los esfuerzos conseguidos hasta ahora: en su estado actual, el concepto de *inteligencia artificial* se refiere a un programa computacional que recurre a *redes neurales* para aprender y procesar información. [Footnote 6]: Una buena parte de la teoría que complementa al Test de Turing no se publicó hasta 1999 (Copeland, 2004). [Footnote 7]: Para Luria (1989), existe un problema básico para definir el pensamiento humano: las estructuras psicológicas que pretenden describir el pensamiento (como la propuesta de la psicología cognitiva) conciben el mismo como un todo, cuando en realidad, las funciones cognitivas complejas son difusas. Las redes neurales están basadas en la forma en que las neuronas del cerebro humano se comunican entre sí para dar lugar a múltiples procesos cognitivos (Copeland, 2004). Cada “neurona” artificial está compuesta por una capa de input, una serie de capas ocultas, y una capa de output, esto quiere decir que cada capa recibe información que procesa según un sub-algoritmo que le dice qué hacer con esa información para después pasarla a la siguiente capa, eventualmente, la “neurona” completa el ciclo a lo largo de sus capas y pasa la información que ha procesado a la siguiente neurona. [Ref: Figura 4 -> figure_4] El *cómo* y *cuándo* la información pasa de neurona a neurona depende, desde luego, de la programación de la red neural: cada conexión entre neuronas tiene un *weight* o parámetro, un valor numérico que se establece durante el entrenamiento de la red neural que controla qué tanto el *output* o producción de una neurona afectará el resultado final (IBM, 2021). Las redes neurales son el elemento clave de conceptos como *machine learning*, *deep learning* y desde luego *inteligencia artificial*. Pero a diferencia de un cerebro humano, el cual continuamente está procesando la información que percibe del exterior, comparándola con la información que tiene en los distintos niveles de memoria y realizando una enorme variedad de juicios y consideraciones para aprender (Leontiev et al., 2004), las redes neurales dependen de los programadores para decirles *qué* hacer con la información que reciben: los antes mencionados *weights*, si bien pueden ser aleatorios durante las primeras iteraciones de la red neural, eventualmente requieren que los programadores los ajusten para obtener resultados consistentes de la red neural (T. Brown et al., 2020; IBM, 2021; Kingma et al., 2021). Una vez que el modelo comienza a generar *outputs* satisfactorios, los programadores pueden modificar ciertas partes de la red neural para que éste realice distintas acciones, esto es lo que hace la diferencia entre un algoritmo de red neural y el verdadero *deep learning*. Este término suele utilizarse, junto con *machine learning*, de manera intercambiable con *inteligencia artificial*, pero la acepción general es que una red neural con tres o más capas puede considerarse un algoritmo de *deep learning*, pues la palabra “deep” se refiere justamente a que el algoritmo tiene un nivel oculto en su red neural, la parte intermedia entre la parte de la red que únicamente prepara la información, la capa de *input*, y la parte que prepara la información para devolverla al usuario, la capa de *output* (IBM, 2021). A su vez, el término *machine learning* suele utilizarse para describir algoritmos de red neural que requieren una intervención más directa por parte de los humanos: mientras que un algoritmo *deep learning* tiene la capacidad de aprender, el *machine learning* no “aprende” a menos que sus desarrolladores explícitamente vuelvan a iniciar el proceso de entrenamiento, es decir, el deep learning es un paso más cerca hacia la “verdadera” *inteligencia artificial* (IBM, 2020). Inteligencia artificial y lenguaje humano Una vez que se tiene la idea general del funcionamiento de las redes neurales es más sencillo apreciar cuán lejos estamos de crear una verdadera inteligencia artificial^8^: los asistentes virtuales (Siri o Alexa) e incluso los chatbots que tanto revuelo han causado últimamente (ChatGPT o Bing Chat), están aún muy lejos de una verdadera inteligencia (IBM, 2020). Hemos hablado de Thomas Hobbes y Alan Turing proponiendo lo que una máquina inteligente tendría que ser capaz de hacer, y a partir de esas propuestas, actualmente se consideran tres tipos de inteligencia artificial (Goertzel, 2014; IBM, 2020): [Footnote 8]: En adelante “IA”. **Artificial Narrow Intelligence, ANI**: sistemas que realizan comportamientos “inteligentes” en contextos muy específicos, es decir, pueden llegar a aparentar que se está tratando con un humano real en tanto no se sobrepase su configuración. El estado actual de las inteligencias artificiales se encuentra aún en este nivel. **Artificial General Intelligence, AGI**: en este nivel se encontrarían sistemas capaces de realizar una gran variedad de tareas para lograr objetivos en distintos contextos y ambientes, en otras palabras, una AGI podría anticiparse y reaccionar a situaciones y problemas que sus creadores no habrían considerado al momento de su programación inicial. Una AGI tendría ya un cierto nivel de conciencia, pues podría recurrir a sus experiencias para razonar un plan de acción que dé solución a una situación a la que esta inteligencia se encontrara. A manera de ejemplo, ya que no hay ningún sistema actualmente que esté siquiera cerca de este nivel, pueden nombrarse algunas inteligencias artificiales de la ficción: los robots en *“Yo robot”* de Asimov, GladOS de la franquicia de videojuegos *“Portal”* y HAL9000 de la película *“2001: odisea del espacio”*. **Artificial Super Intelligence, ASI**: si bien las AGI son teóricamente posibles, las ASI tendrían que superar la sapiencia humana. Desde luego los únicos ejemplos provienen de obras de ficción: La IA de la franquicia de películas *“The Matrix”* y Skynet de la franquicia de películas *“Terminator”*. Tanto las AGI como las ASI se consideran “inteligencias artificiales fuertes” (IBM, 2020) y, si bien algunos autores consideran que actualmente estamos en el puente entre las ANI y las AGI, la principal limitante es bastante burda: No entendemos cómo funciona la mente humana, no podemos ni siquiera señalar a una parte del cerebro humano y declarar “aquí está la conciencia”, por lo tanto, es irreal pretender que pudiésemos imitarla (Goertzel, 2014). Los avances actuales en IA no se han logrado por accidente, como se ha venido planteando a lo largo de este artículo, todo ha sido un largo camino y sería obtuso pretender que el siguiente paso en el desarrollo de la IA sucedería por accidente. Pero entonces, si las IA no aprenden como los humanos y aun necesitan que sus programadores guíen ese aprendizaje, ¿cómo entienden las IA como ChatGPT, Dall-E o Stable Diffusion lo que les pedimos? Y realmente, los modelos de IA disponibles para el público al momento de escribir este artículo logran interpretar el tan elusivo “lenguaje humano” bastante bien. ChatGPT^9^, por ejemplo, es una “IA conversacional que utiliza NPL^10^ para generar respuestas parecidas a las que daría un humano (real)” (T. B. Brown et al., 2020). Esto quiere decir que ChatGPT utiliza una serie de redes neurales para entender y generar expresiones escritas: no es que ChatGPT entienda lo que le escribimos, al menos no en el sentido en el que un humano entiende. Cuando un humano observa una palabra escrita en su lengua materna, se dispara un complejo mecanismo cognitivo en su cerebro (pensamiento) que de forma simultánea recupera la pronunciación de dicha palabra (símbolo) y, particularmente, todo aquello que la persona relaciona con esa palabra (referente), producto de su aprendizaje, experiencia e ideología. [Footnote 9]: GPT significa “Generative Pre-trained Transformer”, literalmente “Transformador Generativo Pre-entrenado”. [Footnote 10]: Natural Language Processing. Este proceso cognitivo tan complejo ha sido replicado, desde luego en una escala mucho menor, en la red neural que ordena a la IA en cuestión: primero se tiene que diseñar un algoritmo con la serie de pasos que la IA tendría que seguir al encontrarse con una producción escrita, ese algoritmo entonces involucra su respectiva red neural en la cual se han de asignar los parámetros o *weights*. Evidentemente esto es más complicado de lo que parece, pues aun siendo una recreación bastante simple del proceso cognitivo que toma lugar en el cerebro humano, el entrenamiento del GPT-3, el modelo en el cual se basa ChatGPT^11^, tomó varios días ininterrumpidos de GPUs^12^ procesando información de entrenamiento. Ahora bien, ¿qué significa “entrenamiento” en el contexto de las inteligencias artificiales? El entrenamiento es, literalmente, el proceso mediante el que los programadores de un modelo de IA le enseñan a las redes neurales a procesar la información, pero sobre todo a establecer una diferencia entre los resultados (*outputs*) deseables de aquellos considerados errores (Radford et al., 2021). Lo que realmente hace único al modelo GPT-3 es la enorme cantidad de parámetros que posee en su red neural: cerca de 175 mil millones de parámetros (T. B. Brown et al., 2020). Como se ha mencionado anteriormente, al comienzo del entrenamiento la red neural comienza con parámetros aleatorios, derivados de los algoritmos particulares de cada modelo y, según las necesidades u objetivos de los desarrolladores, nuevos parámetros se van añadiendo o se van modificando los existentes. [Footnote 11]: El 14 de marzo de 2023 OpenAI, la empresa responsable de ChatGPT, liberó oficialmente GPT-4 para sus usuarios de paga, no hay información clara, pero se asume que la versión gratuita sigue utilizando GPT-3. [Footnote 12]: Graphic Processor Unit, componentes principales de las tarjetas de video. Modelos de IA como ChatGPT, DALL-E o Stable Diffusion^13^ están causando mucho revuelo al momento de escribir este artículo porque no sólo entienden lo que sus usuarios les piden, sino que pueden predecir y aprender sin necesidad de que sus programadores intervengan. Los modelos lingüísticos como ChatGPT, por ejemplo, fueron entrenados oponiendo conocimiento “real” con muestras de lenguaje que los programadores utilizaron para “enseñarle” a analizar construcciones escritas: una vez que la red neural logró entender esas construcciones muestra, se fueron añadiendo más capas o modificando las existentes^14^ (T. B. Brown et al., 2020; Vaswani et al., 2017), de tal forma que los chatbots que están actualmente a la vanguardia pueden hacer mucho más que los bots tradicionales: las redes sociales y los proveedores de correo electrónico llevan varios años utilizando bots^15^ para detectar spam, traducir mensajes e incluso detectar mensajes radicales u ofensivos, mientras que chatbots como ChatGPT o Bing Chat pueden identificar el contexto, las ideas claves e incluso detectar jerga o palabras características de un corpus particular (Kublik & Saboo, 2022). Lo que hace la diferencia entre los bots de detección de spam y los chatbots antes mencionados son los *transformers*, un tipo de red neural propuesto por investigadores de la universidad de Toronto (Kublik & Saboo, 2022) que, habiendo aprendido a identificar las funciones gramaticales de las palabras en distintas muestras de lenguaje durante su entrenamiento, se le ha programado para aplicar lo aprendido en nuevas muestras, que en otras palabras significa que estas redes neurales pueden aplicar lo que sus desarrolladores les han enseñado, mediante horas y horas de ensayo y error, para analizar producciones escritas completamente nuevas; es así como los modelos lingüísticos no solo “entienden” lo que se les pide sino que también pueden producir respuestas a partir de esa expresión inicial con base en los datos duros a los que tiene acceso además de lo que aprendió durante la fase de entrenamiento (T. B. Brown et al., 2020; Douglas Heaven, 2020). [Footnote 13]: DALL-E y Stable DIffusion son modelos de generación de imágenes a partir de texto, dichos modelos serán explorados más adelante en este artículo. [Footnote 14]: Los procesos de entrenamiento de los principales modelos de AI no se han revelado de manera detallada por razones de propiedad intelectual. [Footnote 15]: En este contexto la palabra \"bot\" se refiere a un algoritmo de machine learning. Para aclarar un poco más este proceso obsérvese la Figura 7, donde la flecha azul representa la forma en que el *transformer* le permite a la IA comparar lo que sabe sobre la palabra “El” y compararla con el resto de las palabras en una secuencia. En la figura antes mencionada se ha obviado que cada palabra de la oración pasa por el mismo proceso que las primeras dos: los *transformers*, las redes neurales que como se dijo anteriormente son características de los modelos lingüísticos, procesan las palabras de una producción escrita de manera simultánea, lo que les evita la necesidad de utilizar memoria física para guardar el texto completo durante todos los pasos el proceso y además, evita la interferencia de este durante los procesos intermedios. Esto es lo que caracteriza a los modelos lingüísticos que forman el núcleo de chatbots como ChatGPT o Bing Chat, pues al poder analizar los componentes de un texto dado de manera simultánea pueden entonces aplicar una fórmula matemática para calcular las palabras con mayor probabilidad^16^ de aparecer en el texto, a partir de lo cual, pueden incluso predecir la continuación de este (Vaswani et al., 2017). Esto le permite a ChatGPT, por ejemplo, producir una respuesta aceptable pese a no tener conocimiento sobre lo que el usuario le ha preguntado o solicitado; esto, combinado con la masiva red neural que compone el GPT-3, hace que ChatGPT siempre tenga una respuesta “válida”. [Footnote 16]: Se utilizan fórmulas de cálculo probabilístico que el algoritmo puede aplicar múltiples veces en múltiples pasos del proceso del análisis, desde luego la fórmula es única para cada modelo. [Ref: Figura 7 -> figure_7] Para explorar la capacidad de atención y predicción de ChatGPT se realizaron algunos experimentos simples (ver figura 8): para provocar una respuesta incoherente o extraña, se le presentó a ChatGPT una frase en español, lengua que ChatGPT puede “entender” mediante *machine translation*, es decir, la IA traduce la frase que le introducimos (“a mí me gusta el tangananá”) mediante el proceso de análisis antes descrito, y logra predecir el significado de la frase e incluso generar una respuesta escrita en español; sin embargo, como puede apreciarse en la figura, la IA no tiene forma de saber qué significa “tangananá”. Continuando con el experimento se introdujo la frase “Pese a la negativa de prensa covfefe” (ver figura 9), una traducción al español de un tweet publicado por Donald Trump que de inmediato se volvió viral por su carencia de sentido. En este caso, si bien “covfefe” no tiene significado alguno se esperaba que, por la viralidad del evento, ChatGPT tuviera un registro en su memoria sobre el mismo, de tal forma que la IA pudiera entender el contexto de la frase introducida. Como puede apreciarse en la respuesta generada por el chatbot, éste no entiende la frase, pero logra identificar la palabra “covfefe” y ligarla a información dura en su memoria para dar una respuesta coherente. [Ref: figura 8 -> figure_8] [Ref: figura 9 -> figure_9] Estas dos pruebas, si bien simples, sirven para mostrar lo que los desarrolladores han denominado un “motor de atención”: una red neural con parámetros y capas que le permiten a la IA no solo identificar el contexto de una producción y compararlo con la información “dura” que tienen en su memoria, sino a identificar las palabras clave de una construcción, lo que le permite predecir palabras afines mediante un análisis sintáctico único, pues a diferencia de los humanos, la IA no entiende realmente lo que es un verbo o un adjetivo, sino que los compara con valores (*scores*) que le ha asignado a los componentes de las oraciones usadas durante el periodo de prueba, que en otras palabras significa que la IA identifica la función gramatical de una palabra dependiendo su posición en el texto y comparándola con lo que aprendió durante la fase de entrenamiento (Kublik & Saboo, 2022; Singh, 2023; Vaswani et al., 2017). El modelo sucesor de GPT-3, GPT-4, se puso al alcance del público el 14 de marzo de 2023. En el reporte oficial no hay una mención clara de lo que hace distintos a GPT-3 de GPT-4, pero se alude a que se han agregado más parámetros (*weights*), añadiendo varias capas a la red neural. Además, de acuerdo con OpenAI^17^ (OpenAI, 2023b), GPT-4 ha sido entrenado directamente con textos referentes a exámenes y cursos estandarizados de una gran variedad de ramas, algunas de las cuales incluyen: • **SAT Math** • **Medical Knowledge Self-Assessment Program** • **AP Art History** • **AP Biology** • **AP Chemistry** • **AP Language and Composition** • **AP Macro and Microeconomics** • **Advanced Sommelier** • **AP Psychology** • **AP Statistics** [Footnote 17]: Empresa detrás de GPT y DALL-E. GPT-4 ha logrado obtener puntajes por encima del promedio humano en casi todas las pruebas y exámenes relativos a estos campos. A diferencia de GPT-3, GPT-4 posee una cantidad masiva de información que ha estudiado extensivamente durante horas y horas de entrenamiento (OpenAI, 2023b). Pero más impresionante aun es la capacidad de GPT-4 de “entender” imágenes, describirlas, predecir contenido relativo a dichas e incluso producir contenido específico a una rama del conocimiento (véase figura 10). El potencial de este nuevo modelo lingüístico todavía está por verse^18^ y no cabe duda de que la competencia con Google y Microsoft impulsará la inteligencia artificial a niveles sorprendentes. [Footnote 18]: En el livestream \"GPT-4 Developer Livestream\" (OpenAI, 2023a) disponible en YouTube, Greg Brockman, president y co-fundador de OpenAI le pide a GPT-4 que genere el código java necesario para crear un sitio web que se dibujó en una servilleta. [Ref: figura 10 -> figure_10] Inteligencias artificiales de generación de imágenes a partir de texto El proceso semántico que realizan las inteligencias artificiales como GPT-3 está revolucionando la forma en que los humanos acceden y hacen uso de la información, pero hay también inteligencias artificiales que no sólo pueden interpretar texto, sino que además pueden generar imágenes a partir de ese texto. El principio es el mismo que con los modelos lingüísticos como GPT-3: el texto introducido por el usuario pasa por una red neural con distintos parámetros y genera un *output* de vuelta al usuario, sin embargo, en este caso el *output* ha de ser una imagen, por lo que hay una serie de pasos extras bastante sustanciales. Hazañas como la de GPT-4 siendo capaz de extraer información de una fotografía de un dibujo en una servilleta y generar un *output* sumamente específico a partir de lo que logra predecir de dicha información son sólo una muestra de lo que inteligencias artificiales de generación de imágenes a partir de texto pueden hacer. Actualmente hay dos modelos generativos que han cautivado al mundo: DALL-E y Stable Diffusion, ambos con claras diferencias en su diseño. Comenzaremos por describir la forma en que DALL-E funciona por tratarse de un modelo lingüístico parecido a GPT-3 que ya ha sido explorado anteriormente en este artículo. El primer problema al momento de conectar lenguaje con imágenes es, como sugiere la figura 6, el siguiente paso en la arquitectura de los *transformers*, la arquitectura de red neural que caracteriza a los modelos lingüísticos. En otras palabras, GPT-3 es muy bueno para identificar, generar y predecir lenguaje escrito, pero no puede ni generar imágenes ni obtener información de imágenes. Ni siquiera GPT-4 que puede “leer” imágenes logra completar esta tarea tan bien como lo haría un modelo expresamente diseñado para esta tarea (Goodfellow et al., 2014; Jay Wang, 2021). Así como los modelos lingüísticos deben ser entrenados para interpretar texto, DALL-E tuvo que ser entrenado para interpretar una imagen, a este proceso se le llama *codificación*. El problema es que, a diferencia de los humanos, una inteligencia artificial no asocia lo visual con lo lingüístico, sino que el lenguaje escrito y las imágenes suponen dos dimensiones completamente distintas: se necesita crear una nueva red neural que le permita a la IA “traducir” una imagen en una expresión lingüística a partir de la cual se pueda generar otra imagen, todo esto simplemente para la etapa de entrenamiento. La *codificación* de imágenes ya lleva varios años siendo utilizada por bots moderadores en redes sociales: se entrenó a las redes neurales poco profundas a detectar patrones de píxeles^19^ característicos de contenido sensible que la plataforma en cuestión decide rechazar por defecto. El problema, de manera similar a los bots de detección de spam y traducción, es que estas redes neurales no aprenden, no se adaptan y mucho menos entienden las implicaciones lingüísticas de aquello que analizan. Una de las principales propuestas que marcaría la evolución de las inteligencias artificiales de generación de texto serían los *Visual Auto Encoders* (VAE), redes neurales que convierten una imagen dada en información que la IA pueda analizar mediante *transformers* (Goodfellow et al., 2014; Van Den Oord et al., 2017). Los VAE generan lo que los desarrolladores denominan un “espacio latente”, un punto intermedio entre la codificación y la decodificación; en este espacio latente el VAE recurre a un *codebook*, una especie de vocabulario que los desarrolladores programan en el VAE, de tal forma que éste tiene que asignar partes de la imagen inicial a categorías en ese *codebook* (véase figura 11). Es un proceso muy largo que consume una enorme cantidad de recursos y necesita una gran cantidad de GPUs trabajando en conjunto, pero se descubrieron dos ventajas principales (Van Den Oord et al., 2017): • 1. Los VAE no necesitan decodificar la imagen *input* píxel por píxel sino en de 32 por 32 píxeles (*tiles* o azulejos) y, dependiendo de la capacidad de proceso disponible, se pueden usar múltiplos de esas dimensiones (64x64, 128x128, 256x256, etc.). • 2. Se le pueden agregar capas extra al VAE que le permitan mejorar la calidad de la imagen antes de devolverla en el *output* final (véase figura 12). [Footnote 19]: Una imagen digital está conformada por píxeles, pequeñas extensiones cuadradas que poseen un cierto brillo y color. [Ref: figura 11 -> figure_11] [Ref: figura 12 -> figure_12] Es importante mencionar que DALL-E no utiliza un VAE de forma explícita, OpenAI no explica a detalle cómo funciona este modelo, pero se cree que recurre a un proceso similar durante su entrenamiento, pues OpenAI afirma que este recurre al GPT-3, lo que les permitió entrenar el modelo más rápido y establecer el proceso de decodificación como un proceso discreto (Razavi et al., 2019; Van Den Oord et al., 2017). En otras palabras, lo único que DALL-E tiene que hacer es asignarles categorías a los *tiles* resultantes de la decodificación de la imagen *input*^20^. DALL-E no está disponible para el público, al menos no del todo, pues se trata más de una prueba de concepto que de un modelo con una verdadera aplicación como, digamos, GPT-3 o como se verá más adelante, Stable Diffusion. OpenAI ha reconocido que los resultados de DALL-E presentados en sus artículos y reportes de estado están específicamente seleccionados de entre cientos (potencialmente miles) de imágenes generadas carentes de sentido (Jay Wang, 2021), pues, aunque este modelo es capaz de generar imágenes con buena calidad, además de lograr interpretar indicaciones escritas que podrían considerarse imposibles o carentes de sentido (véase figura 13). [Footnote 20]: Desde luego, por ser parte de lo que hace a DALL-E único, OpenAI no explica este proceso a detalle. [Ref: figura 13 -> figure_13] Pero hay otro problema: si bien es relativamente sencillo obtener información con la cual entrenar y establecer la base de conocimiento en modelos lingüísticos como GPT-3, los modelos generativos de imágenes como DALL-E necesitan, desde luego, muchas imágenes con las cuales aprender. Más importante aún, cada imagen necesita ir acompañada de una descripción más o menos detallada para que la IA pueda aplicar su entrenamiento y obtener resultados consistentes. A los conjuntos de imágenes acompañadas de texto se les denomina *datasets*, y a lo largo de los años compañías como Microsoft, Amazon y Google han construido sus propios datasets^21^, los cuales han sido instrumentales en el entrenamiento de modelos como DALL-E. Como se dijo anteriormente, este entrenamiento ha sido mucho más lento y problemático que el de los modelos lingüísticos que no generan imágenes, pues aún cuando se logran ajustar los parámetros de un modelo de manera satisfactoria, no hay ninguna garantía de que el modelo mantenga su consistencia al exponerlo a un *dataset* nuevo ya que, a diferencia de los humanos, las inteligencias artificiales carecen de un motor cognitivo que enlace experiencias, conocimientos e ideologías con los estímulos del exterior (Luria, 1989), por lo que la IA puede llegar a realizar descripciones extrañas (véase figura 14). Greimas (1987) explicaba (en humanos, desde luego) con su propuesta de los ejes semánticos: el mecanismo semiótico no se da por adición y substracción, sino por una concatenación de lo que el sujeto asume o conoce. Curiosamente, la forma en que las inteligencias artificiales de generación de imágenes como DALL-E parecen realizar un proceso similar al propuesto por Hjelmslev que es más matemático (Bigot, 2010), aunque no hay menciones directas a Hjelmslev en la literatura referente a DALL-E, Stable Diffusion, VAEs o clasificadores, se asume que la aproximación hjelmsleviana se adapta mejor a las fórmulas probabilísticas que componen el motor lógico de las redes neurales de estas inteligencias artificiales. [Footnote 21]: Algunos ejemplos de datasets popularmente usados para entrenar inteligencias artificiales son ImageNet con 14 millones de imágenes (Stanford Vision Lab, 2020); Microsoft COCO con cerca de 330 mil imágenes (Lin et al., 2014) y CIFAR-10 que contiene imágenes de 32x32 píxeles con 10 distintas categorías de objetos (Krizhevsky, 2017). [Ref: figura 14 -> figure_14] A finales de 2022, Stability AI publicó el modelo 1.5 de Stable Diffusion en la página github.com^22^. Aunque Stable Diffusion es comúnmente conocido como un generador de imágenes a partir de texto, este modelo no utiliza un modelo lingüístico como lo hace DALL-E, en su lugar recurre a una combinación de distintos modelos como VAEs y CLIP^23^ para ejecutar la síntesis de imágenes y, sobre todo, Stable Diffusion recurre a un proceso que añade ruido a la imagen durante varios pasos (véase figura 15). Por esta razón este modelo no entiende realmente lo que sus usuarios le piden mediante la caja de indicaciones o prompts (véase figura 16) como lo haría DALL-E, pues incluso cuando Stable Diffusion recurre a VAEs^24^, el proceso de difusión inversa mediante el que este modelo genera imágenes no involucra procesos lingüísticos (Ho et al., 2020; Kingma et al., 2021; Nichol & Dhariwal, 2021; Song et al., 2020). [Footnote 22]: En diciembre de 2022 la misma compañía liberó el modelo 2.0. [Footnote 23]: Contrastive Language-Image Pre-Training (Radford et al., 2021). [Footnote 24]: Stable Diffusion utiliza checkpoints, variaciones de su entrenamiento base que pueden omitir la necesidad de VAEs. [Ref: figura 15 -> figure_15] [Ref: figura 16 -> figure_16] Para compensar esto, Stability AI ha hecho públicos tanto los parámetros generales de la red neural como el checkpoint de su entrenamiento^25^. Un checkpoint puede entonces tomarse como base para re-entrenar el modelo con un cierto tipo de imágenes (Rombach et al., 2021), creando a su vez un nuevo checkpoint sumamente eficaz para generar un cierto tipo de imagen, pero sólo ese tipo de imagen (véase figura 17). Stability AI considera que esto es el punto fuerte de su modelo, pues ha sido entrenado con un *dataset* con un fuerte enfoque en muestras de arte, desde pinturas e ilustraciones hasta modelos tridimensionales, lo que le brinda una gran versatilidad al modelo para generar “imágenes hermosas” (Stability AI, 2022). Esto ha causado una gran controversia por haberse comprobado que Stability AI entrenó su modelo con los trabajos de artistas digitales sin su consentimiento (Vincent, 2023), pero sobre todo ha traído una importante pregunta a la mesa: ¿puede una inteligencia artificial “crear” arte? [Footnote 25]: La comunidad de usuarios suele referirse a los checkpoints como modelos, lo cual puede llegar a provocar confusiones pues el modelo es Stable Diffusion, los checkpoints son información de entrenamiento. [Ref: figura 17 -> figure_17] Hoy más que nunca el término “inteligencia artificial” está llevando a la humanidad a cuestionarse cuánto sabemos realmente sobre nosotros, sobre lo que nos hace humanos: ¿qué significa “significado”? ¿cuál es el significado de “crear”? y, quizás más importante aún, ¿puede una computadora “crear”? Es fácil perderse en la sorprendente capacidad de modelos como DALL-E o Stable Diffusion para no sólo entender lenguaje natural sino además generar imágenes que responden a una descripción textual dada; pero la forma en que estos modelos están revolucionando nuestra realidad va más allá de si las inteligencias artificiales reemplazarán a los humanos en actividades como programación o incluso ilustración: ¿podrían las inteligencias artificiales ayudarnos a entender nuestra propia mente? Así como muchos artistas, con justa razón, se han motivado a emprender acciones legales contra modelos que han sido entrenados explícitamente para replicar su estilo, a muchos otros les ha despertado un cuestionamiento por explorar cómo pueden las inteligencias artificiales elevar su esfuerzo artístico pues, de forma similar a como GPT-4 está ahorrando horas de trabajo tedioso y mecánico a los programadores que a su vez les permite enfocarse en aspectos verdaderamente creativos o hasta experimentales de su labor, las inteligencias artificiales podrían liberar la mente del artista para elevar su intención artística que, sin duda alguna, seguirá siendo una labor exclusivamente humana por bastante tiempo. Referencias Battleship New Jersey. (2020, August 31). Fire Control. Https://Www.Youtube.Com/@BattleshipNewJersey. https://www.youtube.com/watch?v=szxNJydEqOs Bi, W. L., Hosny, A., Schabath, M. B., Giger, M. L., Birkbak, N. J., Mehrtash, A., Allison, T., Arnaout, O., Abbosh, C., Dunn, I. F., Mak, R. H., Tamimi, R. M., Tempany, C. M., Swanton, C., Hoffmann, U., Schwartz, L. H., Gillies, R. J., Huang, R. Y., & Aerts, H. J. W. L. (2019). Artificial intelligence in cancer imaging: Clinical challenges and applications. CA: A Cancer Journal for Clinicians, 69(2), 127–157. https://doi.org/10.3322/CAAC.21552 Bickerton, D. (2009). Adam's tongue: how humans made language, how language made humans. Macmillan. Bigot, M. (2010). Apuntes de lingüística antropológica. Facultad de humanidades y artes de la universidad nacional de Rosario. Boeckx, C. A., & Benítez-Burraco, A. (2014). The shape of the human language-ready brain. Frontiers in Psychology, 5, 282. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). Language Models are Few-Shot Learners. Advances in Neural Information Processing Systems, 2020-December. https://arxiv.org/abs/2005.14165v4 Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., … Amodei, D. (2020). Language Models are Few-Shot Learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, & H. Lin (Eds.), Advances in Neural Information Processing Systems (Vol. 33, pp. 1877–1901). Curran Associates, Inc. https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf Bureau of Ordnance. (1949). Ordnance Pamphlet 1140: Basic Fire Control Mechanisms. Bureau of Ordnance. Cairó Battistutti, O. (2005). Metodología de la Programación (3rd ed.). Alfaomega. Ceccarelli, M. (2007). Distinguished figures in mechanism and machine science: Their contributions and legacies. Springer. Copeland, B. J. (2004). The essential turing. Clarendon Press. Douglas Heaven, W. (2020). OpenAI's new language generator GPT-3 is shockingly good—and completely mindless. MIT Technology Review. https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/ Farahmand, F. (2021, October 22). Innovations and challenges in developing Medical Devices | Dr. Farid Farahmand. GHTC 2021 Plenary Talk. Goertzel, B. (2014). Artificial General Intelligence: Concept, State of the Art, and Future Prospects. Journal of Artificial General Intelligence, 5(1), 1–48. https://doi.org/doi:10.2478/jagi-2014-0001 Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio, Y. (2014). Generative Adversarial Networks. Communications of the ACM, 63(11), 139–144. https://doi.org/10.48550/arxiv.1406.2661 Gregersen, E. (2020). History of technology timeline. Encyclopedia Britannica. Greimas, A. J. (1987). Semántica Estructural: Investigación metodológica. (A. De La Fuente, Trad.). Gredos. Haugeland, J. (1989). Artificial intelligence: The very idea. MIT press. Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. Advances in Neural Information Processing Systems, 2020-December. https://doi.org/10.48550/arxiv.2006.11239 Hockstein, N. G., Gourin, C. G., Faust, R. A., & Terris, D. J. (2007). A history of robots: From science fiction to surgical robotics. Journal of Robotic Surgery, 1(2), 113–118. https://doi.org/10.1007/S11701-007-0021-2/FIGURES/6 IBM. (2020, June 8). AI vs. Machine Learning vs. Deep Learning vs. Neural Networks: What's the Difference? IBM's Cloud Blog. https://www.ibm.com/cloud/blog/ai-vsmachine-learning-vs-deep-learning-vs-neural-networks IBM. (2021, December 16). What are neural networks? Ibm.Com. Janssen, C. P., Donker, S. F., Brumby, D. P., & Kun, A. L. (2019). History and future of human-automation interaction. International Journal of Human-Computer Studies, 131, 99–107. https://doi.org/10.1016/J.IJHCS.2019.05.006 Jay Wang, J. (2021, January 5). DALL·E: Creating images from text. OpenAI Research. https://openai.com/research/dall-e Kaur, R., Kumar, P., & Singh, R. P. (2014). A Journey of digital storage from punch cards to cloud. IOSR Journal of Engineering, 4(3), 36–41. Kingma, D. P., Salimans, T., Poole, B., & Ho, J. (2021). Variational Diffusion Models. Advances in Neural Information Processing Systems, 26, 21696–21707. https://doi.org/10.48550/arxiv.2107.00630 Krizhevsky, A. (2017, September). The CIFAR-10 dataset. Alex Krizhevsky. https://www.cs.toronto.edu/~kriz/cifar.html Kublik, S., & Saboo, S. (2022). GPT-3: Building Innovative NLP Products. O'Reilly Media. Leahey, T. H. (2003). Cognition and learning (D. K. Freedheim & I. B. Weiner, Eds.; Vol. 1, pp. 86–134). John Wiley & Sons. Leendert, A. (1991). C++ for programmers. Wjohn Wiley & sons. Leontiev, A., Luria, A. R., & Vigotsky, L. S. (2004). Psicología y pedagogía. Ediciones Akal. Lin, T. Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., & Zitnick, C. L. (2014). Microsoft COCO: Common Objects in Context. Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 8693 LNCS(PART 5), 740–755. https://doi.org/10.1007/978-3-319-10602-1_48 Luria, A. R. (1989). El cerebro en acción. (M. Torres, Trad.). Ediciones Roca. Murray Hopper, G. (1981). Keynote Address. In R. L. Wexelblat (Ed.), ACM SIGPLAN History of Programming Languages Conference (pp. 5–20). Academic Press. Nichol, A., & Dhariwal, P. (2021). Improved Denoising Diffusion Probabilistic Models. https://doi.org/10.48550/arxiv.2102.09672 OpenAI. (2023a, March 14). GPT-4 Developer Livestream. YouTube. OpenAI. (2023b). GPT-4 Technical Report. https://arxiv.org/abs/2303.08774v2 Palmer, F. R., & Frank Robert, P. (1981). Semantics. Cambridge university press. Pennock, G. R. (2007). James Watt (1736–1819). Distinguished Figures in Mechanism and Machine Science: Their Contributions and Legacies Part 1, 337–369. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. https://doi.org/10.48550/arxiv.2103.00020 Razavi, A., den Oord, A., & Vinyals, O. (2019). Generating diverse high-fidelity images with vq-vae-2. Advances in Neural Information Processing Systems, 32. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021). High-Resolution Image Synthesis with Latent Diffusion Models. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2022-June, 10674–10685. https://doi.org/10.1109/CVPR52688.2022.01042 Sammet, J. E., & Holberton, B. (1981). COBOL Session. In R. L. Wexelblat (Ed.), ACM SIGPLAN History of Programming Languages Conference (pp. 199–278). Academic Press. Singh, D. (2023, March 19). Understanding Large Language Models - The Force Behind chatGPT. The Growth Catalyst Newsletter. https://www.growth-catalyst.in/p/tech-simplified-understanding-large Song, J., Meng, C., & Ermon, S. (2020). Denoising Diffusion Implicit Models. https://doi.org/10.48550/arxiv.2010.02502 Stability AI. (2022). Stable Diffusion Launch Announcement. Stability.Ai. https://stability.ai/blog/stable-diffusion-announcement Stanford Vision Lab. (2020). About ImageNet. Image-Net.Org. https://image-net.org/about.php Van Den Oord, A., Vinyals, O., & others. (2017). Neural discrete representation learning. Advances in Neural Information Processing Systems, 30. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention Is All You Need. Advances in Neural Information Processing Systems, 2017-December, 5999–6009. https://arxiv.org/abs/1706.03762v5 Vincent, J. (2023, January 16). AI art tools Stable Diffusion and Midjourney targeted with copyright lawsuit. The Verge. https://www.theverge.com/2023/1/16/23557098/generative-ai-art-copyright-legal-lawsuit-stable-diffusion-midjourney-deviantart Weik, M. H. (1961). A third survey of domestic electronic digital computing systems (Issue 1115). Ballistic Research Laboratories. Computadora de control de fuego de la USS New Jersey. Obsérvense las manivelas para introducir información sobre el buque y sobre su objetivo. Tomada de “Fire Control” en YouTube (Battleship New Jersey, 2020) Imagen histórica mostrando la computadora electromecánica de control de fuego del acorazado USS New Jersey, con operadores usando manivelas para introducir datos La UNIVAC I sólo podía almacenar el equivalente mil palabras y pesaba un poco más de 7 toneladas (Murray Hopper, 1981). Imagen tomada de “A third survey of domestic electronic digital computing systems” (Weik, 1961) Fotografía histórica de la computadora UNIVAC I, una de las primeras computadoras comerciales Si bien la ENIAC de 1945 aquí mostrada es considerada la primera computadora programable, ésta carecía de un verdadero lenguaje: su programación se basaba en recombinaciones de cables y bulbos para hacer que la computadora realizara tal o cual operación (Sammet & Holberton, 1981). Imagen tomada de los archivos de la ARL Technical Library. Fotografía histórica de la ENIAC de 1945 mostrando la sala con la computadora y operadores trabajando con los paneles de control Ejemplo de dos \"neuronas\" en una red neural procesando información. Adaptada de \"What are neural networks?\" (IBM, 2021) Diagrama técnico mostrando dos neuronas artificiales conectadas con flechas indicando el flujo de información Ejemplo del flujo de la información a través de una red neural. Obsérvese cómo el flujo de datos en color azul continúa su camino por la red mientras que el flujo verde se interrumpe por no cumplir con los parámetros o \"weights\" establecidos en su programación. Adaptada de \"What are neural networks?\" (IBM, 2021) Diagrama de red neural compleja mostrando múltiples neuronas interconectadas con flujos de datos en diferentes colores El triángulo de Ogden es uno de los diagramas más básicos para describir el proceso semántico. Adaptada de \"Semantics\" (Palmer & Frank Robert, 1981). Diagrama triangular mostrando la relación entre Pensamiento, Símbolo ('GATO') y Referente (imagen de un gato), ilustrando el proceso semántico básico En este diagrama se explica, de forma bastante simplificada, la forma en que un modelo lingüístico como GPT-3 analiza producciones escritas. El proceso es muy similar al análisis sintáctico. (T. B. Brown et al., 2020) Diagrama técnico mostrando el análisis de la oración 'El gato se come al ratón' por redes neurales, con flechas indicando conexiones entre palabras En esta \"conversación\" con ChatGPT se ha introducido un fragmento de la canción \"Tangananica, Tangananá\" del programa chileno \"31 minutos\" Captura de pantalla de una conversación con ChatGPT donde se prueba con la frase 'A mí me gusta el tangananá' y la respuesta del sistema User input: A mí me gusta el tangananá ChatGPT response: ¡Me alegra saber que disfrutes del “tangananá”! Sin embargo, debo señalar que “tangananá” es una palabra que no tiene un significado claro en el idioma español, por lo que su uso puede ser confuso o ambiguo para otros. Si tienes alguna otra pregunta o si puedo ayudarte en algo más, por favor házmelo saber. El tweet original leía \"Despite the constant negative press covfefe\", se asume que fue un error de dedo, pero de inmediato se volvió objeto de burlas y parodias. Captura de pantalla de otra conversación con ChatGPT probando con la frase 'Pese a la negativa conferencia de prensa covfefe' y la respuesta del sistema User input: Pese a la negativa conferencia de prensa covfefe ChatGPT response: La frase “Pese a la negativa conferencia de prensa covfefe” parece ser una combinación de palabras sin sentido aparente. La palabra “covfefe” es en sí misma un término que ha sido utilizado en algunas ocasiones por el ex-presidente de Estados Unidos, Donald Trump, en un tweet que él mismo dejó sin terminar, lo que ha llevado a especulaciones sobre su significado. Sin embargo, en términos generales, la frase que proporcionaste no tiene un significado claro en el idioma español. Si tienes alguna otra pregunta o si puedo ayudarte en algo más, por favor házmelo saber. Se le presenta una imagen a GPT-4 y se le pregunta \"¿Qué hace chistosa a esta imagen?\". La IA entonces analiza la composición de dicha imagen y es capaz de formular una explicación relativamente satisfactoria. Tomada de \"GPT-4 Developer Livestream\" (OpenAI, 2023a) Captura de pantalla mostrando GPT-4 analizando una imagen humorística y proporcionando una explicación de por qué es graciosa What's funny about this image? The image is funny because it shows a squirrel holding a camera and taking a photo of a nut as if it were a professional photographer. It's a humorous situation because squirrels typically eat nuts, and we don't expect them to use a camera or act like humans. Este diagrama pretende ejemplificar, de una forma muy simple, lo que los VAEs hacen: durante la fase de entrenamiento se ajustan los parámetros de la red neural de tal forma que el VAE pueda reconstruir la imagen original. Adaptada de \"Neural discrete representation learning\", (Van Den Oord et al., 2017) Diagrama técnico del Visual Auto Encoder mostrando el proceso de codificación y decodificación de imágenes, con ejemplos de una oreja, nariz y lomo Una de las debilidades de los VAE es que, como si de una fotocopia se tratara, la imagen output suele ser borrosa o tener ruido. Izquierda: Imágenes input. Derecha: Imágenes output. Tomada de \"Neural discrete representation learning\" (Van Den Oord et al., 2017) Comparación lado a lado mostrando imágenes originales nítidas a la izquierda y sus versiones procesadas por VAE más borrosas a la derecha DALL-E es capaz de generar imágenes a partir de indicaciones (prompts) complejos. Sin embargo, debe recordarse que OpenAI ha seleccionado estos resultados de entre miles de imágenes generadas con el mismo prompt. Tomada de \"DALL·E: Creating images from text\" (Jay Wang, 2021). Cuadrícula mostrando múltiples variaciones de imágenes de aguacates generadas por DALL-E, demostrando la capacidad del sistema para crear diferentes interpretaciones artísticas an armchair in the shape of an avocado; an armchair imitating an avocado a clock in the form of an avocado; a clock imitating an avocado Radford et al. (2021) llevaron a cabo pruebas con su clasificador CLIP, un tipo de red neural que busca poder generar descripciones textuales para imágenes dadas sin necesidad de supervisión humana, con distintos datasets. El nombre del dataset usado se lee en negritas. Las barras verdes señalizan una descripción aceptable y las naranjas una incorrecta o sin sentido. Tomada de \"Learning Transferable Visual Models From Natural Language Supervision\" (Radford et al., 2021). Gráficos de barras mostrando la precisión del clasificador CLIP en diferentes datasets, con barras verdes para descripciones correctas y naranjas para incorrectas Los modelos de difusión latente (LPM) como Stable Diffusion agregan \"ruido\" a la imagen input poco a poco hasta tener sólo ruido, a partir del cual reconstruyen la imagen mediante un algoritmo probabilístico. Diagrama del modelo de difusión latente mostrando el proceso de codificación y decodificación con imágenes de perros en diferentes etapas del proceso Una de las interfaces gráficas disponibles para Stable Diffusion. Obsérvense las dos cajas de texto en la parte superior. Captura de pantalla de la interfaz gráfica de Stable Diffusion mostrando controles, configuraciones y una imagen generada de una manzana Al introducir la frase \"an apple\" en la caja de instrucciones, Stable Diffusion genera resultados distintos dependiendo el checkpoint utilizado. De izquierda a derecha: El checkpoint base. ProtoGen 5.8 enfocado en fotografía. Anything V3, enfocado en ilustraciones estilo manga/anime. Un ejemplo curioso del proceso semántico incompleto que la IA lleva a cabo. Tres imágenes de manzanas generadas por diferentes checkpoints de Stable Diffusion: una manzana realista, una fotográfica y una en estilo anime",
      "vector": [
        -0.02229328826069832,
        0.028313247486948967,
        -0.04638289287686348,
        -0.004430074710398912,
        0.06206328794360161,
        -0.014318299479782581,
        0.024519367143511772,
        0.03003266081213951,
        -0.0009629602427594364,
        -0.041259150952100754,
        0.006276394240558147,
        0.06442241370677948,
        0.007002523168921471,
        -0.03631098195910454,
        -0.004180046729743481,
        0.03230329230427742,
        0.009184812195599079,
        0.007247226312756538,
        -0.06327582150697708,
        -0.017340783029794693,
        0.003860363969579339,
        -0.05791456252336502,
        -0.020145399495959282,
        -0.0009821407729759812,
        0.017438188195228577,
        0.04422386735677719,
        0.0025014716666191816,
        -0.055645816028118134,
        -0.016277484595775604,
        0.02035299316048622,
        -0.002048829337581992,
        -0.0484965480864048,
        0.025456147268414497,
        -0.044087257236242294,
        -0.02269495278596878,
        0.031113561242818832,
        0.019307006150484085,
        0.014181312173604965,
        0.037498656660318375,
        -0.039920173585414886,
        -0.035311367362737656,
        0.02163238823413849,
        0.017365438863635063,
        0.01610417105257511,
        0.020076336339116096,
        -0.007564160041511059,
        -0.0205690935254097,
        0.053373783826828,
        -0.06067270040512085,
        0.025663502514362335,
        0.03429237753152847,
        0.012119241990149021,
        0.0066497293300926685,
        0.04414163529872894,
        0.07908371835947037,
        -0.013858155347406864,
        -0.048075247555971146,
        -0.005194844678044319,
        -0.011055191978812218,
        0.04706708714365959,
        -0.07534743845462799,
        -0.017283398658037186,
        -0.019112631678581238,
        0.04315492883324623,
        0.02012435533106327,
        -0.013365881517529488,
        -0.056930072605609894,
        -0.02883223071694374,
        0.060291167348623276,
        -0.04109947755932808,
        -0.06546150147914886,
        -0.02115544304251671,
        0.04370433837175369,
        0.02501317672431469,
        -0.010608701035380363,
        -0.009319246746599674,
        0.03175724670290947,
        0.044390033930540085,
        0.005484428256750107,
        0.023211514577269554,
        0.04112528637051582,
        -0.09732636064291,
        0.08622732758522034,
        -0.03536785766482353,
        -0.04047858342528343,
        0.0367598757147789,
        0.011072788387537003,
        0.018655695021152496,
        -0.013110402971506119,
        0.07334453612565994,
        -0.02526872232556343,
        -0.020550591871142387,
        0.0316413976252079,
        -0.035446181893348694,
        -0.002918579615652561,
        0.04796138033270836,
        -0.004938945174217224,
        -0.04215986281633377,
        0.026222391054034233,
        -0.03717978671193123,
        -0.005780191160738468,
        0.12852337956428528,
        -0.11630775779485703,
        0.006207638420164585,
        -0.002559907268732786,
        0.034987110644578934,
        0.026875384151935577,
        -0.05987824872136116,
        0.01758399046957493,
        0.014176063239574432,
        -0.0018699606880545616,
        -0.047220379114151,
        0.05501748248934746,
        -0.003607781371101737,
        -0.06541755795478821,
        -0.05155835673213005,
        -0.011840537190437317,
        -0.08837825059890747,
        0.01839509606361389,
        0.013213494792580605,
        -0.049540530890226364,
        0.02463332563638687,
        0.0007874267757870257,
        -0.030147269368171692,
        0.007158713880926371,
        -0.01652938686311245,
        0.028179146349430084,
        -0.02302335947751999,
        0.00502232788130641,
        0.04264998808503151,
        -0.012583480216562748,
        0.009786920621991158,
        -0.03870580345392227,
        -0.054184578359127045,
        0.008830520324409008,
        -0.035101037472486496,
        -0.03931543231010437,
        0.03199660778045654,
        0.060342706739902496,
        0.04901545122265816,
        -0.02660301700234413,
        -0.012170352973043919,
        0.03517661988735199,
        0.0025651706382632256,
        0.03366725891828537,
        0.04623328149318695,
        -0.004565648268908262,
        -0.03624418005347252,
        0.11087962239980698,
        0.021683812141418457,
        0.005218299105763435,
        0.01693781465291977,
        0.042459867894649506,
        -0.03470885753631592,
        0.02283564582467079,
        0.049447670578956604,
        0.01134671363979578,
        0.0652085691690445,
        -0.021349458023905754,
        -0.01731114648282528,
        -0.014198878780007362,
        -0.03415457904338837,
        -0.04406698793172836,
        -0.017234543338418007,
        -0.017920108512043953,
        -0.06000640243291855,
        0.03605141490697861,
        -0.01437416672706604,
        0.027559548616409302,
        -0.03292743116617203,
        0.04643815755844116,
        0.024844413623213768,
        0.045883238315582275,
        0.03477592021226883,
        0.009582143276929855,
        -0.026698896661400795,
        0.01614590734243393,
        0.018609121441841125,
        0.023190109059214592,
        -0.030568648129701614,
        -0.023363590240478516,
        -0.06963657587766647,
        0.028514452278614044,
        0.06438206136226654,
        -0.044006139039993286,
        0.02804715745151043,
        0.0972510278224945,
        -0.0022035047877579927,
        0.010243556462228298,
        -0.008353565819561481,
        -0.04708937555551529,
        0.0001357670407742262,
        0.03757430613040924,
        -0.017272215336561203,
        -0.054767489433288574,
        0.031058315187692642,
        0.06964270025491714,
        0.0332023948431015,
        0.04488587751984596,
        -0.017796942964196205,
        0.02330368384718895,
        -0.07058392465114594,
        0.04459814727306366,
        0.050286512821912766,
        0.008175645023584366,
        0.04602500796318054,
        -0.06256832927465439,
        8.928519673645496e-05,
        -0.005131945945322514,
        -0.014469828456640244,
        -0.0003020365256816149,
        -0.07209155708551407,
        0.037432100623846054,
        0.005353241227567196,
        -0.01680331863462925,
        0.004245245363563299,
        0.004374307114630938,
        0.05201629921793938,
        0.0028538403566926718,
        0.0018274005269631743,
        -0.009405000135302544,
        -0.005175629165023565,
        0.02892378345131874,
        0.13962699472904205,
        -0.045490533113479614,
        -0.0048464140854775906,
        0.025106577202677727,
        -0.05746724084019661,
        -0.036231666803359985,
        -0.08472290635108948,
        -0.029036084190011024,
        0.05308251827955246,
        0.01990707404911518,
        -0.02883051708340645,
        0.00383743061684072,
        0.04029051214456558,
        -0.025327758863568306,
        0.058558389544487,
        -0.0035558752715587616,
        0.04064936563372612,
        -0.07859903573989868,
        -0.03421410918235779,
        -0.03837236016988754,
        0.010413352400064468,
        0.04668896645307541,
        -0.0013894614530727267,
        -0.0218539759516716,
        -0.006920032203197479,
        0.02834545634686947,
        -0.0266351830214262,
        -0.1068134605884552,
        0.05780954286456108,
        -0.014703666791319847,
        -0.05130697041749954,
        0.0021570187527686357,
        -0.013655216433107853,
        0.0263706985861063,
        0.0046407063491642475,
        0.07337801903486252,
        0.021281426772475243,
        -0.014186282642185688,
        0.0021868005860596895,
        0.05456557869911194,
        0.026571592316031456,
        -0.019354701042175293,
        -0.03284166008234024,
        -0.01953311637043953,
        0.036163751035928726,
        -0.06764322519302368,
        0.0035844133235514164,
        0.04186595603823662,
        0.0451078861951828,
        0.05278938263654709,
        0.053311169147491455,
        -0.001737707876600325,
        -0.06625109910964966,
        0.006010131444782019,
        -0.004701234400272369,
        0.03643503785133362,
        0.008518313989043236,
        0.07139434665441513,
        -0.07312288135290146,
        0.0483388788998127,
        -0.0010263962903991342,
        -0.05754236876964569,
        -0.02000431902706623,
        0.05037792772054672,
        -0.023925023153424263,
        -0.043620605021715164,
        -0.035405661910772324,
        0.011276505887508392,
        -0.035654615610837936,
        0.09252587705850601,
        0.0248920489102602,
        -0.0491911917924881,
        -0.029247403144836426,
        -0.03464494273066521,
        -0.03514214977622032,
        -0.018636202439665794,
        0.00505871744826436,
        -0.02751344069838524,
        0.016951508820056915,
        0.028479857370257378,
        0.05451824143528938,
        0.008257298730313778,
        -0.040529899299144745,
        -0.024955859407782555,
        -0.06320478022098541,
        -0.06363675743341446,
        0.016505947336554527,
        0.011569442227482796,
        -0.035276781767606735,
        -0.04392096772789955,
        0.06318517029285431,
        -0.028524847701191902,
        0.03652545064687729,
        0.013476768508553505,
        0.006323788780719042,
        -0.02651761844754219,
        0.01779133453965187,
        0.021765651181340218,
        0.02359973080456257,
        -0.02566344290971756,
        0.030362294986844063,
        -0.02511439472436905,
        0.03463953360915184,
        -0.02060178481042385,
        -0.08892510086297989,
        -0.007432722952216864,
        0.0016356853302568197,
        0.0264752134680748,
        0.020031066611409187,
        -0.02840007282793522,
        0.036354538053274155,
        -0.04277360439300537,
        -0.02413935586810112,
        -0.014129087328910828,
        -0.000822516914922744,
        -0.03192465379834175,
        -0.010432034730911255,
        -0.062899649143219,
        -0.02386230044066906,
        0.021679408848285675,
        0.022617509588599205,
        0.00915140938013792,
        -0.05335996672511101,
        0.02204792946577072,
        -0.0025183968245983124,
        0.011852731928229332,
        -0.019153185188770294,
        0.055848948657512665,
        0.0422394797205925,
        0.031958576291799545,
        -0.00833746138960123,
        -0.057943474501371384,
        0.013632108457386494,
        -0.0362560898065567,
        0.03451123088598251,
        0.0636313185095787,
        -0.055022042244672775,
        0.0075716604478657246,
        0.02845102734863758,
        -0.010300826281309128,
        0.03446880355477333,
        0.004355755168944597,
        -0.0229533351957798,
        0.027872126549482346,
        -0.054202623665332794,
        0.02380361221730709,
        -0.06503559648990631,
        -0.04041014984250069,
        -0.00109780952334404,
        -0.03281004726886749,
        -0.011721700429916382,
        -0.07531720399856567,
        -0.05228812247514725,
        -0.026573238894343376,
        0.0453195758163929,
        0.051068421453237534,
        -0.05530865862965584,
        0.02162828855216503,
        0.010285094380378723,
        0.013040606863796711,
        0.009003691375255585,
        0.014106337912380695,
        -0.022752853110432625,
        0.01494324766099453,
        0.014422865584492683,
        0.028978899121284485,
        -0.014614223502576351,
        -0.04000881686806679,
        0.06354697793722153,
        0.012312797829508781,
        0.07911216467618942,
        0.014933224767446518,
        0.04058244824409485,
        -0.053158827126026154,
        -0.000632732582744211,
        -0.017320435494184494,
        0.05981776490807533,
        0.03394186869263649,
        0.05458502843976021,
        0.042514607310295105,
        3.285321872681379e-05,
        -0.0414465107023716,
        -0.01946524903178215,
        0.03612656891345978,
        0.06127595528960228,
        -0.03525635227560997,
        -0.013207067735493183,
        0.031996969133615494,
        0.058278925716876984,
        -0.016207963228225708,
        -0.05273311585187912,
        0.004792869556695223,
        -0.01647190749645233,
        -0.02089065872132778,
        -0.005807599984109402,
        0.049023956060409546,
        -0.05194690451025963,
        -0.0337466299533844,
        -0.05916910246014595,
        0.004107187502086163,
        -0.020390374585986137,
        0.013355687260627747,
        -0.035488199442625046,
        -0.019837506115436554,
        -0.0018119236920028925,
        -0.0339793786406517,
        -0.035092517733573914,
        0.07001438736915588,
        -0.004253052640706301,
        0.054072678089141846,
        -0.036614980548620224,
        -0.03145295009016991,
        0.016270149499177933,
        0.08182106912136078,
        0.06058245897293091,
        -0.012925646267831326,
        0.005133230704814196,
        -0.022450868040323257,
        -0.07060692459344864,
        -0.03954184427857399,
        0.00940309464931488,
        0.052106477320194244,
        -0.016115432605147362,
        0.08308479189872742,
        0.02563098445534706,
        -0.0007836598088033497,
        -0.05217142403125763,
        0.028157977387309074,
        0.018005630001425743,
        0.03667924553155899,
        0.04367009922862053,
        -0.0771411582827568,
        -0.0029902183450758457,
        0.0024439243134111166,
        0.03764216974377632,
        -0.04766995459794998,
        -0.016883131116628647,
        0.03996555507183075,
        0.010111594572663307,
        -0.006231869105249643,
        0.07950135320425034,
        -0.060840535908937454,
        -0.05500447750091553,
        -0.024897685274481773,
        -0.01132579892873764,
        -0.04323643818497658,
        0.028287801891565323,
        0.008166550658643246,
        0.0440763458609581,
        -0.011906920932233334,
        0.04510730132460594,
        -0.026810314506292343,
        0.035514067858457565,
        -0.04644383490085602,
        -0.02346758171916008,
        0.005923380609601736,
        -0.051602061837911606,
        -0.002018395811319351,
        0.0065921698696911335,
        -0.09943093359470367,
        -0.016008440405130386,
        0.03699620068073273,
        0.010686851106584072,
        -0.016236091032624245,
        0.027057262137532234,
        0.005799123086035252,
        -0.011689972132444382,
        -0.06200125068426132,
        0.030129745602607727,
        -0.005586754996329546,
        0.004637215286493301,
        -0.031985826790332794,
        0.02025509439408779,
        0.0020011265296489,
        0.001864919438958168,
        0.0028517029713839293,
        0.0037505915388464928,
        -0.03791764751076698,
        -0.02743823640048504,
        -0.039278220385313034,
        0.02835778146982193,
        -0.0018683235393837094,
        0.014809668995440006,
        -0.004860685206949711,
        -0.0917028859257698,
        -0.03403538838028908,
        0.046047814190387726,
        -0.013689536601305008,
        -0.03315087407827377
      ],
      "title": "Conversando con una computadora: ¿Cómo entienden las inteligencias artificiales lo que les pedimos?"
    }
  ]
}